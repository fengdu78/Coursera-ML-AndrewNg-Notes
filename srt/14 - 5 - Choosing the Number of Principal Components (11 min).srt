1
00:00:00,090 --> 00:00:01,560
In the PCA algorithm we take
在PCA算法中，我们将（字幕整理：中国海洋大学，黄海广，haiguang2000@qq.com）

2
00:00:01,980 --> 00:00:03,530
N dimensional features and reduce
N维特征减少

3
00:00:03,970 --> 00:00:06,260
them to some K dimensional feature representation.
为某k维特征。

4
00:00:07,620 --> 00:00:09,090
This number K is a parameter
这个数字K是PCA算法

5
00:00:09,820 --> 00:00:10,800
of the PCA algorithm.
的一个参数。

6
00:00:11,810 --> 00:00:13,240
This number K is also called
这个数K也被称为

7
00:00:13,620 --> 00:00:15,080
the number of principle components
主成分的数字

8
00:00:15,830 --> 00:00:17,480
or the number of principle components that we've retained.
或者，我们保留的主成分的数字。

9
00:00:18,530 --> 00:00:19,640
And in this video I'd like
在这个视频中，我想

10
00:00:19,810 --> 00:00:20,850
to give you some guidelines,
给你们一些指引，

11
00:00:21,730 --> 00:00:23,090
tell you about how people
告诉你们一般情况下人们

12
00:00:23,430 --> 00:00:24,490
tend to think about how to
如何考虑

13
00:00:24,610 --> 00:00:26,740
choose this parameter K for PCA.
选取这个参数K。

14
00:00:28,650 --> 00:00:29,670
In order to choose k,
为了选择K，

15
00:00:30,110 --> 00:00:30,990
that is to choose the number
即选择这个主成分的

16
00:00:31,360 --> 00:00:34,110
of principal components, here are a couple of useful concepts.
数字，这里有几个有用的概念。

17
00:00:36,430 --> 00:00:37,520
What PCA tries to do
PCA试图

18
00:00:37,760 --> 00:00:38,760
is it tries to minimize
去减少

19
00:00:40,070 --> 00:00:41,510
the average squared projection error.
投影误差平方的平均值。

20
00:00:42,030 --> 00:00:43,200
So it tries to minimize
因此，它试图减少

21
00:00:43,430 --> 00:00:45,480
this quantity, which I'm writing down,
这个数量，就是我正在写的这个，

22
00:00:46,410 --> 00:00:47,980
which is the difference between the
它是

23
00:00:48,150 --> 00:00:50,010
original data X and the
原始数据X和

24
00:00:50,690 --> 00:00:53,470
projected version, X-approx-i, which
投影，X约-i(上个视频中定义）

25
00:00:53,620 --> 00:00:54,930
was defined last video, so
所以

26
00:00:55,020 --> 00:00:55,900
it tries to minimize the squared
它试图尽量减少

27
00:00:56,160 --> 00:00:57,360
distance between x and it's projection
x和x在低维面的投影

28
00:00:58,330 --> 00:00:59,750
onto that lower dimensional surface.
距离的差的平方

29
00:01:01,220 --> 00:01:02,990
So that's the average square projection error.
所以这是投影误差平方的平均值。

30
00:01:03,990 --> 00:01:05,320
Also let me define the
同时我也将定义

31
00:01:05,440 --> 00:01:07,020
total variation in the
数据的总方差

32
00:01:07,100 --> 00:01:08,730
data to be the
为样本X的平方和

33
00:01:09,020 --> 00:01:11,730
average length squared of
的平均值

34
00:01:12,140 --> 00:01:14,130
these examples Xi

35
00:01:14,450 --> 00:01:16,010
so the total variation in the
所以数据的

36
00:01:16,260 --> 00:01:17,930
data is the average of
总方差就是

37
00:01:18,070 --> 00:01:19,250
my training sets of the
我的训练集中

38
00:01:19,370 --> 00:01:21,640
length of each of my training examples.
每个训练样本的平均长度

39
00:01:22,190 --> 00:01:23,690
And this one says, "On average, how
这个说明了：“平均来说，

40
00:01:23,940 --> 00:01:24,850
far are my training examples
我的训练样本

41
00:01:25,690 --> 00:01:27,960
from the vector, from just being all zeros?"
距离全零向量的距离“

42
00:01:28,770 --> 00:01:30,460
How far is, how far
或者说，我的训练样本

43
00:01:30,820 --> 00:01:32,820
on average are my training examples from the origin?
距离原点有多远？

44
00:01:33,510 --> 00:01:34,450
When we're trying to choose k, a
当我们试图选择K，

45
00:01:35,870 --> 00:01:37,210
pretty common rule of thumb
一个常见的

46
00:01:37,400 --> 00:01:38,620
for choosing k is to choose
经验方法是

47
00:01:38,800 --> 00:01:40,290
the smaller values so that
选择较小的值，

48
00:01:40,980 --> 00:01:43,810
the ratio between these is less than 0.01.
使得这两者之间的比值小于0.01。

49
00:01:44,550 --> 00:01:45,540
So in other words,
换句话说，

50
00:01:46,340 --> 00:01:47,370
a pretty common way to
一个很常见的方式，

51
00:01:47,510 --> 00:01:48,460
think about how we choose k
来选择K

52
00:01:48,800 --> 00:01:51,180
is we want the average squared projection error.
是我们想让平均投影误差平方

53
00:01:51,580 --> 00:01:54,700
That is the average distance
即x和其投影的

54
00:01:55,240 --> 00:01:56,340
between x and it's projections
平均距离

55
00:01:57,570 --> 00:02:00,330
divided by the total variation of the data.
除以数据的总方差（

56
00:02:00,800 --> 00:02:01,870
That is how much the data varies.
数据的波动程度）。

57
00:02:02,940 --> 00:02:04,060
We want this ratio to be
我们希望这个比例

58
00:02:04,240 --> 00:02:06,760
less than, let's say, 0.01.
比如说，小于0.01

59
00:02:06,830 --> 00:02:09,450
Or to be less than 1%, which is another way of thinking about it.
或小于1％，这是另一种表述方式。

60
00:02:10,860 --> 00:02:11,940
And the way most people think
而且大多数人

61
00:02:12,150 --> 00:02:13,640
about choosing K is rather
选择K

62
00:02:13,860 --> 00:02:15,660
than choosing K directly the
并不是像多数人谈论的那样

63
00:02:15,890 --> 00:02:17,110
way most people talk about
直接进行选择

64
00:02:17,480 --> 00:02:18,940
it is as what this
这是因为这个数

65
00:02:19,160 --> 00:02:20,630
number is, whether it is 0.01
无论是0.01

66
00:02:20,740 --> 00:02:23,330
or some other number.
或一些其它数字。

67
00:02:23,720 --> 00:02:25,320
And if it is 0.01, another way
如果是0.01，另一种方式，

68
00:02:25,490 --> 00:02:27,020
to say this to use the
用PCA的语言来表述，

69
00:02:27,270 --> 00:02:30,120
language of PCA is that 99% of the variance is retained.
就是99％的方差性会被保留。

70
00:02:32,060 --> 00:02:33,480
I don't really want to, don't
不必担心

71
00:02:33,850 --> 00:02:34,810
worry about what this phrase
这个短语

72
00:02:35,140 --> 00:02:36,920
really means technically but this
的真正含义，这个

73
00:02:37,830 --> 00:02:39,170
phrase "99% of variance is retained" just means
短语“99％的方差会被保留”只是说

74
00:02:39,420 --> 00:02:41,710
that this quantity on the left is less than 0.01.
左侧的这个量小于0.01

75
00:02:42,340 --> 00:02:43,910
And so, if you
所以，如果你

76
00:02:44,930 --> 00:02:46,510
are using PCA and if you want
使用主成分分析，如果你想

77
00:02:46,630 --> 00:02:47,730
to tell someone, you know,
告诉别人，

78
00:02:48,220 --> 00:02:49,860
how many principle components you've
你保留了多少主成分

79
00:02:49,980 --> 00:02:51,080
retained it would be
这样去表述会

80
00:02:51,140 --> 00:02:52,360
more common to say well, I
比较好，我

81
00:02:52,450 --> 00:02:55,360
chose k so that 99% of the variance was retained.
选择k使得99％的方差被保留。

82
00:02:55,990 --> 00:02:56,960
And that's kind of a useful thing
这是一个需要知道的有用的

83
00:02:57,660 --> 00:02:58,530
to know, it means that you
东西，这意味着你

84
00:02:58,620 --> 00:02:59,820
know, the average squared projection
知道，平均投影误差平方

85
00:03:00,760 --> 00:03:01,720
error divided by the total
除以数据总方差，

86
00:03:01,900 --> 00:03:03,260
variation that was at most 1%.
结果不会超过1%。

87
00:03:03,820 --> 00:03:04,770
That's kind of an insightful
这样的表述方式是很有见地

88
00:03:05,270 --> 00:03:06,790
thing to think about, whereas if
的，否则

89
00:03:06,920 --> 00:03:08,420
you tell someone that, "Well I
你告诉别人，“嗯，我

90
00:03:09,170 --> 00:03:10,710
had to 100 principle
有100主成分

91
00:03:10,890 --> 00:03:12,030
components" or "k was equal
或”在1000维的数据中

92
00:03:12,720 --> 00:03:13,850
to 100 in a thousand dimensional
k等于100“

93
00:03:14,220 --> 00:03:15,350
data" it's a little
这对于

94
00:03:15,420 --> 00:03:16,600
hard for people to interpret
人们来说

95
00:03:19,100 --> 00:03:19,100
that.
不太好理解

96
00:03:19,320 --> 00:03:22,220
So this number 0.01 is what people often use.
因此这个数字0.01是人们经常使用的东西。

97
00:03:23,070 --> 00:03:25,380
Other common values is 0.05,
其他常见的值是0.05，

98
00:03:26,840 --> 00:03:27,810
and so this would be 5%,
因此，这将是5％，

99
00:03:27,990 --> 00:03:28,870
and if you do that then
如果你这样做，那么

100
00:03:29,210 --> 00:03:30,390
you go and say well 95%
你会说95％的

101
00:03:30,740 --> 00:03:32,320
of the variance is
方差

102
00:03:32,480 --> 00:03:34,280
retained and, you know
会保留，你知道

103
00:03:34,700 --> 00:03:36,710
other numbers maybe 90% of the variance is
其它数字也许90％的方差被

104
00:03:37,980 --> 00:03:40,030
retained, maybe as low as 85%.
保留，也许低至85％。

105
00:03:40,150 --> 00:03:42,410
So 90% would correspond to say
因此，90％将对应于说

106
00:03:44,340 --> 00:03:46,950
0.10, kinda 10%.
0.10，或10％。

107
00:03:47,250 --> 00:03:49,160
And so range of values
所以，值的范围从

108
00:03:49,900 --> 00:03:50,770
from, you know, 90, 95,
90，95，

109
00:03:50,870 --> 00:03:51,470
99, maybe as low as 85% of
99，也许低至85％

110
00:03:51,500 --> 00:03:55,100
the variables contained would be a fairly typical range in values.
都是一些具有代表性的范围。

111
00:03:55,780 --> 00:03:56,900
Maybe 95 to 99
也许95到99

112
00:03:57,690 --> 00:03:58,810
is really the most
是真正最

113
00:03:59,020 --> 00:04:02,080
common range of values that people use.
对被使用的范围

114
00:04:02,130 --> 00:04:02,950
For many data sets you'd be
对于许多数据集你会

115
00:04:03,010 --> 00:04:04,320
surprised, in order to retain
惊讶，为了保留

116
00:04:04,790 --> 00:04:06,590
99% of the variance, you can
99％的方差，你可以

117
00:04:06,790 --> 00:04:08,160
often reduce the dimension of
往往减少数据维数

118
00:04:08,200 --> 00:04:11,810
the data significantly and still retain most of the variance.
但仍保留大部分的方差。

119
00:04:12,440 --> 00:04:13,410
Because for most real life
因为对于真实世界的数据

120
00:04:13,560 --> 00:04:15,210
data says many features are
来说，许多特征都

121
00:04:15,280 --> 00:04:17,060
just highly correlated, and so
是高度相关的，因此

122
00:04:17,310 --> 00:04:17,940
it turns out to be possible
结果证明：

123
00:04:18,490 --> 00:04:19,540
to compress the data a
对数据进行

124
00:04:19,610 --> 00:04:20,990
lot and still retain you
很多压缩，仍然可以保留

125
00:04:21,360 --> 00:04:22,310
know 99% of the variance
99％的方差

126
00:04:22,530 --> 00:04:26,260
or 95% of the variance. So how do you implement this?
或95％的方差。那么，你如何实现呢？

127
00:04:26,810 --> 00:04:28,610
Well, here's one algorithm that you might use.
那么，这里提供一个你可能会用到的算法。

128
00:04:28,890 --> 00:04:30,360
You may start off, if you
你可以开始了，如果你

129
00:04:30,540 --> 00:04:31,360
want to choose the value of
要选择K的值，

130
00:04:31,470 --> 00:04:33,510
k, we might start off with k equals 1.
我们开始可以给K赋值1

131
00:04:33,550 --> 00:04:34,670
And then we run through PCA.
然后我们运行PCA算法

132
00:04:35,350 --> 00:04:36,440
You know, so we compute, you
然后我们计算，你

133
00:04:36,570 --> 00:04:38,880
reduce, compute z1, z2, up to zm.
减少，计算Z1，Z2，一直到ZM。

134
00:04:39,520 --> 00:04:40,790
Compute all of those x1 approx
计算所有这些X1-approx

135
00:04:41,090 --> 00:04:42,540
approx and so on up to xm approx
到 XM-approx

136
00:04:43,200 --> 00:04:45,110
and then we check if 99% of the variance is retained.
然后我们检查，是否保留了99％的方差。

137
00:04:47,140 --> 00:04:48,890
Then we're good and we use k equals 1.
如果保留了，那么我们让K等于1。

138
00:04:49,020 --> 00:04:51,960
But if it isn't then what we'll do we'll next try K equals 2.
但如果不是，那么我们将做什么，我们接下来会尝试让K等于2。

139
00:04:52,620 --> 00:04:53,810
And then we'll again
然后我们将再次

140
00:04:54,200 --> 00:04:56,070
run through this entire procedure and
运行PCA算法，

141
00:04:56,170 --> 00:04:57,770
check, you know is this expression satisfied.
检查一下，是否满足表达式。

142
00:04:58,470 --> 00:05:00,980
Is this less than 0.01. And if not then we do this again.
是否小于0.01。如果没有的话，我们重复这个步骤。

143
00:05:01,220 --> 00:05:03,070
Let's try k equals 3,
让我们试着让k等于3，

144
00:05:03,310 --> 00:05:04,910
then try k equals 4,
然后尝试k等于4，

145
00:05:04,970 --> 00:05:06,250
and so on until maybe
依此类推，直至也许

146
00:05:06,630 --> 00:05:07,730
we get up to k equals
我们让k等于

147
00:05:08,070 --> 00:05:09,040
17 and we find 99% of
17，我们发现99％的

148
00:05:09,090 --> 00:05:13,060
the data have is retained and then
的数据已被保留，然后

149
00:05:14,120 --> 00:05:15,110
we use k equals 17, right?
我们使得k等于17，对吗？

150
00:05:15,570 --> 00:05:17,160
That is one way
这是一种方式

151
00:05:17,240 --> 00:05:18,790
to choose the smallest value
来选择最小的K

152
00:05:19,130 --> 00:05:20,920
of k, so that and 99% of the variance is retained.
从而使99％的方差被保留。

153
00:05:22,380 --> 00:05:23,440
But as you can imagine,
但正如你可以想像的，

154
00:05:23,550 --> 00:05:25,140
this procedure seems horribly inefficient
这个过程看上去非常低效

155
00:05:26,210 --> 00:05:28,120
we're trying k equals one, k equals two, we're doing all these calculations.
我们尝试让k等于1，k等于2，我们正在做的所有这些计算。

156
00:05:29,580 --> 00:05:30,540
Fortunately when you implement
幸运的是，当你实现

157
00:05:31,130 --> 00:05:33,510
PCA it actually, in
PCA是，在

158
00:05:33,960 --> 00:05:35,530
this step, it actually gives us
这一步， 它实际上给了我们

159
00:05:35,910 --> 00:05:37,080
a quantity that makes it
一个量，使得它

160
00:05:37,320 --> 00:05:40,160
much easier to compute these things as well.
我们可以更容易地计算这些东西。

161
00:05:41,110 --> 00:05:42,160
Specifically when you're calling
特别是当你调用

162
00:05:42,820 --> 00:05:44,120
SVD to get these
SVD得到这些

163
00:05:44,340 --> 00:05:45,550
matrices u, s, and d,
矩阵U，S和D，

164
00:05:45,610 --> 00:05:46,780
when you're calling usvd on the
当你在协方差矩阵Σ上

165
00:05:47,040 --> 00:05:48,560
covariance matrix sigma, it also
调用usvd，它也会

166
00:05:48,860 --> 00:05:49,780
gives us back this matrix
给我们返回矩阵

167
00:05:50,300 --> 00:05:52,170
S and what
S

168
00:05:52,360 --> 00:05:53,430
S is, is going to
S实际上将会是

169
00:05:53,520 --> 00:05:56,790
be a square matrix an N by N matrix in fact,
一个N*N的方阵。

170
00:05:57,640 --> 00:05:58,090
that is
就是说

171
00:05:58,290 --> 00:05:58,290
diagonal.
对角线。

172
00:05:58,830 --> 00:06:00,380
So is diagonal entries s one
因此对角线元素s1

173
00:06:00,540 --> 00:06:01,640
one, s two two, s
1，S2，S3

174
00:06:01,980 --> 00:06:03,240
three three down to s
直到

175
00:06:03,590 --> 00:06:05,130
n n are going to
Sn都将

176
00:06:05,260 --> 00:06:07,010
be the only non-zero elements of
是这个矩阵的唯一的非零元素

177
00:06:07,130 --> 00:06:08,880
this matrix, and everything off
并且对角线之外

178
00:06:09,060 --> 00:06:11,470
the diagonals is going to be zero.
的所有元素都将是0。

179
00:06:11,590 --> 00:06:11,590
Okay?
OK？

180
00:06:11,670 --> 00:06:12,530
So those big O's that I'm drawing,
So 我正在画的这些大O，

181
00:06:13,340 --> 00:06:14,260
by that what I mean is
我想表达的意思是

182
00:06:14,740 --> 00:06:16,330
that everything off the diagonal
在这个矩阵中

183
00:06:17,130 --> 00:06:18,220
of this matrix all of those
对角线之外的所有

184
00:06:18,480 --> 00:06:20,310
entries there are going to be zeros.
元素都是0

185
00:06:22,300 --> 00:06:23,790
And so, what is possible
所以，在这里向表示的

186
00:06:24,190 --> 00:06:25,250
to show, and I won't prove
但我在这里不给出证明

187
00:06:25,480 --> 00:06:26,380
this here, and it turns out
但事实证明

188
00:06:26,620 --> 00:06:27,880
that for a given value of
对于一个给定的K

189
00:06:27,980 --> 00:06:29,920
k, this quantity
这个量

190
00:06:30,590 --> 00:06:37,820
over here can be computed much more simply.
的计算将会比较简单

191
00:06:38,800 --> 00:06:40,310
And that quantity can be computed
而且这个量可以这样计算：

192
00:06:41,000 --> 00:06:42,900
as one minus sum from
1减去

193
00:06:43,130 --> 00:06:44,400
i equals 1 through
从1到k

194
00:06:44,610 --> 00:06:47,960
K of Sii divided by
的sii的和

195
00:06:48,640 --> 00:06:50,050
sum from I equals 1
除以从1到n的

196
00:06:50,170 --> 00:06:52,010
through N of Sii.
的Sii的和。

197
00:06:53,360 --> 00:06:54,820
So just to say that it words, or
所以只是

198
00:06:55,000 --> 00:06:56,170
just to take another
从另一个角度

199
00:06:56,450 --> 00:06:57,330
view of how to explain that,
来进行解释的话

200
00:06:57,960 --> 00:06:59,370
if K equals 3 let's say.
如果K等于3，比如

201
00:07:00,810 --> 00:07:01,970
What we're going to do to
我们将要做的是

202
00:07:02,080 --> 00:07:03,200
compute the numerator is sum
计算分子的和

203
00:07:03,340 --> 00:07:04,680
from one--  I equals 1
从i=1到

204
00:07:04,820 --> 00:07:05,830
through 3 of of Sii, so just
3 的sii的值，所以仅仅

205
00:07:06,240 --> 00:07:08,170
compute the sum of these first three elements.
计算前三个元素的和。

206
00:07:09,280 --> 00:07:09,710
So that's the numerator.
这就是分子。

207
00:07:10,980 --> 00:07:12,880
And then for the denominator, well that's
然后对于分母，

208
00:07:13,090 --> 00:07:14,970
the sum of all of these diagonal entries.
计算对角项的和。

209
00:07:16,210 --> 00:07:17,470
And one minus the ratio of
然后1减去这个比值

210
00:07:17,660 --> 00:07:19,080
that, that gives me this
那给了我这个量

211
00:07:19,300 --> 00:07:21,330
quantity over here, that I've
在这里，我用

212
00:07:21,650 --> 00:07:23,440
circled in blue.
蓝色圆圈标注的。

213
00:07:23,650 --> 00:07:24,380
And so, what we can do
所以，我们需要做的，

214
00:07:24,650 --> 00:07:26,000
is just test if this
只是测试，这个值

215
00:07:26,430 --> 00:07:29,330
is less than or equal to 0.01.
是否小于或等于0.01。

216
00:07:29,370 --> 00:07:30,460
Or equivalently, we can test
或者等价地，我们可以测试

217
00:07:30,830 --> 00:07:31,960
if the sum from
1到k的

218
00:07:32,180 --> 00:07:33,010
i equals 1 through k, s-i-i
sii的和，

219
00:07:33,970 --> 00:07:35,180
divided by sum from i
除以

220
00:07:35,320 --> 00:07:37,090
equals 1 through n, s-i-i
1到n的sii的和，

221
00:07:37,650 --> 00:07:38,580
if this is greater than or
如果是大于或

222
00:07:38,770 --> 00:07:40,600
equal to 0.99, if you
等于0.99，如果你

223
00:07:40,720 --> 00:07:42,920
want to be sure that 99% of the variance is retained.
要确保99％的方差被保留。

224
00:07:44,770 --> 00:07:45,650
And so what you can do
那么你可以这样做：

225
00:07:45,940 --> 00:07:48,360
is just slowly increase k,
只是慢慢地增加K，

226
00:07:48,770 --> 00:07:49,820
set k equals one, set k equals
让k=1，让k=2

227
00:07:50,100 --> 00:07:51,290
two, set k equals three and so
k=3，以此类推

228
00:07:52,140 --> 00:07:53,240
on, and just test this quantity
然后检查哪个是

229
00:07:54,720 --> 00:07:56,120
to see what is the
使得99％的方差被保留的

230
00:07:56,350 --> 00:07:58,820
smallest value of k that ensures that 99% of the variance is retained.
的最小值。

231
00:08:00,600 --> 00:08:01,810
And if you do
如果你这样做

232
00:08:02,000 --> 00:08:02,790
this, then you need to call
那么你需要调用

233
00:08:03,170 --> 00:08:04,660
the SVD function only once.
一次SVD。

234
00:08:04,970 --> 00:08:05,830
Because that gives you the
因为这可以给你

235
00:08:06,010 --> 00:08:07,060
S matrix and once you
矩阵S，一旦你

236
00:08:07,090 --> 00:08:08,350
have the S matrix, you can
获得矩阵S，你可以

237
00:08:08,490 --> 00:08:09,540
then just keep on doing
继续进行这种

238
00:08:09,770 --> 00:08:11,370
this calculation by increasing
计算通过增加

239
00:08:11,930 --> 00:08:12,910
the value of K in the
分子中

240
00:08:13,070 --> 00:08:14,450
numerator and so you
K的值，而且你

241
00:08:14,560 --> 00:08:16,290
don't need keep to calling SVD over
不需要反复调用SVD

242
00:08:16,540 --> 00:08:18,620
and over again to test out the different values of K.
来测试出K的不同值

243
00:08:18,910 --> 00:08:20,030
So this procedure is much more
所以此过程是更

244
00:08:20,150 --> 00:08:21,700
efficient, and this can
高效的，这可以

245
00:08:21,910 --> 00:08:24,020
allow you to select the
允许你选择

246
00:08:24,090 --> 00:08:25,890
value of K without needing
k的值而不用

247
00:08:26,260 --> 00:08:27,620
to run PCA from scratch
从头到尾

248
00:08:28,030 --> 00:08:30,650
over and over. You just run SVD once, this
重复运行PCA。只需计算一次奇异值分解，这

249
00:08:30,850 --> 00:08:32,350
gives you all of these diagonal numbers,
给你所有这些对角线的数字，

250
00:08:32,780 --> 00:08:35,090
all of these numbers S11, S22 down to SNN,
所有这些数字充S11，S22一直到Snn的，

251
00:08:35,780 --> 00:08:36,820
and then you can
然后你就可以

252
00:08:36,920 --> 00:08:38,440
just you know, vary K
你知道的，变化K

253
00:08:38,730 --> 00:08:40,740
in this expression to find
在这个表达式中来查找

254
00:08:41,010 --> 00:08:42,250
the smallest value of K, so
k的最小值，使得

255
00:08:43,140 --> 00:08:44,030
that 99% of the variance is retained.
99％的方差被保留。

256
00:08:44,850 --> 00:08:45,870
So to summarize, the way
总结一下，

257
00:08:46,050 --> 00:08:47,850
that I often use, the
我经常使用的方法，

258
00:08:47,970 --> 00:08:49,050
way that I often choose K
我一般选择K的方法，

259
00:08:49,420 --> 00:08:50,790
when I am using PCA for compression
当我使用PCA进行压缩时，

260
00:08:51,120 --> 00:08:52,590
is I would call SVD once
我会在协方差矩阵上进行一次

261
00:08:52,950 --> 00:08:54,480
in the covariance matrix, and then
SVD，然后

262
00:08:54,540 --> 00:08:55,750
I would use this formula and
我会用这个公式，

263
00:08:56,030 --> 00:08:57,930
pick the smallest value of
挑选k的最小值

264
00:08:58,020 --> 00:09:00,390
K for which this expression is satisfied.
使得k满足这个公式。

265
00:09:01,580 --> 00:09:02,560
And by the way, even if you
顺便说一句，即使你

266
00:09:02,650 --> 00:09:03,850
were to pick some different value
是挑选一些不同的k

267
00:09:04,180 --> 00:09:04,960
of K, even if you were
值，即使你

268
00:09:05,000 --> 00:09:05,920
to pick the value of K
手动选择K值，

269
00:09:06,090 --> 00:09:07,250
manually, you know maybe you
你知道，也许你

270
00:09:07,300 --> 00:09:08,620
have a thousand dimensional data
有一千个维度数据

271
00:09:09,540 --> 00:09:11,590
and I just want to choose K equals one hundred.
我只是想选择K=100。

272
00:09:12,430 --> 00:09:13,450
Then, if you want to explain
然后，如果你想解释

273
00:09:13,690 --> 00:09:14,760
to others what you just did,
给别人你刚才做了什么，

274
00:09:15,230 --> 00:09:17,070
a good way to explain the performance
一个向他们

275
00:09:17,750 --> 00:09:18,910
of your implementation of PCA to
解释你的PCA算法性能的好方法是

276
00:09:19,220 --> 00:09:20,300
them, is actually to take
其实是取

277
00:09:20,540 --> 00:09:21,670
this quantity and compute what
这个量，并计算结果

278
00:09:21,890 --> 00:09:23,000
this is, and that will
这将

279
00:09:23,110 --> 00:09:25,770
tell you what was the percentage of variance retained.
告诉你方差被保留的百分比。

280
00:09:26,300 --> 00:09:28,070
And if you report that number, then,
如果你报告这个数字，那么，

281
00:09:28,340 --> 00:09:29,720
you know, people that are familiar
你知道，熟悉PCA的人

282
00:09:30,100 --> 00:09:31,610
with PCA, and people can
他们可以

283
00:09:31,880 --> 00:09:33,020
use this to get a
通过这个来

284
00:09:33,080 --> 00:09:34,560
good understanding of how well
很好的理解

285
00:09:34,900 --> 00:09:37,340
your hundred dimensional representation is
你的100维表示是如何

286
00:09:37,690 --> 00:09:39,270
approximating your original data
逼近原始数据的

287
00:09:39,580 --> 00:09:41,300
set, because there's 99% of variance retained.
因为有99％的方差保留。

288
00:09:41,990 --> 00:09:44,140
That's really a measure of your
这真的是对你的构造误差的

289
00:09:44,360 --> 00:09:45,860
square of construction error, that
好的度量，那个比值

290
00:09:46,240 --> 00:09:47,870
ratio being 0.01, just
为0.01，正好

291
00:09:48,430 --> 00:09:49,940
gives people a good intuitive
给人一种很好的直观的

292
00:09:50,430 --> 00:09:51,820
sense of whether your implementation
感受，是否你的PCA

293
00:09:52,580 --> 00:09:53,840
of PCA is finding a
找到了一个

294
00:09:54,000 --> 00:09:56,530
good approximation of your original data set.
对原始数据集的良好近似。

295
00:09:58,440 --> 00:09:59,600
So hopefully, that gives you
所以希望，那给你

296
00:09:59,800 --> 00:10:01,260
an efficient procedure for choosing
一个高效的步骤去选择

297
00:10:01,850 --> 00:10:02,800
the number K. For choosing
K，将你的数据维数

298
00:10:03,260 --> 00:10:04,940
what dimension to reduce your
进行缩减，

299
00:10:05,160 --> 00:10:06,630
data to, and if
并且如果

300
00:10:06,750 --> 00:10:07,830
you apply PCA to very
你将PCA应用到

301
00:10:07,970 --> 00:10:09,740
high dimensional data sets, you know, to like
高维数据集，你知道，比方说

302
00:10:09,990 --> 00:10:11,570
a thousand dimensional data, very often,
1000维数据，很常见的维数，

303
00:10:11,980 --> 00:10:13,340
just because data sets tend
只是因为数据集往往

304
00:10:13,530 --> 00:10:14,720
to have highly correlated
有高度相关

305
00:10:15,070 --> 00:10:16,140
features, this is just a
的特征，这仅仅是一个

306
00:10:16,280 --> 00:10:17,190
property of most of the data sets you see,
你看到的大部分数据集的一个属性，

307
00:10:18,440 --> 00:10:19,420
you often find that PCA
你经常会发现PCA

308
00:10:20,040 --> 00:10:21,610
will be able to retain ninety nine
将可保留99

309
00:10:21,840 --> 00:10:22,940
per cent of the variance or say,
的方差或者说，

310
00:10:23,110 --> 00:10:24,440
ninety five ninety nine, some
95，99，一些

311
00:10:24,720 --> 00:10:25,910
high fraction of the variance,
高比例的方差，

312
00:10:26,360 --> 00:10:27,580
even while compressing the data
即使是因为一个很大的因素

313
00:10:28,560 --> 00:10:29,720
by a very large factor.
对数据进行压缩

