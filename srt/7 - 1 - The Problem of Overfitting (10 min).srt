1
00:00:00,360 --> 00:00:01,753
By now, you've seen a
到现在为止 你已经见识了
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,760 --> 00:00:04,097
couple different learning algorithms, linear
几种不同的学习算法

3
00:00:04,097 --> 00:00:06,504
regression and logistic regression.
包括线性回归和逻辑回归

4
00:00:06,510 --> 00:00:08,583
They work well for many problems,
它们能够有效地解决许多问题

5
00:00:08,583 --> 00:00:09,684
but when you apply them
但是当将它们应用到

6
00:00:09,684 --> 00:00:11,903
to certain machine learning applications, they
某些特定的机器学习应用时

7
00:00:11,903 --> 00:00:13,889
can run into a problem called
会遇到过度拟合(over-fitting)的问题

8
00:00:13,900 --> 00:00:18,052
overfitting that can cause them to perform very poorly.
可能会导致它们效果很差

9
00:00:18,052 --> 00:00:18,866
What I'd like to do in
在这段视频中

10
00:00:18,866 --> 00:00:20,393
this video is explain to
我将为你解释

11
00:00:20,393 --> 00:00:22,400
you what is this overfitting
什么是过度拟合问题

12
00:00:22,400 --> 00:00:24,083
problem, and in the
并且

13
00:00:24,083 --> 00:00:25,861
next few videos after this,
在此之后接下来的几个视频中

14
00:00:25,861 --> 00:00:27,759
we'll talk about a technique called
我们将谈论一种

15
00:00:27,760 --> 00:00:29,787
regularization, that will allow
称为正则化(regularization)的技术

16
00:00:29,787 --> 00:00:31,529
us to ameliorate or to
它可以改善或者

17
00:00:31,529 --> 00:00:33,607
reduce this overfitting problem and
减少过度拟合问题

18
00:00:33,607 --> 00:00:36,844
get these learning algorithms to maybe work much better.
以使学习算法更好实现

19
00:00:36,860 --> 00:00:39,607
So what is overfitting?
那么什么是过度拟合呢？

20
00:00:39,607 --> 00:00:41,616
Let's keep using our running
让我们继续使用

21
00:00:41,620 --> 00:00:44,030
example of predicting housing
那个用线性回归

22
00:00:44,050 --> 00:00:46,146
prices with linear regression
来预测房价的例子

23
00:00:46,146 --> 00:00:47,123
where we want to predict the
我们通过建立

24
00:00:47,123 --> 00:00:50,730
price as a function of the size of the house.
以住房面积为自变量的函数来预测房价

25
00:00:50,730 --> 00:00:51,870
One thing we could do is
我们可以

26
00:00:51,910 --> 00:00:53,620
fit a linear function to
对该数据做线性回归

27
00:00:53,620 --> 00:00:54,892
this data, and if we
如果这么做

28
00:00:54,892 --> 00:00:56,296
do that, maybe we get
我们也许能够获得

29
00:00:56,296 --> 00:00:58,913
that sort of straight line fit to the data.
拟合数据的这样一条直线

30
00:00:58,913 --> 00:01:01,012
But this isn't a very good model.
但是 这不是一个很好的模型

31
00:01:01,012 --> 00:01:02,543
Looking at the data, it seems
我们看看这些数据

32
00:01:02,560 --> 00:01:04,100
pretty clear that as the
很明显

33
00:01:04,100 --> 00:01:06,274
size of the housing increases, the
随着房子面积增大

34
00:01:06,274 --> 00:01:08,268
housing prices plateau, or kind
住房价格的变化趋于稳定

35
00:01:08,270 --> 00:01:11,721
of flattens out as we move to the right and so
或者越往右越平缓

36
00:01:11,740 --> 00:01:14,020
this algorithm does not
因此该算法

37
00:01:14,020 --> 00:01:15,898
fit the training and we
没有很好拟合训练数据

38
00:01:15,898 --> 00:01:19,166
call this problem underfitting, and
我们把这个问题称为欠拟合(underfitting)

39
00:01:19,180 --> 00:01:20,494
another term for this is
这个问题的另一个术语叫做

40
00:01:20,500 --> 00:01:24,666
that this algorithm has high bias.
高偏差(bias)

41
00:01:25,140 --> 00:01:26,841
Both of these roughly
这两种说法大致相似

42
00:01:26,890 --> 00:01:30,760
mean that it's just not even fitting the training data very well.
意思是它只是没有很好地拟合训练数据

43
00:01:30,760 --> 00:01:32,328
The term is kind of
这个词是

44
00:01:32,328 --> 00:01:34,515
a historical or technical one,
过去传下来的一个专业名词

45
00:01:34,515 --> 00:01:36,109
but the idea is that
它的意思是

46
00:01:36,110 --> 00:01:37,303
if a fitting a straight line to
如果拟合一条直线

47
00:01:37,303 --> 00:01:38,909
the data, then, it's as
到训练数据

48
00:01:38,920 --> 00:01:40,290
if the algorithm has a
就好像算法

49
00:01:40,330 --> 00:01:42,638
very strong preconception, or a
有一个很强的偏见

50
00:01:42,638 --> 00:01:44,633
very strong bias that housing
或者说非常大的偏差

51
00:01:44,650 --> 00:01:46,339
prices are going to vary
因为该算法认为房子价格与面积仅仅线性相关

52
00:01:46,339 --> 00:01:49,988
linearly with their size and despite the data to the contrary.
尽管与该数据的事实相反

53
00:01:50,000 --> 00:01:51,281
Despite the evidence of the
尽管相反的证据

54
00:01:51,290 --> 00:01:54,174
contrary is preconceptions still
被事前定义为

55
00:01:54,174 --> 00:01:55,413
are bias, still closes
偏差 它还是接近于

56
00:01:55,440 --> 00:01:56,974
it to fit a straight line
拟合一条直线

57
00:01:56,974 --> 00:02:00,638
and this ends up being a poor fit to the data.
而此法最终导致拟合数据效果很差

58
00:02:00,638 --> 00:02:02,173
Now, in the middle, we could
我们现在可以在中间

59
00:02:02,210 --> 00:02:04,626
fit a quadratic functions enter and,
加入一个二次项

60
00:02:04,626 --> 00:02:06,222
with this data set, we fit the
在这组数据中

61
00:02:06,222 --> 00:02:07,793
quadratic function, maybe, we get
我们用二次函数来拟合它

62
00:02:07,810 --> 00:02:10,211
that kind of curve
然后可以拟合出一条曲线

63
00:02:10,211 --> 00:02:14,361
and, that works pretty well.
事实证明这个拟合效果很好

64
00:02:14,361 --> 00:02:17,543
And, at the other extreme, would be if we were to fit, say a fourth other polynomial to the data.
另一个极端情况是 如果我们拟合一个四次多项式

65
00:02:17,550 --> 00:02:19,442
So, here we have five parameters,
因此在这里我们有五个参数

66
00:02:19,470 --> 00:02:23,196
theta zero through theta four,
θ0到θ4

67
00:02:23,210 --> 00:02:23,926
and, with that, we can actually fill a curve
这样我们可以拟合一条曲线

68
00:02:23,926 --> 00:02:26,727
that process through all five of our training examples.
通过我们的五个训练样本

69
00:02:26,727 --> 00:02:29,507
You might get a curve that looks like this.
你可以得到看上去如此的一条曲线

70
00:02:31,260 --> 00:02:32,454
That, on the one
一方面

71
00:02:32,460 --> 00:02:33,791
hand, seems to do
似乎

72
00:02:33,791 --> 00:02:35,052
a very good job fitting the
对训练数据

73
00:02:35,052 --> 00:02:36,291
training set and, that is
做了一个很好的拟合

74
00:02:36,291 --> 00:02:38,269
processed through all of my data, at least.
因为这条曲线通过了所有的训练实例

75
00:02:38,270 --> 00:02:40,284
But, this is still a very wiggly curve, right?
但是 这仍然是一条扭曲的曲线 对吧？

76
00:02:40,300 --> 00:02:41,660
So, it's going up and down all
它不停上下波动

77
00:02:41,660 --> 00:02:43,430
over the place, and, we don't actually
因此事实上

78
00:02:43,430 --> 00:02:46,996
think that's such a good model for predicting housing prices.
我们并不认为它是一个预测房价的好模型

79
00:02:47,000 --> 00:02:48,924
So, this problem we
所以 这个问题我们把他叫做

80
00:02:48,924 --> 00:02:51,967
call overfitting, and, another
过度拟合或过拟合(overfitting)

81
00:02:51,970 --> 00:02:53,165
term for this is that
另一个描述该问题的术语是

82
00:02:53,170 --> 00:02:57,304
this algorithm has high variance..
高方差(variance)

83
00:02:57,890 --> 00:02:59,951
The term high variance is another
高方差是另一个

84
00:02:59,951 --> 00:03:02,110
historical or technical one.
历史上的叫法

85
00:03:02,130 --> 00:03:03,797
But, the intuition is that,
但是 从第一印象上来说

86
00:03:03,800 --> 00:03:05,080
if we're fitting such a high
如果我们拟合一个

87
00:03:05,080 --> 00:03:07,326
order polynomial, then, the
高阶多项式 那么

88
00:03:07,330 --> 00:03:08,603
hypothesis can fit, you know,
这个函数能很好的拟合训练集

89
00:03:08,620 --> 00:03:09,584
it's almost as if it can
能拟合几乎所有的

90
00:03:09,584 --> 00:03:11,995
fit almost any function and
训练数据

91
00:03:11,995 --> 00:03:14,159
this face of possible hypothesis
这就面临可能函数太过庞大的问题

92
00:03:14,159 --> 00:03:16,601
is just too large, it's too variable.
变量太多

93
00:03:16,610 --> 00:03:18,052
And we don't have enough data
同时如果我们没有足够的数据

94
00:03:18,052 --> 00:03:19,279
to constrain it to give
去约束这个变量过多的模型

95
00:03:19,279 --> 00:03:22,714
us a good hypothesis so that's called overfitting.
那么这就是过度拟合

96
00:03:22,740 --> 00:03:24,340
And in the middle, there isn't really
在两者之间的情况 叫"刚好合适"

97
00:03:24,350 --> 00:03:26,990
a name but I'm just going to write, you know, just right.
这并不是一个真正的名词 我只是把它写在这里

98
00:03:26,990 --> 00:03:29,911
Where a second degree polynomial, quadratic function
这个二次多项式 二次函数

99
00:03:29,911 --> 00:03:32,559
seems to be just right for fitting this data.
可以说是恰好拟合这些数据

100
00:03:32,559 --> 00:03:34,684
To recap a bit the
概括地说

101
00:03:34,690 --> 00:03:37,042
problem of over fitting comes
过度拟合的问题

102
00:03:37,042 --> 00:03:38,258
when if we have
将会在变量过多的时候

103
00:03:38,258 --> 00:03:40,729
too many features, then to
发生

104
00:03:40,729 --> 00:03:43,881
learn hypothesis may fit the training side very well.
这种时候训练出的方程总能很好的拟合训练数据

105
00:03:43,881 --> 00:03:46,023
So, your cost function
所以 你的代价函数

106
00:03:46,023 --> 00:03:47,344
may actually be very close
实际上可能非常接近于0

107
00:03:47,344 --> 00:03:48,446
to zero or may be
或者

108
00:03:48,446 --> 00:03:50,750
even zero exactly, but you
就是0

109
00:03:50,750 --> 00:03:52,063
may then end up with a
但是

110
00:03:52,063 --> 00:03:53,950
curve like this that, you
这样的曲线

111
00:03:53,950 --> 00:03:55,314
know tries too hard to
它千方百计的拟合于训练数据

112
00:03:55,314 --> 00:03:57,103
fit the training set, so that it
这样导致

113
00:03:57,110 --> 00:03:59,233
even fails to generalize to
它无法泛化到

114
00:03:59,250 --> 00:04:01,117
new examples and fails to
新的数据样本中

115
00:04:01,120 --> 00:04:03,018
predict prices on new examples
以至于无法预测新样本价格

116
00:04:03,050 --> 00:04:04,337
as well, and here the
在这里

117
00:04:04,350 --> 00:04:06,853
term generalized refers to
术语"泛化"

118
00:04:06,853 --> 00:04:10,868
how well a hypothesis applies even to new examples.
指的是一个假设模型能够应用到新样本的能力

119
00:04:10,868 --> 00:04:12,274
That is to data to
新样本数据是

120
00:04:12,320 --> 00:04:16,467
houses that it has not seen in the training set.
没有出现在训练集中的房子

121
00:04:16,600 --> 00:04:17,910
On this slide, we looked at
在这张幻灯片上 我们看到了

122
00:04:17,910 --> 00:04:20,802
over fitting for the case of linear regression.
线性回归情况下的过拟合

123
00:04:20,810 --> 00:04:24,182
A similar thing can apply to logistic regression as well.
类似的方法同样可以应用到逻辑回归

124
00:04:24,190 --> 00:04:26,090
Here is a logistic regression
这里是一个以x1与x2为变量的

125
00:04:26,090 --> 00:04:28,871
example with two features X1 and x2.
逻辑回归

126
00:04:28,910 --> 00:04:30,136
One thing we could do, is
我们可以做的就是

127
00:04:30,140 --> 00:04:31,522
fit logistic regression with
用这样一个简单的假设模型

128
00:04:31,522 --> 00:04:34,518
just a simple hypothesis like this,
来拟合逻辑回归

129
00:04:34,530 --> 00:04:38,076
where, as usual, G is my sigmoid function.
和以前一样 字母g代表S型函数

130
00:04:38,120 --> 00:04:39,334
And if you do that, you end up
如果这样做

131
00:04:39,334 --> 00:04:41,593
with a hypothesis, trying to
你会得到一个假设模型

132
00:04:41,600 --> 00:04:42,923
use, maybe, just a straight
这个假设模型是一条直线

133
00:04:42,923 --> 00:04:45,713
line to separate the positive and the negative examples.
它直接分开了正样本和负样本

134
00:04:45,713 --> 00:04:49,071
And this doesn't look like a very good fit to the hypothesis.
但这个模型并不能够很好的拟合数据

135
00:04:49,100 --> 00:04:50,659
So, once again, this
因此

136
00:04:50,659 --> 00:04:52,577
is an example of underfitting
这又是一个欠拟合的例子

137
00:04:52,577 --> 00:04:56,040
or of the hypothesis having high bias.
或者说假设模型具有高偏差

138
00:04:56,210 --> 00:04:57,504
In contrast, if you were
相比之下 如果

139
00:04:57,504 --> 00:04:59,146
to add to your features
如果再加入一些变量

140
00:04:59,170 --> 00:05:01,032
these quadratic terms, then,
比如这些二次项

141
00:05:01,032 --> 00:05:02,613
you could get a decision
那么你可以得到一个判定边界

142
00:05:02,613 --> 00:05:05,620
boundary that might look more like this.
像这样

143
00:05:05,620 --> 00:05:07,784
And, you know, that's a pretty good fit to the data.
这样就很好的拟合了数据

144
00:05:07,784 --> 00:05:10,838
Probably, about as
这很可能

145
00:05:10,860 --> 00:05:13,991
good as we could get, on this training set.
是训练集的最好拟合结果

146
00:05:14,010 --> 00:05:15,157
And, finally, at the other
最后

147
00:05:15,170 --> 00:05:16,169
extreme, if you were to
在另一种极端情况下

148
00:05:16,169 --> 00:05:18,207
fit a very high-order polynomial, if
如果你用高阶多项式来拟合数据

149
00:05:18,207 --> 00:05:20,036
you were to generate lots of
你加入了很多

150
00:05:20,036 --> 00:05:22,461
high-order polynomial terms of speeches,
高阶项

151
00:05:22,490 --> 00:05:24,730
then, logistical regression may contort
那么逻辑回归可能发生自身扭曲

152
00:05:24,750 --> 00:05:26,551
itself, may try really
它千方百计的

153
00:05:26,560 --> 00:05:28,233
hard to find a
形成这样一个

154
00:05:28,233 --> 00:05:31,742
decision boundary that fits
判定边界

155
00:05:31,742 --> 00:05:33,013
your training data or go
来拟合你的训练数据

156
00:05:33,030 --> 00:05:35,006
to great lengths to contort itself,
以至于成为一条扭曲的曲线

157
00:05:35,006 --> 00:05:37,689
to fit every single training example well.
使其能够拟合每一个训练集中的样本

158
00:05:37,700 --> 00:05:38,757
And, you know, if the
而且

159
00:05:38,757 --> 00:05:39,547
features X1 and
如果x1和x2

160
00:05:39,550 --> 00:05:41,435
X2 offer predicting, maybe,
能够预测

161
00:05:41,435 --> 00:05:43,350
the cancer to the,
癌症

162
00:05:43,390 --> 00:05:46,448
you know, cancer is a malignant, benign breast tumors.
你知道 癌症是一种恶性肿瘤 同时肿瘤也可能是良性

163
00:05:46,448 --> 00:05:47,988
This doesn't, this really doesn't
确实

164
00:05:47,988 --> 00:05:51,893
look like a very good hypothesis, for making predictions.
这个假设模型不是一个很好的预测

165
00:05:51,930 --> 00:05:53,463
And so, once again, this is
因此

166
00:05:53,463 --> 00:05:55,432
an instance of overfitting
这又是一个过拟合例子

167
00:05:55,432 --> 00:05:57,128
and, of a hypothesis having
是一个

168
00:05:57,128 --> 00:05:59,403
high variance and not really,
有高方差的假设模型

169
00:05:59,403 --> 00:06:04,243
and, being unlikely to generalize well to new examples.
并且不能够很好泛化到新样本

170
00:06:04,560 --> 00:06:06,158
Later, in this course, when we
在今后课程中

171
00:06:06,158 --> 00:06:08,453
talk about debugging and diagnosing
我们会讲到调试和诊断

172
00:06:08,460 --> 00:06:09,794
things that can go wrong with
诊断出导致学习算法故障的东西

173
00:06:09,810 --> 00:06:11,490
learning algorithms, we'll give you
我们告诉你如何用

174
00:06:11,490 --> 00:06:13,297
specific tools to recognize
专门的工具来识别

175
00:06:13,297 --> 00:06:14,953
when overfitting and, also,
过拟合

176
00:06:14,953 --> 00:06:17,503
when underfitting may be occurring.
和可能发生的欠拟合

177
00:06:17,503 --> 00:06:18,775
But, for now, lets talk about
但是 现在 让我们谈谈

178
00:06:18,780 --> 00:06:20,342
the problem of, if we
过拟合

179
00:06:20,360 --> 00:06:22,206
think overfitting is occurring,
的问题

180
00:06:22,250 --> 00:06:24,864
what can we do to address it?
我们怎么样解决呢

181
00:06:24,864 --> 00:06:26,640
In the previous examples, we had
在前面的例子中

182
00:06:26,660 --> 00:06:28,701
one or two dimensional data so,
当我们使用一维或二维数据时

183
00:06:28,701 --> 00:06:31,335
we could just plot the hypothesis and see what was going
我们可以通过绘出假设模型的图像来研究问题所在

184
00:06:31,335 --> 00:06:34,612
on and select the appropriate degree polynomial.
再选择合适的多项式来拟合数据

185
00:06:34,620 --> 00:06:36,836
So, earlier for the housing
因此 以之前的房屋价格为例

186
00:06:36,836 --> 00:06:38,405
prices example, we could just
我们可以

187
00:06:38,410 --> 00:06:40,597
plot the hypothesis and, you
绘制假设模型的图像

188
00:06:40,600 --> 00:06:41,628
know, maybe see that it
就能看到

189
00:06:41,628 --> 00:06:42,830
was fitting the sort of
模型的曲线

190
00:06:42,830 --> 00:06:46,339
very wiggly function that goes all over the place to predict housing prices.
非常扭曲并通过所有样本房价

191
00:06:46,339 --> 00:06:47,701
And we could then use figures
我们可以通过绘制这样的图形

192
00:06:47,740 --> 00:06:50,667
like these to select an appropriate degree polynomial.
来选择合适的多项式阶次

193
00:06:50,680 --> 00:06:54,166
So plotting the hypothesis, could
因此绘制假设模型曲线

194
00:06:54,166 --> 00:06:55,728
be one way to try to
可以作为决定多项式阶次

195
00:06:55,750 --> 00:06:58,160
decide what degree polynomial to use.
的一种方法

196
00:06:58,160 --> 00:07:00,163
But that doesn't always work.
但是这并不是总是有用的

197
00:07:00,180 --> 00:07:02,019
And, in fact more often we
而且事实上更多的时候我们

198
00:07:02,019 --> 00:07:06,075
may have learning problems that where we just have a lot of features.
会遇到有很多变量的假设模型

199
00:07:06,075 --> 00:07:07,563
And there is not
并且

200
00:07:07,563 --> 00:07:10,599
just a matter of selecting what degree polynomial.
这不仅仅是选择多项式阶次的问题

201
00:07:10,630 --> 00:07:12,147
And, in fact, when we
事实上 当我们

202
00:07:12,170 --> 00:07:13,779
have so many features, it also
有这么多的特征变量

203
00:07:13,779 --> 00:07:15,593
becomes much harder to plot
这也使得绘图变得更难

204
00:07:15,630 --> 00:07:17,698
the data and it becomes
并且

205
00:07:17,710 --> 00:07:19,211
much harder to visualize it,
更难使其可视化

206
00:07:19,211 --> 00:07:22,396
to decide what features to keep or not.
因此并不能通过这种方法决定保留哪些特征变量

207
00:07:22,420 --> 00:07:24,142
So concretely, if we're trying
具体地说 如果我们试图

208
00:07:24,160 --> 00:07:27,849
predict housing prices sometimes we can just have a lot of different features.
预测房价 同时又拥有这么多特征变量

209
00:07:27,880 --> 00:07:31,373
And all of these features seem, you know, maybe they seem kind of useful.
这些变量看上去都很有用

210
00:07:31,373 --> 00:07:32,609
But, if we have a
但是 如果我们有

211
00:07:32,609 --> 00:07:34,123
lot of features, and, very little
过多的变量 同时

212
00:07:34,123 --> 00:07:35,820
training data, then, over
只有非常少的训练数据

213
00:07:35,840 --> 00:07:37,776
fitting can become a problem.
就会出现过度拟合的问题

214
00:07:37,776 --> 00:07:39,180
In order to address over
为了解决过度拟合

215
00:07:39,180 --> 00:07:40,651
fitting, there are two
有两个办法

216
00:07:40,651 --> 00:07:43,780
main options for things that we can do.
来解决问题

217
00:07:43,780 --> 00:07:45,759
The first option is, to try
第一个办法是要尽量

218
00:07:45,770 --> 00:07:47,976
to reduce the number of features.
减少选取变量的数量

219
00:07:47,990 --> 00:07:49,337
Concretely, one thing we
具体而言

220
00:07:49,337 --> 00:07:51,383
could do is manually look through
我们可以人工检查

221
00:07:51,383 --> 00:07:53,236
the list of features, and, use
变量的条目

222
00:07:53,236 --> 00:07:54,894
that to try to decide which
并以此决定哪些变量更为重要

223
00:07:54,894 --> 00:07:57,256
are the more important features, and, therefore,
然后

224
00:07:57,256 --> 00:07:58,476
which are the features we should
决定保留哪些特征变量

225
00:07:58,476 --> 00:08:01,844
keep, and, which are the features we should throw out.
哪些应该舍弃

226
00:08:01,844 --> 00:08:03,401
Later in this course, where also
在今后的课程中

227
00:08:03,401 --> 00:08:06,018
talk about model selection algorithms.
我们会提到模型选择算法

228
00:08:06,040 --> 00:08:08,361
Which are algorithms for automatically
这种算法是为了自动选择

229
00:08:08,361 --> 00:08:09,788
deciding which features
采用哪些特征变量

230
00:08:09,800 --> 00:08:12,500
to keep and, which features to throw out.
自动舍弃不需要的变量

231
00:08:12,500 --> 00:08:13,987
This idea of reducing the
这种减少特征变量

232
00:08:13,987 --> 00:08:15,562
number of features can work
的做法是非常有效的

233
00:08:15,562 --> 00:08:17,853
well, and, can reduce over fitting.
并且可以减少过拟合的发生

234
00:08:17,853 --> 00:08:19,383
And, when we talk about model
当我们今后讲到模型选择时

235
00:08:19,383 --> 00:08:22,534
selection, we'll go into this in much greater depth.
我们将深入探讨这个问题

236
00:08:22,534 --> 00:08:24,386
But, the disadvantage is that, by
但是其缺点是

237
00:08:24,386 --> 00:08:25,603
throwing away some of the
舍弃一部分特征变量

238
00:08:25,603 --> 00:08:27,010
features, is also throwing
你也舍弃了

239
00:08:27,370 --> 00:08:30,615
away some of the information you have about the problem.
问题中的一些信息

240
00:08:30,650 --> 00:08:31,942
For example, maybe, all of
例如 也许所有的

241
00:08:31,942 --> 00:08:33,760
those features are actually useful
特征变量

242
00:08:33,780 --> 00:08:35,050
for predicting the price of a
对于预测房价都是有用的

243
00:08:35,070 --> 00:08:36,636
house, so, maybe, we don't actually
我们实际上并不想

244
00:08:36,640 --> 00:08:37,687
want to throw some of
舍弃一些信息

245
00:08:37,687 --> 00:08:40,990
our information or throw some of our features away.
或者舍弃这些特征变量

246
00:08:41,540 --> 00:08:44,515
The second option, which we'll
第二个选择

247
00:08:44,515 --> 00:08:45,995
talk about in the
我们将在接下来的视频中讨论

248
00:08:46,010 --> 00:08:49,268
next few videos, is regularization.
就是正则化

249
00:08:49,268 --> 00:08:50,390
Here, we're going to keep
正则化中我们将保留

250
00:08:50,390 --> 00:08:52,579
all the features, but we're
所有的特征变量

251
00:08:52,579 --> 00:08:55,063
going to reduce the magnitude
但是数量级

252
00:08:55,063 --> 00:08:56,506
or the values of the parameters
或参数数值的大小

253
00:08:56,520 --> 00:08:58,745
theta J. And, this
θ(j)

254
00:08:58,750 --> 00:09:00,690
method works well, we'll see,
这个方法非常有效

255
00:09:00,690 --> 00:09:01,925
when we have a lot of
当我们有很多特征变量时

256
00:09:01,925 --> 00:09:03,822
features, each of which contributes
其中每一个变量

257
00:09:03,822 --> 00:09:05,502
a little bit to predicting
都能对预测产生一点影响

258
00:09:05,502 --> 00:09:07,723
the value of Y, like we
y的值

259
00:09:07,740 --> 00:09:10,283
saw in the housing price prediction example.
正如我们在房价的例子中看到的那样

260
00:09:10,283 --> 00:09:11,413
Where we could have a lot
在那里我们可以有很多特征变量

261
00:09:11,413 --> 00:09:12,720
of features, each of which
其中每一个变量

262
00:09:12,750 --> 00:09:16,902
are, you know, somewhat useful, so, maybe, we don't want to throw them away.
都是有用的 因此我们不希望把它们删掉

263
00:09:16,930 --> 00:09:19,247
So, this subscribes the
这就导致了

264
00:09:19,250 --> 00:09:22,790
idea of regularization at a very high level.
正则化概念的发生

265
00:09:22,790 --> 00:09:24,354
And, I realize that, all
我知道

266
00:09:24,360 --> 00:09:26,763
of these details probably don't make sense to you yet.
这些东西你们现在可能还听不懂

267
00:09:26,763 --> 00:09:28,316
But, in the next video, we'll
但是在接下来的视频中

268
00:09:28,316 --> 00:09:30,960
start to formulate exactly how
我们将开始详细讲述

269
00:09:30,960 --> 00:09:35,117
to apply regularization and, exactly what regularization means.
怎样应用正则化和什么叫做正则化均值

270
00:09:35,140 --> 00:09:36,810
And, then we'll start to
然后我们将开始

271
00:09:36,810 --> 00:09:38,310
figure out, how to use this,
讲解怎样使用正则化

272
00:09:38,310 --> 00:09:40,412
to make how learning algorithms work
怎样使学习算法正常工作

273
00:09:40,412 --> 00:09:42,460
well and avoid overfitting.
并避免过拟合

