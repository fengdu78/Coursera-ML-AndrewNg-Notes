1
00:00:00,440 --> 00:00:01,400
In this and in the
在这节课和接下来的课程中
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,480 --> 00:00:02,640
next set of videos, I'd like
我将给大家介绍

3
00:00:02,780 --> 00:00:04,270
to tell you about a learning
一种叫“神经网络”(Neural Network)

4
00:00:04,550 --> 00:00:06,110
algorithm called a Neural Network.
的机器学习算法

5
00:00:07,190 --> 00:00:07,900
We're going to first talk about
我们将首先讨论

6
00:00:08,079 --> 00:00:09,330
the representation and then
神经网络的表层结构

7
00:00:09,600 --> 00:00:10,390
in the next set of videos
在后续课程中

8
00:00:10,410 --> 00:00:12,160
talk about learning algorithms for it.
再来具体讨论的学习算法

9
00:00:12,660 --> 00:00:14,070
Neutral networks is actually
神经网络实际上是一个

10
00:00:14,510 --> 00:00:15,870
a pretty old idea, but had
相对古老的算法

11
00:00:16,290 --> 00:00:17,680
fallen out of favor for a while.
并且后来沉寂了一段时间

12
00:00:18,200 --> 00:00:19,270
But today, it is the
不过到了现在

13
00:00:19,580 --> 00:00:20,820
state of the art technique for
它又成为许多机器学习问题

14
00:00:21,090 --> 00:00:22,390
many different machine learning problems.
的首选技术

15
00:00:23,740 --> 00:00:25,740
So why do we need yet another learning algorithm?
不过我们为什么还需要这个学习算法？

16
00:00:26,300 --> 00:00:28,030
We already have linear regression and
我们已经有线性回归和逻辑回归算法了

17
00:00:28,180 --> 00:00:31,260
we have logistic regression, so why do we need, you know, neural networks?
为什么还要研究神经网络？

18
00:00:32,280 --> 00:00:34,260
In order to motivate the discussion
为了阐述研究

19
00:00:34,790 --> 00:00:35,970
of neural networks, let me
神经网络算法的目的

20
00:00:36,120 --> 00:00:37,130
start by showing you a few
我们首先来看几个

21
00:00:37,310 --> 00:00:38,720
examples of machine learning
机器学习问题作为例子

22
00:00:38,930 --> 00:00:40,100
problems where we need
这几个问题的解决

23
00:00:40,300 --> 00:00:41,850
to learn complex non-linear hypotheses.
都依赖于研究复杂的非线性分类器

24
00:00:43,850 --> 00:00:45,650
Consider a supervised learning classification
考虑这个监督学习分类的问题

25
00:00:46,530 --> 00:00:48,440
problem where you have a training set like this.
我们已经有了对应的训练集

26
00:00:49,280 --> 00:00:50,530
If you want to apply logistic
如果利用逻辑回归算法

27
00:00:50,960 --> 00:00:52,710
regression to this problem, one
来解决这个问题

28
00:00:52,900 --> 00:00:54,250
thing you could do is apply
首先需要构造

29
00:00:54,660 --> 00:00:56,140
logistic regression with a
一个包含很多非线性项的

30
00:00:56,190 --> 00:00:57,720
lot of nonlinear features like that.
逻辑回归函数

31
00:00:58,170 --> 00:00:59,580
So here, g as usual
这里g仍是s型函数 (即f(x)=1/(1+e^-x) )

32
00:01:00,070 --> 00:01:01,710
is the sigmoid function, and we
我们能让函数

33
00:01:01,780 --> 00:01:04,680
can include lots of polynomial terms like these.
包含很多像这样的多项式项

34
00:01:05,450 --> 00:01:06,790
And, if you include enough polynomial
事实上  当多项式项数足够多时

35
00:01:07,370 --> 00:01:08,280
terms then, you know, maybe
那么可能

36
00:01:08,950 --> 00:01:10,280
you can get a hypotheses
你能够得到一个

37
00:01:11,600 --> 00:01:13,780
that separates the positive and negative examples.
分开正样本和负样本的分界线

38
00:01:14,630 --> 00:01:16,080
This particular method works well
当只有两项时

39
00:01:16,470 --> 00:01:18,400
when you have only, say, two
比如 x1 x2

40
00:01:18,620 --> 00:01:20,180
features - x1 and x2
这种方法确实能得到不错的结果

41
00:01:20,190 --> 00:01:20,980
- because you can then include
因为你可以

42
00:01:21,500 --> 00:01:22,880
all those polynomial terms of
把x1和x2的所有组合

43
00:01:23,400 --> 00:01:24,620
x1 and x2.
都包含到多项式中

44
00:01:24,810 --> 00:01:26,280
But for many interesting machine learning
但是对于许多

45
00:01:26,520 --> 00:01:27,730
problems would have a
复杂的机器学习问题

46
00:01:27,910 --> 00:01:29,230
lot more features than just two.
涉及的项往往多于两项

47
00:01:30,780 --> 00:01:31,760
We've been talking for a while
我们之前已经讨论过

48
00:01:32,320 --> 00:01:34,560
about housing prediction, and suppose
房价预测的问题

49
00:01:35,130 --> 00:01:36,990
you have a housing classification
假设现在要处理的是

50
00:01:38,020 --> 00:01:39,280
problem rather than a
关于住房的分类问题

51
00:01:39,390 --> 00:01:41,170
regression problem, like maybe
而不是一个回归问题

52
00:01:41,580 --> 00:01:43,350
if you have different features of
假设你对一栋房子的多方面特点

53
00:01:43,440 --> 00:01:44,760
a house, and you want
都有所了解

54
00:01:45,010 --> 00:01:46,000
to predict what are the
你想预测

55
00:01:46,050 --> 00:01:47,590
odds that your house will
房子在未来半年内

56
00:01:47,700 --> 00:01:48,710
be sold within the next
能被卖出去的概率

57
00:01:48,910 --> 00:01:51,040
six months, so that will be a classification problem.
这是一个分类问题

58
00:01:52,100 --> 00:01:53,060
And as we saw we can
我们可以想出

59
00:01:53,260 --> 00:01:55,130
come up with quite a
很多特征

60
00:01:55,260 --> 00:01:56,480
lot of features, maybe a hundred
对于不同的房子有可能

61
00:01:56,840 --> 00:01:58,270
different features of different houses.
就有上百个特征

62
00:02:00,130 --> 00:02:01,610
For a problem like this, if
对于这类问题

63
00:02:01,880 --> 00:02:03,260
you were to include all the
如果要包含

64
00:02:03,370 --> 00:02:04,980
quadratic terms, all of
所有的二次项

65
00:02:05,100 --> 00:02:06,260
these, even all of the
即使只包含

66
00:02:06,540 --> 00:02:07,540
quadratic that is the second
二项式或多项式的计算

67
00:02:07,930 --> 00:02:10,450
or the polynomial terms, there would be a lot of them.
最终的多项式也可能有很多项

68
00:02:10,560 --> 00:02:11,580
There would be terms like x1 squared,
比如x1^2

69
00:02:12,960 --> 00:02:17,610
x1x2, x1x3, you know, x1x4
x1x2   x1x3   x1x4

70
00:02:18,750 --> 00:02:21,880
up to x1x100 and then
直到x1x100

71
00:02:21,980 --> 00:02:23,620
you have x2 squared, x2x3
还有x2^2  x2x3

72
00:02:25,620 --> 00:02:25,980
and so on.
等等很多项

73
00:02:26,510 --> 00:02:27,770
And if you include just
因此

74
00:02:28,060 --> 00:02:29,200
the second order terms, that
即使只考虑二阶项

75
00:02:29,330 --> 00:02:30,750
is, the terms that are
也就是说

76
00:02:30,840 --> 00:02:32,090
a product of, you know,
两个项的乘积

77
00:02:32,220 --> 00:02:33,390
two of these terms, x1
x1乘以x1

78
00:02:33,510 --> 00:02:35,010
times x1 and so on, then,
等等类似于此的项

79
00:02:35,780 --> 00:02:36,920
for the case of n equals
那么  在n=100的情况下

80
00:02:38,180 --> 00:02:40,280
100, you end up with about five thousand features.
最终也有5000个二次项

81
00:02:41,890 --> 00:02:44,880
And, asymptotically, the
而且渐渐地

82
00:02:45,000 --> 00:02:46,330
number of quadratic features grows
随着特征个数n的增加

83
00:02:46,770 --> 00:02:48,670
roughly as order n
二次项的个数大约以n^2的量级增长

84
00:02:48,820 --> 00:02:50,330
squared, where n is the
其中

85
00:02:50,460 --> 00:02:52,790
number of the original features,
n是原始项的个数

86
00:02:53,370 --> 00:02:54,780
like x1 through x100 that we had.
即我们之前说过的x1到x100这些项

87
00:02:55,700 --> 00:02:58,750
And its actually closer to n squared over two.
事实上二次项的个数大约是(n^2)/2

88
00:02:59,920 --> 00:03:01,440
So including all the
因此要包含所有的

89
00:03:01,560 --> 00:03:02,920
quadratic features doesn't seem
二次项是很困难的

90
00:03:03,220 --> 00:03:04,220
like it's maybe a good
所以这可能

91
00:03:04,300 --> 00:03:05,380
idea, because that is a
不是一个好的做法

92
00:03:05,580 --> 00:03:07,050
lot of features and you
而且由于项数过多

93
00:03:07,220 --> 00:03:08,920
might up overfitting the training
最后的结果很有可能是过拟合的

94
00:03:09,330 --> 00:03:10,500
set, and it can
此外

95
00:03:10,740 --> 00:03:12,800
also be computationally expensive, you know, to
在处理这么多项时

96
00:03:14,080 --> 00:03:15,120
be working with that many features.
也存在运算量过大的问题

97
00:03:16,450 --> 00:03:17,540
One thing you could do is
当然  你也可以试试

98
00:03:17,770 --> 00:03:19,090
include only a subset of
只包含上边这些二次项的子集

99
00:03:19,290 --> 00:03:20,950
these, so if you include only the
例如 我们只考虑

100
00:03:21,050 --> 00:03:22,630
features x1 squared, x2 squared,
x1^2  x2^2

101
00:03:23,590 --> 00:03:25,180
x3 squared, up to
x3^2直到

102
00:03:25,580 --> 00:03:27,750
maybe x100 squared, then
x100^2 这些项

103
00:03:28,100 --> 00:03:29,500
the number of features is much smaller.
这样就可以将二次项的数量大幅度减少

104
00:03:29,980 --> 00:03:31,720
Here you have only 100 such
减少到只有100个二次项

105
00:03:32,070 --> 00:03:33,850
quadratic features, but this
但是由于

106
00:03:34,120 --> 00:03:35,950
is not enough features and
忽略了太多相关项

107
00:03:36,100 --> 00:03:37,170
certainly won't let you fit
在处理类似左上角的数据时

108
00:03:37,290 --> 00:03:39,330
the data set like that on the upper left.
不可能得到理想的结果

109
00:03:39,570 --> 00:03:40,550
In fact, if you include
实际上

110
00:03:41,040 --> 00:03:42,720
only these quadratic features together
如果只考虑x1的平方

111
00:03:43,170 --> 00:03:44,870
with the original x1, and
到x100的平方

112
00:03:45,350 --> 00:03:46,500
so on, up to x100 features,
这一百个二次项

113
00:03:47,460 --> 00:03:48,530
then you can actually fit very
那么你可能会

114
00:03:48,910 --> 00:03:50,210
interesting hypotheses. So, you
拟合出一些特别的假设

115
00:03:50,330 --> 00:03:52,350
can fit things like, you know, access a
比如可能拟合出

116
00:03:52,490 --> 00:03:53,860
line of the ellipses like these, but
一个椭圆状的曲线

117
00:03:55,080 --> 00:03:56,240
you certainly cannot fit a more
但是肯定不能拟合出

118
00:03:56,340 --> 00:03:57,930
complex data set like that shown here.
像左上角这个数据集的分界线

119
00:03:59,360 --> 00:04:00,530
So 5000 features seems like
所以5000个二次项看起来已经很多了

120
00:04:00,620 --> 00:04:03,090
a lot, if you were
而现在假设

121
00:04:03,230 --> 00:04:04,860
to include the cubic, or
包括三次项

122
00:04:05,140 --> 00:04:06,100
third order known of each others,
或者三阶项

123
00:04:06,440 --> 00:04:08,050
the x1, x2, x3.
例如x1 x2 x3

124
00:04:08,400 --> 00:04:09,800
You know, x1 squared,
x1^2

125
00:04:10,310 --> 00:04:12,240
x2, x10 and
x2 x10

126
00:04:12,900 --> 00:04:15,280
x11, x17 and so on.
x11 x17等等

127
00:04:15,700 --> 00:04:18,110
You can imagine there are gonna be a lot of these features.
类似的三次项有很多很多

128
00:04:19,040 --> 00:04:19,770
In fact, they are going to be
事实上

129
00:04:20,050 --> 00:04:21,260
order and cube such features
三次项的个数是以n^3的量级增加

130
00:04:22,210 --> 00:04:23,830
and if any is 100
当n=100时

131
00:04:24,150 --> 00:04:25,660
you can compute that, you
可以计算出来

132
00:04:25,740 --> 00:04:26,870
end up with on the order
最后能得到

133
00:04:27,730 --> 00:04:29,650
of about 170,000 such cubic
大概17000个三次项

134
00:04:30,040 --> 00:04:31,670
features and so including
所以

135
00:04:32,260 --> 00:04:34,470
these higher auto-polynomial features when
当初始特征个数n增大时

136
00:04:34,920 --> 00:04:36,050
your original feature set end
这些高阶多项式项数

137
00:04:36,230 --> 00:04:37,730
is large this really dramatically
将以几何级数递增

138
00:04:38,530 --> 00:04:40,440
blows up your feature space and
特征空间也随之急剧膨胀

139
00:04:41,070 --> 00:04:42,180
this doesn't seem like a
当特征个数n很大时

140
00:04:42,320 --> 00:04:43,320
good way to come up with
如果找出附加项

141
00:04:43,560 --> 00:04:45,050
additional features with which
来建立一些分类器

142
00:04:45,240 --> 00:04:48,100
to build none many classifiers when n is large.
这并不是一个好做法

143
00:04:49,590 --> 00:04:52,560
For many machine learning problems, n will be pretty large.
对于许多实际的机器学习问题 特征个数n是很大的

144
00:04:53,270 --> 00:04:53,560
Here's an example.
我们看看下边这个例子

145
00:04:55,000 --> 00:04:58,140
Let's consider the problem of computer vision.
关于计算机视觉中的一个问题

146
00:04:59,670 --> 00:05:00,770
And suppose you want to
假设你想要

147
00:05:01,260 --> 00:05:02,620
use machine learning to train
使用机器学习算法

148
00:05:02,710 --> 00:05:04,610
a classifier to examine an
来训练一个分类器

149
00:05:04,710 --> 00:05:05,880
image and tell us whether
使它检测一个图像

150
00:05:06,160 --> 00:05:08,030
or not the image is a car.
来判断图像是否为一辆汽车

151
00:05:09,480 --> 00:05:11,900
Many people wonder why computer vision could be difficult.
很多人可能会好奇  这对计算机视觉来说有什么难的

152
00:05:12,390 --> 00:05:13,140
I mean when you and I
当我们自己看这幅图像时

153
00:05:13,270 --> 00:05:15,670
look at this picture it is so obvious what this is.
里面有什么是一目了然的事情

154
00:05:15,900 --> 00:05:17,000
You wonder how is it
你肯定会很奇怪

155
00:05:17,190 --> 00:05:18,320
that a learning algorithm could possibly
为什么学习算法竟可能会不知道

156
00:05:18,910 --> 00:05:20,880
fail to know what this picture is.
图像是什么

157
00:05:22,110 --> 00:05:23,330
To understand why computer vision
为了解答这个疑问

158
00:05:23,720 --> 00:05:25,380
is hard let's zoom
我们取出这幅图片中的一小部分

159
00:05:25,650 --> 00:05:26,690
into a small part of the
将其放大

160
00:05:26,940 --> 00:05:28,180
image like that area where the
比如图中

161
00:05:28,510 --> 00:05:30,240
little red rectangle is.
这个红色方框内的部分

162
00:05:30,400 --> 00:05:31,330
It turns out that where you
结果表明

163
00:05:31,450 --> 00:05:34,270
and I see a car, the computer sees that.
当人眼看到一辆汽车时 计算机实际上看到的却是这个

164
00:05:34,780 --> 00:05:35,930
What it sees is this matrix,
一个数据矩阵

165
00:05:36,600 --> 00:05:38,110
or this grid, of pixel
或像这种格网

166
00:05:38,580 --> 00:05:40,350
intensity values that tells
它们表示了像素强度值

167
00:05:40,610 --> 00:05:42,930
us the brightness of each pixel in the image.
告诉我们图像中每个像素的亮度值

168
00:05:43,640 --> 00:05:45,170
So the computer vision problem is
因此 对于计算机视觉来说问题就变成了

169
00:05:45,550 --> 00:05:47,230
to look at this matrix of
根据这个像素点亮度矩阵

170
00:05:47,310 --> 00:05:49,140
pixel intensity values, and tell
来告诉我们

171
00:05:49,410 --> 00:05:52,440
us that these numbers represent the door handle of a car.
这些数值代表一个汽车门把手

172
00:05:54,230 --> 00:05:55,740
Concretely, when we use
具体而言

173
00:05:56,030 --> 00:05:57,220
machine learning to build a
当用机器学习算法构造

174
00:05:57,430 --> 00:05:59,060
car detector, what we do
一个汽车识别器时

175
00:05:59,360 --> 00:06:00,510
is we come up with a
我们要想出

176
00:06:00,800 --> 00:06:02,690
label training set, with, let's
一个带标签的样本集

177
00:06:02,890 --> 00:06:04,250
say, a few label examples
其中一些样本

178
00:06:04,730 --> 00:06:05,850
of cars and a few
是各类汽车

179
00:06:06,000 --> 00:06:07,150
label examples of things that
另一部分样本

180
00:06:07,380 --> 00:06:08,780
are not cars, then we
是其他任何东西

181
00:06:09,090 --> 00:06:10,590
give our training set to
将这个样本集输入给学习算法

182
00:06:10,720 --> 00:06:12,230
the learning algorithm trained a
以训练出一个分类器

183
00:06:12,310 --> 00:06:13,500
classifier and then, you
训练完毕后

184
00:06:13,680 --> 00:06:14,700
know, we may test it and show
我们输入一幅新的图片

185
00:06:14,890 --> 00:06:16,710
the new image and ask, "What is this new thing?".
让分类器判定  “这是什么东西？”

186
00:06:17,980 --> 00:06:20,030
And hopefully it will recognize that that is a car.
理想情况下 分类器能识别出这是一辆汽车

187
00:06:21,410 --> 00:06:24,000
To understand why we
为了理解引入

188
00:06:24,120 --> 00:06:26,810
need nonlinear hypotheses, let's take
非线性分类器的必要性

189
00:06:27,050 --> 00:06:27,940
a look at some of the
我们从学习算法的训练样本中

190
00:06:28,190 --> 00:06:29,360
images of cars and maybe
挑出一些汽车图片

191
00:06:29,480 --> 00:06:31,780
non-cars that we might feed to our learning algorithm.
和一些非汽车图片

192
00:06:32,960 --> 00:06:33,920
Let's pick a couple of pixel
让我们从其中

193
00:06:34,090 --> 00:06:35,630
locations in our images, so
每幅图片中挑出一组像素点

194
00:06:35,750 --> 00:06:37,040
that's pixel one location and
这是像素点1的位置

195
00:06:37,180 --> 00:06:39,500
pixel two location, and let's
这是像素点2的位置

196
00:06:39,730 --> 00:06:42,390
plot this car, you know, at the
在坐标系中标出这幅汽车的位置

197
00:06:42,510 --> 00:06:44,010
location, at a certain
在某一点上

198
00:06:44,360 --> 00:06:45,890
point, depending on the intensities
车的位置取决于

199
00:06:46,430 --> 00:06:47,870
of pixel one and pixel two.
像素点1和像素点2的亮度

200
00:06:49,260 --> 00:06:50,630
And let's do this with a few other images.
让我们用同样的方法标出其他图片中汽车的位置

201
00:06:51,060 --> 00:06:52,450
So let's take a different example
然后我们再举一个

202
00:06:52,980 --> 00:06:53,980
of the car and you know,
关于汽车的不同的例子

203
00:06:54,080 --> 00:06:55,010
look at the same two pixel locations
观察这两个相同的像素位置

204
00:06:56,160 --> 00:06:57,570
and that image has a
这幅图片中

205
00:06:57,770 --> 00:06:58,970
different intensity for pixel one
像素1有一个像素强度

206
00:06:59,230 --> 00:07:00,660
and a different intensity for pixel two.
像素2也有一个不同的像素强度

207
00:07:00,960 --> 00:07:02,940
So, it ends up at a different location on the figure.
所以在这幅图中它们两个处于不同的位置

208
00:07:03,360 --> 00:07:05,740
And then let's plot some negative examples as well.
我们继续画上两个非汽车样本

209
00:07:05,990 --> 00:07:07,590
That's a non-car, that's a
这个不是汽车

210
00:07:07,720 --> 00:07:09,470
non-car  .
这个也不是汽车

211
00:07:09,730 --> 00:07:10,910
And if we do this for
然后我们继续

212
00:07:11,070 --> 00:07:12,720
more and more examples using
在坐标系中画上更多的新样本

213
00:07:13,280 --> 00:07:14,680
the pluses to denote cars
用''+"表示汽车图片

214
00:07:15,080 --> 00:07:16,310
and minuses to denote non-cars,
用“-”表示非汽车图片

215
00:07:16,890 --> 00:07:18,500
what we'll find is that
我们将发现

216
00:07:18,830 --> 00:07:20,680
the cars and non-cars end up
汽车样本和非汽车样本

217
00:07:20,890 --> 00:07:22,430
lying in different regions of
分布在坐标系中的不同区域

218
00:07:22,570 --> 00:07:24,910
the space, and what we
因此

219
00:07:25,180 --> 00:07:26,570
need therefore is some sort
我们现在需要一个

220
00:07:26,750 --> 00:07:28,790
of non-linear hypotheses to try
非线性分类器

221
00:07:29,000 --> 00:07:30,900
to separate out the two classes.
来尽量分开这两类样本

222
00:07:32,480 --> 00:07:34,300
What is the dimension of the feature space?
这个分类问题中特征空间的维数是多少？

223
00:07:35,290 --> 00:07:38,210
Suppose we were to use just 50 by 50 pixel images.
假设我们用50*50像素的图片

224
00:07:38,770 --> 00:07:40,050
Now that suppose our images were
我们的图片已经很小了

225
00:07:40,520 --> 00:07:42,760
pretty small ones, just 50 pixels on the side.
长宽只各有50个像素

226
00:07:43,470 --> 00:07:44,990
Then we would have 2500 pixels,
但这依然是2500个像素点

227
00:07:46,330 --> 00:07:47,650
and so the dimension of
因此

228
00:07:47,740 --> 00:07:49,310
our feature size will be N
我们的特征向量的元素数量

229
00:07:49,520 --> 00:07:51,450
equals 2500 where our feature
N=2500

230
00:07:51,700 --> 00:07:52,910
vector x is a list
特征向量X

231
00:07:53,180 --> 00:07:54,570
of all the pixel testings, you
包含了所有像素点的亮度值

232
00:07:54,710 --> 00:07:56,690
know, the pixel brightness of pixel
这是像素点1的亮度

233
00:07:57,080 --> 00:07:58,030
one, the brightness of pixel
这是像素点2的亮度

234
00:07:58,330 --> 00:07:59,580
two, and so on down
如此类推

235
00:07:59,870 --> 00:08:01,310
to the pixel brightness of the
直到最后一个

236
00:08:01,400 --> 00:08:03,420
last pixel where, you know, in a
像素点的亮度

237
00:08:03,590 --> 00:08:05,450
typical computer representation, each of
对于典型的计算机图片表示方法

238
00:08:05,540 --> 00:08:07,190
these may be values between say
如果存储的是每个像素点的灰度值 (色彩的强烈程度)

239
00:08:07,480 --> 00:08:09,020
0 to 255 if it gives
那么每个元素的值

240
00:08:09,230 --> 00:08:12,110
us the grayscale value.
应该在0到255之间

241
00:08:12,520 --> 00:08:13,290
So we have n equals 2500,
因此 这个问题中n=2500

242
00:08:13,950 --> 00:08:15,580
and that's if we
但是

243
00:08:15,740 --> 00:08:17,140
were using grayscale images.
这只是使用灰度图片的情况

244
00:08:17,790 --> 00:08:18,800
If we were using RGB
如果我们用的是RGB彩色图像

245
00:08:19,440 --> 00:08:21,140
images with separate red, green
每个像素点包含红、绿、蓝三个子像素

246
00:08:21,420 --> 00:08:23,870
and blue values, we would have n equals 7500.
那么n=7500

247
00:08:27,650 --> 00:08:28,630
So, if we were to
因此 如果我们非要

248
00:08:29,000 --> 00:08:29,920
try to learn a nonlinear
通过包含所有的二次项

249
00:08:30,370 --> 00:08:32,020
hypothesis by including all
来解决这个非线性问题

250
00:08:32,300 --> 00:08:33,710
the quadratic features, that is
那么

251
00:08:33,810 --> 00:08:34,680
all the terms of the form, you know,
这就是式子中的所有条件

252
00:08:35,430 --> 00:08:38,900
Xi times Xj, while with the
xi*xj

253
00:08:39,130 --> 00:08:40,370
2500 pixels we would end
连同开始的2500像素

254
00:08:40,580 --> 00:08:42,500
up with a total of three million features.
总共大约有300万个

255
00:08:43,050 --> 00:08:44,620
And that's just too large to
这数字大得有点离谱了

256
00:08:44,720 --> 00:08:46,430
be reasonable; the computation would
对于每个样本来说

257
00:08:46,600 --> 00:08:48,680
be very expensive to find and
要发现并表示

258
00:08:48,840 --> 00:08:50,070
to represent all of these
所有这300万个项

259
00:08:50,310 --> 00:08:52,250
three million features per training example.
这计算成本太高了

260
00:08:55,470 --> 00:08:57,580
So, simple logistic regression together
因此 只是简单的增加

261
00:08:58,100 --> 00:08:59,230
with adding in maybe the
二次项或者三次项

262
00:08:59,300 --> 00:09:00,510
quadratic or the cubic features
之类的逻辑回归算法

263
00:09:00,930 --> 00:09:01,910
- that's just not a
并不是一个解决

264
00:09:01,980 --> 00:09:03,950
good way to learn complex
复杂非线性问题的好办法

265
00:09:04,550 --> 00:09:06,090
nonlinear hypotheses when n
因为当n很大时

266
00:09:06,310 --> 00:09:08,410
is large because you just end up with too many features.
将会产生非常多的特征项

267
00:09:09,370 --> 00:09:10,620
In the next few videos, I
在接下来的视频课程中

268
00:09:10,840 --> 00:09:11,890
would like to tell you about Neural
我将为大家讲解神经网络

269
00:09:12,080 --> 00:09:13,670
Networks, which turns out
它在解决复杂的非线性分类问题上

270
00:09:13,980 --> 00:09:15,370
to be a much better way to
被证明是

271
00:09:15,650 --> 00:09:17,720
learn complex hypotheses, complex nonlinear
是一种好得多的算法

272
00:09:17,960 --> 00:09:19,780
hypotheses even when your
即使你输入特征空间

273
00:09:20,070 --> 00:09:22,080
input feature space, even when n is large.
或输入的特征维数n很大也能轻松搞定

274
00:09:22,860 --> 00:09:24,080
And along the way I'll
在后面的课程中

275
00:09:24,420 --> 00:09:25,580
also get to show
我将给大家展示

276
00:09:25,780 --> 00:09:26,730
you a couple of fun videos
一些有趣的视频

277
00:09:27,240 --> 00:09:29,030
of historically important applications
视频中讲述了神经网络在历史上的重要应用

278
00:09:30,300 --> 00:09:31,290
of Neural networks as well that I
我也希望

279
00:09:32,100 --> 00:09:33,480
hope those videos that
这些我们即将看到的视频

280
00:09:33,570 --> 00:09:35,460
we'll see later will be fun for you to watch as well.
能给你的学习过程带来一些乐趣 ^ ^

