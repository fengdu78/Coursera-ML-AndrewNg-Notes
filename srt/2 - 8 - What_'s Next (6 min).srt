1
00:00:00,000 --> 00:00:04,839
You now know about linear regression
and gradient descent. The plan from here
你现在已经了解了线性回归和梯度下降 接下来我想
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:04,839 --> 00:00:09,437
on out is to tell you about a couple of
important extensions of these ideas.
给大家介绍这些概念的一些重要扩展

3
00:00:09,437 --> 00:00:13,668
Concretely here they are. First it turns
out that in order to solve this
具体来讲是这样的 首先在解决这个

4
00:00:13,668 --> 00:00:18,468
minimization problem, turns out there's an
algorithm for solving for theta zero and
最小化问题时 有一个算法可以直接解出theta 0和

5
00:00:18,468 --> 00:00:22,978
theta one exactly without needing an
iterative algorithm. Without needing this
theta 1 而不必借助迭代 也就是梯度下降这一类

6
00:00:22,978 --> 00:00:27,555
algorithm like gradient descent that we
had to iterate, you know, multiple times
要求多次迭代的算法

7
00:00:27,555 --> 00:00:32,285
over. So it turns out there are advantages
and disadvantages of this algorithm that
这个算法可以让你直接求出Theta 0 和Theta 1

8
00:00:32,285 --> 00:00:36,897
lets you just solve for theta zero and
theta one, basically in just one shot. One
因此也同时带来了优点和缺点

9
00:00:36,897 --> 00:00:41,685
advantage is that there is no longer a
learning rate alpha that you need to worry
好处之一是你不再需要设定学习速率

10
00:00:41,685 --> 00:00:46,414
about and set. And so it can be much
faster for some problems. We'll talk about
因此你可以更快地解决一些问题 关于它的其他优缺点

11
00:00:46,414 --> 00:00:51,051
its advantages and disadvantages later.
Second, we'll also talk about algorithms
我们将在后面继续讨论的 其次 我们会讲到

12
00:00:51,051 --> 00:00:55,424
for learning with a larger number of
features. So, so far we've been learning
应用于学习多个特征时所用的算法 至今为止

13
00:00:55,424 --> 00:01:00,027
with just one feature, the size of the
house and using that to predict the price,
我们一直在只有一个特征的情况下学习 也就是用房子的大小来预测它的价格

14
00:01:00,027 --> 00:01:04,688
so we're trying to take x and use that to
predict y. But for other learning problems
也就是我们试图用一个变量x来预测另一个变量y 但对于其他学习问题来说

15
00:01:04,688 --> 00:01:09,899
we may have a larger number of features.
So for example let's say that you know not
我们可能会面对更多的特征作为变量 比方说 你不仅知道房子的大小

16
00:01:09,899 --> 00:01:15,448
only the size, but also the number of
bedrooms, the number of floors, and the age
还知道卧室的数量 楼层数 和房子的年份

17
00:01:15,448 --> 00:01:20,930
of these houses. And you want
to use that to predict the price of the
接着你想利用这些变量来预测房子的价格

18
00:01:20,930 --> 00:01:26,005
houses. In that case maybe we'll call
these features x1, x2, x3, and x4. So now
在这种情况下 我们可以叫这些特征为x1 x2 x3 x4 所以现在我们就

19
00:01:26,005 --> 00:01:31,554
we have, you know, four features. We want to
use these four features to predict why the
有了四个特征 我们希望使用这些特征来预测房价

20
00:01:31,554 --> 00:01:36,797
price of the house. It turns out with all
of these features, four of them in this
事实上 在有多个这些特征的情况下 在此例中是四个

21
00:01:36,797 --> 00:01:41,858
case, it turns out that with multiple
features it becomes harder to
想要描绘并可视化这些变量变得十分困难

22
00:01:41,858 --> 00:01:47,243
plot or visualize the data. So for example
here we try to plot this type of data
比如这里 我们要想绘制这种类型的数据集

23
00:01:47,243 --> 00:01:52,823
set. Maybe we will have the vertical axis
be the price and maybe we can have one
我们可以将垂直的轴标为房价

24
00:01:52,823 --> 00:01:58,078
axis here, and another one here where this
axis is the size of the house, and that
将一条轴标为房子的大小

25
00:01:58,078 --> 00:02:02,822
axis is the number of bedrooms. You know,
but this is just plotting, right my first
并将另一条轴标为卧室的数量 这只是描点而已 使用我们的前两个特征

26
00:02:02,822 --> 00:02:07,414
two features: size and number of bedrooms.
And when we have these additional features
房子的大小和卧室数量 但是 当我们有了这些更多的特征时

27
00:02:07,414 --> 00:02:11,677
I don't know, I just don't know how to
plot all of these data, right cuz I need
我就不知道如何绘制出所有的这些数据 因为我需要

28
00:02:11,677 --> 00:02:15,886
like a 4-dimensional or 5-dimensional
figure. I don't really know how to plot
绘制一个四维或五维的图 我们也确实不知道如何绘制

29
00:02:15,886 --> 00:02:19,930
you know something more than like a
3-dimensional figure, like, like what I
超过三维的图像 就像我这里的例子一样

30
00:02:19,930 --> 00:02:24,139
have over here. Also as you can tell, the
notation starts to get a little more
另外你也一定发现了 我们使用的符号开始变得更加复杂

31
00:02:24,139 --> 00:02:28,238
complicated, right. So rather than just
having x our features we now have x1
比起之前我们只有x一个特征 现在我们面对着从x1到x4

32
00:02:28,238 --> 00:02:33,519
through x4. And we're using these
subscripts to denote my four different
总共4个特征 所以我们使用这些下标来区别这些不同的特征

33
00:02:33,519 --> 00:02:38,059
features. It turns out the best notation
to keep all of this straight and to
事实上 我们有一套数学标记可以很好地

34
00:02:38,059 --> 00:02:42,828
understand what's going on with the data
even when we don't quite know how to plot
对这些数据集进行标注 即使是在我们无法绘制它们的情况下

35
00:02:42,828 --> 00:02:47,425
it. It turns out that the best notation is
the notation of linear algebra. Linear
也就是运用线性代数的符号

36
00:02:47,425 --> 00:02:52,194
algebra gives us a notation and a set of
things or a set of operations that we can
线性代数赋予了我们一套符号系统和操作来进行

37
00:02:52,194 --> 00:02:58,234
do with matrices and vectors. For example.
Here's a matrix where the columns of this
矩阵和向量的处理 举例来说 这里是一个矩阵 我们来看它的每一列

38
00:02:58,234 --> 00:03:03,377
matrix are: The first column is the sizes
of the four houses, the second column was
第一列是四间房子的大小 第二列是

39
00:03:03,377 --> 00:03:08,025
the number of bedrooms, that's the number
of floors and that was the age of the
卧室的数量 这里是楼层数 这里是房子的年份

40
00:03:08,025 --> 00:03:12,496
home. And so a matrix is a block of
numbers that lets me take all of my data,
所以这个矩阵是一些数字的组合 其中包括了我们所有的数据 所有的x

41
00:03:12,496 --> 00:03:17,881
all of my x's. All of my features and
organize them efficiently into sort of one
我们将所有的特征有效地组织起来 排入这样一整块的数字中

42
00:03:17,881 --> 00:03:23,565
big block of numbers like that. And here
is what we call a vector in linear algebra
接着 这是我们在线性代数中所说的向量

43
00:03:23,565 --> 00:03:29,118
where the four numbers here are the prices
of the four houses that we saw on the
这里的四个数字就是在之前课件上的四间房子的价格

44
00:03:29,118 --> 00:03:34,334
previous slide. So. In the next set of
videos what I'm going to do is do a quick
所以 在接下来的一组视频中 我会对线性代数

45
00:03:34,334 --> 00:03:38,730
review of linear algebra. If you haven't
seen matrices and vectors before, so if
进行一个快速的复习回顾 如果你从来没有接触过向量和矩阵

46
00:03:38,730 --> 00:03:43,293
all of this, everything on this slide is
brand new to you or if you've seen linear
这张课件上所有的一切对你来说都是新知识

47
00:03:43,293 --> 00:03:47,745
algebra before, but it's been a while so
you aren't completely familiar with it
或者你之前对线性代数有所了解 但由于隔得久了 对其有所遗忘

48
00:03:47,745 --> 00:03:52,419
anymore, then please watch the next set of
videos. And I'll quickly review the linear
那就请学习接下来的一组视频 我会快速地回顾你将用到的线性代数知识

49
00:03:52,419 --> 00:03:57,093
algebra you need in order to implement and
use the more powerful versions of linear
通过它们 你可以实现和使用更强大的线性回归模型

50
00:03:57,093 --> 00:04:01,489
regression. It turns out linear algebra
isn't just useful for linear regression
事实上 线性代数不仅仅在线性回归中应用广泛

51
00:04:01,489 --> 00:04:05,972
models but these ideas of matrices and
vectors will be useful for helping us
它其中的矩阵和向量将有助于帮助我们

52
00:04:05,972 --> 00:04:10,272
to implement and actually get
computationally efficient implementations
实现之后更多的机器学习模型

53
00:04:10,272 --> 00:04:15,088
for many later machines learning models as
well. And as you can tell these sorts of
并在计算上更有效率 正是因为这些

54
00:04:15,088 --> 00:04:19,617
matrices and vectors will give us an
efficient way to start to organize large
矩阵和向量提供了一种有效的方式来组织

55
00:04:19,617 --> 00:04:23,918
amounts of data, when we work with larger
training sets. So, in case, in case
大量的数据 特别是当我们处理巨大的训练集时

56
00:04:23,918 --> 00:04:28,619
you're not familiar with linear algebra or
in case linear algebra seems like a
如果你不熟悉线性代数 如果你觉得线性代数看上去是一个

57
00:04:28,619 --> 00:04:33,263
complicated, scary concept for those of you who've
never seen it before, don't worry about
复杂 可怕的概念 特别是对于之前从未接触过它的人

58
00:04:33,263 --> 00:04:37,793
it. It turns out in order to implement
machine learning algorithms we need only
不必担心 事实上 为了实现机器学习算法 我们只需要

59
00:04:37,793 --> 00:04:42,002
the very, very basics of
linear algebra and you'll be able to very
一些非常非常基础的线性代数知识 通过接下来几个视频

60
00:04:42,002 --> 00:04:46,840
quickly pick up everything you need to
know in the next few videos.
你可以很快地学会所有你需要了解的线性代数知识

61
00:04:46,840 --> 00:04:53,386
Concretely, to decide if you should
watch the next set of videos, here are the
具体来说 为了帮助你判断是否有需要学习接下来的一组视频​

62
00:04:53,386 --> 00:04:57,804
topics I'm going to cover. Talk about
what are matrices and vectors. Talk about how
这里是一些我会涵盖的主题 我会讨论什么是矩阵和向量 谈谈如何

63
00:05:00,013 --> 00:05:02,222
to add, subtract, multiply matrices and vectors.
Talk about the ideas of matrix inverses
加 、减 、乘矩阵和向量 讨论逆矩阵和转置矩阵的概念

64
00:05:02,222 --> 00:05:06,696
and transposes. And so, if you are not
sure if you should watch the next set of
所以如果你不确定自己是否需要学习接下来的视频

65
00:05:06,696 --> 00:05:11,393
videos take a look at these two things. So
if you think you know how to compute this
你可以看看这两个式子 如果你知道该如何计算这个数值

66
00:05:11,393 --> 00:05:15,643
quantity, this matrix transpose times
another matrix. If you think you know, if
一个转置矩阵乘以另一个矩阵

67
00:05:15,643 --> 00:05:20,173
you have seen this stuff before, if you
know how to compute the inverse of matrix
如果你之前见过这样的式子 知道如何计算一个逆矩阵

68
00:05:20,173 --> 00:05:24,423
times a vector, minus a number, times
another vector. If these two things look
乘以一个向量减去一个数乘以另一个向量 如果你十分熟悉这些概念

69
00:05:24,423 --> 00:05:29,309
completely familiar to you then you can
safely skip the optional set of videos on
那么你完全可以跳过这组关于线性代数的选修视频

70
00:05:29,309 --> 00:05:34,607
linear algebra. But if these, concepts, if you're
slightly uncertain what these blocks of
但是如果你对这些概念仍有些许的不确定

71
00:05:34,607 --> 00:05:39,906
numbers or these matrices of numbers mean,
then please take a look of the next set of
不确定这些数字或这些矩阵的意思 那么请看一看下一组的视频

72
00:05:39,906 --> 00:05:45,142
videos and, it'll very quickly teach you what
you need to know about linear algebra in
它会很快地教你一些你需要知道的线性代数的知识

73
00:05:45,142 --> 00:05:49,936
order to program machine learning
algorithms and deal with large amounts of data.
便于之后编写机器学习算法和处理大量数据

