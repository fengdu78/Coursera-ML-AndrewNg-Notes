1
00:00:00,280 --> 00:00:04,479
In this video, I'd like to tell you about the idea of vectorization.
在这段视频中 我将介绍有关向量化的内容
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:04,480 --> 00:00:06,471
So, whether you're using Octave
无论你是用Octave

3
00:00:06,471 --> 00:00:08,277
or a similar language like MATLAB
还是别的语言 比如MATLAB

4
00:00:08,277 --> 00:00:09,604
or whether you're using Python
或者你正在用Python

5
00:00:09,604 --> 00:00:12,520
and NumPy or Java CC++.
NumPy 或 Java C C++

6
00:00:12,520 --> 00:00:14,850
All of these languages have either
所有这些语言都具有

7
00:00:14,850 --> 00:00:16,708
built into them or have
各种线性代数库

8
00:00:16,720 --> 00:00:19,439
readily and easily accessible, different
这些库文件都是内置的

9
00:00:19,439 --> 00:00:21,806
numerical linear algebra libraries.
容易阅读和获取

10
00:00:21,820 --> 00:00:23,335
They're usually very well written,
他们通常写得很好

11
00:00:23,335 --> 00:00:25,695
highly optimized, often so that developed by
已经经过高度优化

12
00:00:25,695 --> 00:00:29,181
people that, you know, have PhDs in numerical computing or
通常是数值计算方面的博士

13
00:00:29,181 --> 00:00:32,075
they are really specializing numerical computing.
或者专业人士开发的

14
00:00:32,075 --> 00:00:33,944
And when you're implementing machine
而当你实现机器学习算法时

15
00:00:33,960 --> 00:00:35,904
learning algorithms, if you're able
如果你能

16
00:00:35,930 --> 00:00:37,797
to take advantage of these
好好利用这些

17
00:00:37,810 --> 00:00:39,296
linear algebra libraries or these
线性代数库或者说

18
00:00:39,310 --> 00:00:41,600
numerical linear algebra libraries and
数值线性代数库

19
00:00:41,620 --> 00:00:43,387
mix the routine calls to them
并联合调用它们

20
00:00:43,387 --> 00:00:45,172
rather than sort of right call
而不是自己去做那些

21
00:00:45,180 --> 00:00:48,029
yourself to do things that these libraries could be doing.
函数库可以做的事情

22
00:00:48,040 --> 00:00:49,612
If you do that then
如果是这样的话 那么

23
00:00:49,612 --> 00:00:51,872
often you get that "first is more efficient".
通常你会发现 首先 这样更有效

24
00:00:51,880 --> 00:00:53,179
So, just run more quickly and
也就是说运行速度更快

25
00:00:53,179 --> 00:00:54,891
take better advantage of
并且更好地利用

26
00:00:54,891 --> 00:00:56,631
any parallel hardware your computer
你的计算机里可能有的一些并行硬件系统

27
00:00:56,631 --> 00:00:58,254
may have and so on.
等等

28
00:00:58,270 --> 00:01:00,533
And second, it also means
第二 这也意味着

29
00:01:00,540 --> 00:01:03,075
that you end up with less code that you need to write.
你可以用更少的代码来实现你需要的功能

30
00:01:03,075 --> 00:01:04,962
So have a simpler implementation
因此 实现的方式更简单

31
00:01:04,962 --> 00:01:08,532
that is, therefore, maybe also more likely to be bug free.
代码出现问题的有可能性也就越小

32
00:01:08,550 --> 00:01:10,534
And as a concrete example.
举个具体的例子

33
00:01:10,570 --> 00:01:12,726
Rather than writing code
与其自己写代码

34
00:01:12,726 --> 00:01:15,061
yourself to multiply matrices, if
做矩阵乘法

35
00:01:15,061 --> 00:01:16,300
you let Octave do it by
如果你只在Octave中

36
00:01:16,300 --> 00:01:18,145
typing a times b,
输入 a乘以b

37
00:01:18,145 --> 00:01:19,833
that will use a very efficient
就是一个非常有效的

38
00:01:19,833 --> 00:01:22,318
routine to multiply the 2 matrices.
两个矩阵相乘的程序

39
00:01:22,340 --> 00:01:23,985
And there's a bunch of examples like
有很多例子可以说明

40
00:01:24,010 --> 00:01:27,220
these where you use appropriate vectorized implementations.
如果你用合适的向量化方法来实现

41
00:01:27,220 --> 00:01:30,062
You get much simpler code, and much more efficient code.
你就会有一个简单得多 也有效得多的代码

42
00:01:30,280 --> 00:01:33,071
Let's look at some examples.
让我们来看一些例子

43
00:01:33,071 --> 00:01:34,937
Here's a usual hypothesis of linear
这是一个常见的线性回归假设函数

44
00:01:34,937 --> 00:01:36,415
regression and if you
如果

45
00:01:36,415 --> 00:01:37,348
want to compute H of
你想要计算 h(x)

46
00:01:37,348 --> 00:01:40,032
X, notice that there is a sum on the right.
注意到右边是求和

47
00:01:40,032 --> 00:01:41,130
And so one thing you could
那么你可以

48
00:01:41,130 --> 00:01:42,775
do is compute the sum
自己计算

49
00:01:42,775 --> 00:01:46,611
from J equals 0 to J equals N yourself.
j =0 到 j = n 的和

50
00:01:46,620 --> 00:01:48,000
Another way to think of this
但换另一种方式来想想

51
00:01:48,000 --> 00:01:49,210
is to think of h
是把 h 看作

52
00:01:49,210 --> 00:01:52,029
of x as theta transpose x
θ 转置乘以 x

53
00:01:52,029 --> 00:01:53,262
and what you can do is
那么

54
00:01:53,262 --> 00:01:55,654
think of this as you know, computing this
你就可以写成

55
00:01:55,660 --> 00:01:57,823
in a product between 2 vectors
两个向量的内积

56
00:01:57,840 --> 00:02:00,135
where theta is, you know, your
其中 θ 就是

57
00:02:00,135 --> 00:02:01,784
vector say theta 0, theta 1,
θ0  θ1

58
00:02:01,800 --> 00:02:04,812
theta 2 if you have 2 features.
θ2 如果你有两个特征量

59
00:02:04,812 --> 00:02:06,410
If n equals 2 and if
如果 n 等于2 并且如果

60
00:02:06,450 --> 00:02:08,133
you think of x as this
你把 x 看作

61
00:02:08,133 --> 00:02:11,810
vector, x0, x1, x2
x0 x1 x2

62
00:02:11,884 --> 00:02:13,952
and these 2 views can
这两种思考角度

63
00:02:13,952 --> 00:02:17,539
give you 2 different implementations.
会给你两种不同的实现方式

64
00:02:17,560 --> 00:02:18,909
Here's what I mean.
比如说

65
00:02:18,909 --> 00:02:21,012
Here's an unvectorized implementation for
这是未向量化的代码实现方式

66
00:02:21,040 --> 00:02:22,454
how to compute h of
计算 h(x) 是未向量化的

67
00:02:22,454 --> 00:02:26,120
x and by unvectorized I mean, without vectorization.
我的意思是 没有被向量化

68
00:02:26,130 --> 00:02:29,479
We might first initialize, you know, prediction to be 0.0.
我们可能首先要初始化变量 prediction 的值为0.0

69
00:02:29,479 --> 00:02:32,383
This is going to eventually, the
而这个

70
00:02:32,383 --> 00:02:34,287
prediction is going to be
变量 prediction 的最终结果就是

71
00:02:34,300 --> 00:02:36,090
h of x and then
h(x) 然后

72
00:02:36,090 --> 00:02:37,258
I'm going to have a for loop for
我要用一个 for 循环

73
00:02:37,270 --> 00:02:38,354
j equals one through n+1
j 取值 0 到 n+1

74
00:02:38,354 --> 00:02:40,792
prediction gets incremented by
变量prediction 每次就通过

75
00:02:40,792 --> 00:02:41,822
theta j times xj.
自身加上 θ(j) 乘以 x(j) 更新值

76
00:02:41,822 --> 00:02:44,737
So, it's kind of this expression over here.
这个就是算法的代码实现

77
00:02:44,737 --> 00:02:47,223
By the way, I should mention in these
顺便我要提醒一下

78
00:02:47,223 --> 00:02:48,894
vectors right over here, I
这里的向量

79
00:02:48,900 --> 00:02:51,102
had these vectors being 0 index.
我用的下标是 0

80
00:02:51,110 --> 00:02:52,600
So, I had theta 0 theta 1,
所以我有 θ0 θ1

81
00:02:52,600 --> 00:02:54,390
theta 2, but because MATLAB
θ2 但因为 MATLAB

82
00:02:54,390 --> 00:02:56,713
is one index, theta 0
的下标从1开始 在 MATLAB 中 θ0

83
00:02:56,713 --> 00:02:58,019
in MATLAB, we might
我们可能会

84
00:02:58,019 --> 00:03:00,204
end up representing as theta
用 θ1 来表示

85
00:03:00,204 --> 00:03:02,042
1 and this second element
这第二个元素

86
00:03:02,042 --> 00:03:04,392
ends up as theta
最后就会变成

87
00:03:04,392 --> 00:03:05,862
2 and this third element
θ2 而第三个元素

88
00:03:05,880 --> 00:03:08,002
may end up as theta
最终可能就用

89
00:03:08,002 --> 00:03:09,952
3 just because vectors in
θ3 表示 因为

90
00:03:09,960 --> 00:03:11,998
MATLAB are indexed starting
MATLAB 中的下标从1开始

91
00:03:11,998 --> 00:03:13,525
from 1 even though our real
即使我们实际的

92
00:03:13,525 --> 00:03:15,436
theta and x here starting,
θ 和 x 的下标从0开始

93
00:03:15,450 --> 00:03:17,002
indexing from 0, which
这就是为什么

94
00:03:17,002 --> 00:03:18,785
is why here I have a for loop
这里我的 for 循环

95
00:03:18,785 --> 00:03:20,498
j goes from 1 through n+1
j 取值从 1 直到 n+1

96
00:03:20,498 --> 00:03:22,225
rather than j go through
而不是

97
00:03:22,225 --> 00:03:26,243
0 up to n, right? But
从 0 到 n 清楚了吗？

98
00:03:26,300 --> 00:03:27,870
so, this is an
但这是一个

99
00:03:27,870 --> 00:03:29,571
unvectorized implementation in that we
未向量化的代码实现方式

100
00:03:29,571 --> 00:03:31,373
have a for loop that summing up
我们用一个 for 循环

101
00:03:31,373 --> 00:03:34,018
the n elements of the sum.
对 n 个元素进行加和

102
00:03:34,050 --> 00:03:35,646
In contrast, here's how you
作为比较 接下来是

103
00:03:35,646 --> 00:03:38,400
write a vectorized implementation which
向量化的代码实现

104
00:03:38,410 --> 00:03:39,959
is that you would think
你把

105
00:03:39,959 --> 00:03:42,618
of x and theta
x 和 θ

106
00:03:42,618 --> 00:03:43,955
as vectors, and you just set
看做向量 而你只需要

107
00:03:43,955 --> 00:03:46,039
prediction equals theta transpose
令变量 prediction 等于 θ转置

108
00:03:46,039 --> 00:03:48,347
times x. You're just computing like so.
乘以 x 你就可以这样计算

109
00:03:48,360 --> 00:03:51,011
Instead of writing all these
与其写所有这些

110
00:03:51,011 --> 00:03:52,966
lines of code with the for loop,
for 循环的代码

111
00:03:52,966 --> 00:03:54,242
you instead have one line
你只需要一行代码

112
00:03:54,242 --> 00:03:56,648
of code and what this
这行代码

113
00:03:56,648 --> 00:03:57,555
line of code on the right
右边所做的

114
00:03:57,555 --> 00:03:59,237
will do is it use
就是

115
00:03:59,237 --> 00:04:01,829
Octaves highly optimized numerical
利用 Octave 的高度优化的数值

116
00:04:01,840 --> 00:04:03,859
linear algebra routines to compute
线性代数算法来计算

117
00:04:03,859 --> 00:04:06,245
this inner product between the
两个向量的内积

118
00:04:06,245 --> 00:04:08,186
two vectors, theta and X. And not
θ 以及 x

119
00:04:08,190 --> 00:04:10,182
only is the vectorized implementation
这样向量化的实现不仅仅是更简单

120
00:04:10,182 --> 00:04:14,664
simpler, it will also run more efficiently.
它运行起来也将更加高效

121
00:04:15,820 --> 00:04:17,792
So, that was Octave, but
这就是 Octave 所做的

122
00:04:17,792 --> 00:04:19,912
issue of vectorization applies to
而向量化的方法

123
00:04:19,920 --> 00:04:22,020
other programming languages as well.
在其他编程语言中同样可以实现

124
00:04:22,040 --> 00:04:24,947
Let's look at an example in C++.
让我们来看一个 C++ 的例子

125
00:04:24,947 --> 00:04:27,965
Here's what an unvectorized implementation might look like.
这就是未向量化的代码实现可能看起来的样子

126
00:04:27,965 --> 00:04:31,395
We again initialize prediction, you know, to
我们再次初始化变量 prediction 为 0.0

127
00:04:31,395 --> 00:04:32,518
0.0 and then we now have a full
然后我们现在有一个完整的

128
00:04:32,518 --> 00:04:34,508
loop for J0 up to
从 j 等于 0 直到 n

129
00:04:34,508 --> 00:04:36,819
n.  Prediction + equals
变量 prediction +=

130
00:04:36,830 --> 00:04:38,546
theta j times x j where
theta[j] 乘以 x[j]

131
00:04:38,560 --> 00:04:42,777
again, you have this x + for loop that you write yourself.
再一次 你有这样的自己写的 for 循环

132
00:04:42,777 --> 00:04:44,843
In contrast, using a good
与此相反 使用一个比较好的

133
00:04:44,850 --> 00:04:46,498
numerical linear algebra library in
C++ 数值线性代数库

134
00:04:46,498 --> 00:04:48,965
C++, you could use
你就可以用这个方程

135
00:04:48,990 --> 00:04:54,440
write the function like or rather.
write the function like or rather.

136
00:04:54,560 --> 00:04:56,533
In contrast, using a good
与此相反 使用较好的

137
00:04:56,533 --> 00:04:58,152
numerical linear algebra library in
C++ 数值线性代数库

138
00:04:58,152 --> 00:05:00,686
C++, you can instead
你可以写出这样的代码

139
00:05:00,686 --> 00:05:02,470
write code that might look like this.
write code that might look like this.

140
00:05:02,470 --> 00:05:03,985
So, depending on the details
因此取决于你的

141
00:05:03,985 --> 00:05:05,595
of your numerical linear algebra
数值线性代数库的内容

142
00:05:05,595 --> 00:05:06,790
library, you might be
你可以有一个

143
00:05:06,830 --> 00:05:08,580
able to have an object that
able to have an object that

144
00:05:08,580 --> 00:05:09,918
is a C++ object which is
C++ 对象

145
00:05:09,918 --> 00:05:11,328
vector theta and a C++
theta 和一个 C++

146
00:05:11,350 --> 00:05:13,436
object which is a vector X,
对象 向量 x

147
00:05:13,436 --> 00:05:15,552
and you just take theta dot
你只需要用 theta.transpose ()

148
00:05:15,552 --> 00:05:18,115
transpose times x where
乘以 x

149
00:05:18,120 --> 00:05:20,092
this times becomes C++ to
而这次是让 C++ 来实现运算

150
00:05:20,092 --> 00:05:22,028
overload the operator so
因此

151
00:05:22,028 --> 00:05:26,156
that you can just multiply these two vectors in C++.
你只需要在 C++ 中将两个向量相乘

152
00:05:26,156 --> 00:05:28,091
And depending on, you know,  the details
根据

153
00:05:28,110 --> 00:05:29,515
of your numerical and linear algebra
你所使用的数值和线性代数库的使用细节的不同

154
00:05:29,515 --> 00:05:30,855
library, you might end
你最终使的代码表达方式

155
00:05:30,855 --> 00:05:31,894
up using a slightly different and
可能会有些许不同

156
00:05:31,894 --> 00:05:33,636
syntax, but by relying
但是通过

157
00:05:33,636 --> 00:05:35,758
on a library to do this in a product.
一个库来做内积

158
00:05:35,760 --> 00:05:37,064
You can get a much simpler piece
你可以得到一段更简单

159
00:05:37,064 --> 00:05:40,623
of code and a much more efficient one.
更有效的代码

160
00:05:40,623 --> 00:05:43,582
Let's now look at a more sophisticated example.
现在 让我们来看一个更为复杂的例子

161
00:05:43,582 --> 00:05:45,015
Just to remind you here's our
提醒一下

162
00:05:45,015 --> 00:05:46,792
update rule for gradient descent
这是线性回归算法梯度下降的更新规则

163
00:05:46,792 --> 00:05:48,794
for linear regression and so,
所以

164
00:05:48,794 --> 00:05:50,488
we update theta j using this
我们用这条规则对 j 等于 0 1 2 等等的所有值

165
00:05:50,488 --> 00:05:53,672
rule for all values of J equals 0, 1, 2, and so on.
更新 对象 θ j

166
00:05:53,672 --> 00:05:56,259
And if I just write
我只是

167
00:05:56,260 --> 00:05:58,206
out these equations for
用 θ0 θ1 θ2 来写方程

168
00:05:58,206 --> 00:06:00,048
theta 0 Theta one, theta two.
theta 0 Theta one, theta two.

169
00:06:00,048 --> 00:06:02,173
Assuming we have two features.
那就是假设我们有两个特征量

170
00:06:02,173 --> 00:06:03,469
So N equals 2.
所以 n等于2

171
00:06:03,469 --> 00:06:04,607
Then these are the updates we
这些都是我们需要对

172
00:06:04,610 --> 00:06:07,388
perform to theta zero, theta one, theta two.
theta0 theta1 theta2的更新

173
00:06:07,410 --> 00:06:08,982
where you might remember my
你可能还记得

174
00:06:08,982 --> 00:06:10,825
saying in an earlier video
在以前的视频中说过

175
00:06:10,825 --> 00:06:14,783
that these should be simultaneous updates.
这些都应该是同步更新

176
00:06:14,783 --> 00:06:16,268
So let's see if
因此 让我们来看看

177
00:06:16,268 --> 00:06:17,725
we can come up with a
我们是否可以拿出一个

178
00:06:17,725 --> 00:06:20,723
vectorized implementation of this.
向量化的代码实现

179
00:06:20,740 --> 00:06:22,598
Here are my same 3 equations written
这里是和之前相同的三个方程

180
00:06:22,598 --> 00:06:24,182
on a slightly smaller font and you
只不过用更小一些的字体写出来

181
00:06:24,182 --> 00:06:25,517
can imagine that 1 wait
你可以想象

182
00:06:25,520 --> 00:06:26,716
to implement this three lines
实现这三个方程的方式之一

183
00:06:26,720 --> 00:06:27,798
of code is to have a
就是用

184
00:06:27,798 --> 00:06:28,968
for loop that says, you
一个 for 循环

185
00:06:28,968 --> 00:06:31,682
know, for j equals 0,
就是让 j 等于0

186
00:06:31,682 --> 00:06:33,305
1 through 2 the update
1 2

187
00:06:33,305 --> 00:06:35,603
theta J or something like that.
来更新 θ j

188
00:06:35,603 --> 00:06:36,760
But instead, let's come up
但让我们

189
00:06:36,760 --> 00:06:40,975
with a vectorized implementation and see if we can have a simpler way.
用向量化的方式来实现 并看看我们是否能够有一个更简单的方法

190
00:06:40,975 --> 00:06:42,711
So, basically compress these three
基本上用三行代码

191
00:06:42,757 --> 00:06:44,314
lines of code or a
或者一个 for 循环

192
00:06:44,314 --> 00:06:48,518
for loop that, you know, effectively does these 3 sets, 1 set at a time.
一次实现这三个方程

193
00:06:48,518 --> 00:06:49,688
Let's see who can these 3
让我们来看看谁能用这三步

194
00:06:49,688 --> 00:06:51,402
steps and compress them into
并将它们压缩成

195
00:06:51,402 --> 00:06:53,972
1 line of vectorized code.
一行向量化的代码

196
00:06:53,976 --> 00:06:55,476
Here's the idea.
这个想法是

197
00:06:55,480 --> 00:06:56,462
What I'm going to do is I'm
我打算

198
00:06:56,462 --> 00:06:59,131
going to think of theta
把 θ 看做一个向量

199
00:06:59,131 --> 00:07:00,633
as a vector and I'm
然后我用

200
00:07:00,633 --> 00:07:04,214
going to update theta as theta
θ 减去

201
00:07:04,270 --> 00:07:07,468
minus alpha times some
α 乘以 某个别的向量

202
00:07:07,468 --> 00:07:11,650
other vector, delta, where
δ 来更新 θ

203
00:07:11,650 --> 00:07:13,689
delta is going to be
这里的 δ 等于 m 分之 1

204
00:07:13,700 --> 00:07:15,876
equal to 1 over
equal to 1 over

205
00:07:15,876 --> 00:07:18,408
m, sum from I equals
i=1 到 m 加和

206
00:07:18,450 --> 00:07:22,151
one through m and then
one through m and then

207
00:07:22,180 --> 00:07:25,570
this term on the
然后这个表达式

208
00:07:25,720 --> 00:07:28,118
right, okay?
好吗?

209
00:07:28,118 --> 00:07:31,205
So, let me explain what's going on here.
让我解释一下是怎么回事

210
00:07:31,220 --> 00:07:32,666
Here, I'm going to treat
在这里 我要把

211
00:07:32,666 --> 00:07:35,322
theta as a vector
θ 看作一个向量

212
00:07:35,350 --> 00:07:38,106
so, there's an N+1 dimensional vector.
有一个 n+1 维向量

213
00:07:38,110 --> 00:07:40,291
I'm saying that theta gets, you know, updated
我是说 θ 被更新

214
00:07:40,310 --> 00:07:43,922
as--that's the vector, our N+1.
我们的 n+1 维向量

215
00:07:43,922 --> 00:07:45,319
Alpha is a real
α 是一个实数

216
00:07:45,319 --> 00:07:47,395
number and delta
δ

217
00:07:47,410 --> 00:07:49,941
here is a vector.
在这里是一个向量

218
00:07:49,960 --> 00:07:54,278
So, this subtraction operation, that's a vector subtraction.
所以这个减法运算是一个向量减法

219
00:07:54,278 --> 00:07:55,255
Okay?
清楚吗 ?

220
00:07:55,255 --> 00:07:56,977
Because alpha times delta
因为 α 乘以 δ

221
00:07:56,977 --> 00:07:58,385
is a vector and so
是一个向量 所以

222
00:07:58,385 --> 00:08:00,369
I'm saying if theta gets, you know, this
θ 就是 θ 减去 α 乘以 δ 得到的向量

223
00:08:00,369 --> 00:08:04,217
vector, alpha times delta subtracted from it.
vector, alpha times delta subtracted from it.

224
00:08:04,240 --> 00:08:06,563
So, what is the vector delta?
那么什么是向量 δ 呢 ?

225
00:08:06,563 --> 00:08:10,220
Well, this vector delta looks like this.
嗯 向量 δ 是这样子的

226
00:08:10,256 --> 00:08:12,092
And what this meant to
这实际上代表的是

227
00:08:12,092 --> 00:08:14,595
be is really meant to be
be is really meant to be

228
00:08:14,620 --> 00:08:17,102
this thing over here.
这部分内容

229
00:08:17,140 --> 00:08:19,200
Concretely, delta will be
具体地说 δ 将成为

230
00:08:19,220 --> 00:08:22,165
a N+1 dimensional vector and
n +1 维向量

231
00:08:22,165 --> 00:08:23,978
the very first element of
并且向量的第一个元素

232
00:08:23,978 --> 00:08:27,767
the vector delta is going to be equal to that.
就等于这个

233
00:08:27,770 --> 00:08:29,513
So, if we have
所以我们的 δ

234
00:08:29,513 --> 00:08:31,565
the delta, you know, if we index it
如果要写下标的话

235
00:08:31,565 --> 00:08:34,469
from 0--this is delta 0, delta 1, delta 2.
就是从零开始 δ0 δ1 δ2

236
00:08:34,469 --> 00:08:36,541
What I want is that
我想要的是

237
00:08:36,560 --> 00:08:39,033
delta 0 is equal
δ0 等于

238
00:08:39,040 --> 00:08:41,267
to, you know, this
这个

239
00:08:41,267 --> 00:08:42,359
first box also green up
第一行绿色框起来的部分

240
00:08:42,360 --> 00:08:45,306
above and indeed, you might
事实上 你可能会

241
00:08:45,306 --> 00:08:47,108
be able to convince yourself that delta
写出 δ0 是

242
00:08:47,108 --> 00:08:48,681
0 is this 1 of m,
m 分之 1

243
00:08:48,681 --> 00:08:50,102
sum of, you know, h of
h(x) 减去 y(i)

244
00:08:50,102 --> 00:08:53,356
x.   xi minus
乘以 x(i)0 的和

245
00:08:53,400 --> 00:08:58,315
yi times xi0.
yi times xi0.

246
00:08:58,315 --> 00:08:59,748
So, let's just make
所以让我们

247
00:08:59,748 --> 00:09:01,064
sure that we're on the
在同一页上

248
00:09:01,064 --> 00:09:03,998
same page about how delta really is computed.
计算真正的 δ

249
00:09:03,998 --> 00:09:05,488
Delta is one of m
δ 就是 m 分之 1

250
00:09:05,488 --> 00:09:08,284
times the sum over here
乘以这个和

251
00:09:08,284 --> 00:09:09,871
and, you know, what is this sum?
那这个和是什么 ?

252
00:09:09,871 --> 00:09:11,426
Well, this term over
恩 这个符号

253
00:09:11,426 --> 00:09:17,115
here, that's a real number.
是一个实数

254
00:09:17,150 --> 00:09:21,219
And the second term over here, xi.
这里的第二个符号 是 x(i)

255
00:09:21,219 --> 00:09:23,892
This term over there is a
这个符号是一个向量

256
00:09:23,910 --> 00:09:26,109
vector, right? Because xi might
对吧 ? 因为 x(i)

257
00:09:26,109 --> 00:09:26,982
be a vector.
可能是一个向量

258
00:09:26,990 --> 00:09:29,630
That would be
这将是

259
00:09:29,975 --> 00:09:36,115
xi0, xi1, xi2 right?
x(i)0 x(i)1 x(i)2 对吧 ?

260
00:09:36,130 --> 00:09:38,246
And what is the summation?
那这个求和是什么 ?

261
00:09:38,246 --> 00:09:40,241
Well, what does summation say
恩 这个求和就是

262
00:09:40,250 --> 00:09:43,292
is that this term
这里的式子

263
00:09:43,502 --> 00:09:46,555
over here.
就在这里

264
00:09:47,280 --> 00:09:54,801
This is equal to h+x1-y1 times
等于 h(x(1)) - y(1) 乘以 x(1)

265
00:09:54,870 --> 00:09:59,099
x1 + h of
加上

266
00:09:59,115 --> 00:10:02,778
x2-y2 times x2
h(x(2)) - y(2) 乘以 x(2)

267
00:10:02,778 --> 00:10:05,396
+ you know, and so on.
依此类推

268
00:10:05,396 --> 00:10:06,404
Okay?
对吧 ?

269
00:10:06,404 --> 00:10:07,420
Because this is a summation of
因为这是对 i 的加和

270
00:10:07,420 --> 00:10:09,013
the I. So, as I
所以

271
00:10:09,013 --> 00:10:11,345
ranges from I1 through m,
当 i 从 1 到 m

272
00:10:11,345 --> 00:10:15,144
you get these different terms and you're summing up these terms.
你就会得到这些不同的式子 然后作加和

273
00:10:15,160 --> 00:10:16,221
And the meaning of each of these
每个式子的意思

274
00:10:16,221 --> 00:10:18,262
terms is a lot like
很像

275
00:10:18,262 --> 00:10:19,807
- if you remember actually from
如果你还记得实际上

276
00:10:19,807 --> 00:10:24,100
the earlier quiz in this, if you solve this equation.
在以前的一个小测验 如果你要解这个方程

277
00:10:24,110 --> 00:10:25,560
We said that in order to
我们说过

278
00:10:25,560 --> 00:10:27,250
vectorize this code, we
为了向量化这段代码

279
00:10:27,250 --> 00:10:30,755
will instead set u2v+5w. So,
我们会令 u = 2v +5w 因此

280
00:10:30,770 --> 00:10:32,391
we're saying that the vector u
我们说 向量u

281
00:10:32,391 --> 00:10:33,706
is equal to 2 times
等于2乘以向量v

282
00:10:33,706 --> 00:10:35,568
the vector v plus 5 times
加上 5乘以向量 w

283
00:10:35,570 --> 00:10:37,198
the vector w. So, just an
用这个例子说明

284
00:10:37,198 --> 00:10:39,023
example of how to
如何对不同的向量进行相加

285
00:10:39,023 --> 00:10:42,453
add different vectors and this summation is the same thing.
这里的求和是同样的道理

286
00:10:42,453 --> 00:10:44,919
It's a saying that this
这一部分

287
00:10:44,950 --> 00:10:49,766
summation over here is just some real number right?
只是一个实数

288
00:10:49,840 --> 00:10:50,996
That's kind of like the number
就有点像数字 2

289
00:10:51,010 --> 00:10:52,698
2 and some other number
而这里是别的一些数字

290
00:10:52,711 --> 00:10:54,085
times the vector x1.
来乘以向量x1

291
00:10:54,085 --> 00:10:56,792
This is like 2 times v instead
这就像是 2v

292
00:10:56,792 --> 00:10:59,177
with some other number times x1
只不过用别的数字乘以 x1

293
00:10:59,177 --> 00:11:01,712
and then plus, you know, instead of
然后加上 你知道

294
00:11:01,712 --> 00:11:03,475
5xw, we instead have some
不是5w 而是用

295
00:11:03,475 --> 00:11:05,212
other real number plus some
别的实数乘以

296
00:11:05,212 --> 00:11:06,850
other vector and then you
一个别的向量 然后你

297
00:11:06,860 --> 00:11:08,909
add on other vectors, you know,
加上其他的向量

298
00:11:08,909 --> 00:11:10,528
plus ... plus the other
plus ... plus the other

299
00:11:10,540 --> 00:11:12,234
vectors, which is why
这就是为什么

300
00:11:12,234 --> 00:11:15,178
overall, this thing
总体而言

301
00:11:15,178 --> 00:11:17,015
over here, that whole
在这里 这整个量

302
00:11:17,015 --> 00:11:19,745
quantity, that delta is
δ 就是一个向量

303
00:11:19,770 --> 00:11:23,685
just some vector, and concretely, the
具体而言

304
00:11:23,685 --> 00:11:26,373
3 elements of delta correspond
对应这三个 δ 的元素

305
00:11:26,373 --> 00:11:28,813
if n2, the 3 elements
如果n等于2

306
00:11:28,820 --> 00:11:31,512
of delta correspond exactly to
δ 的三个元素一一对应

307
00:11:31,512 --> 00:11:33,349
this thing to the second
这个

308
00:11:33,349 --> 00:11:35,075
thing and this third
第二个 以及这第三个

309
00:11:35,075 --> 00:11:36,401
thing, which is why
式子 这就是为什么

310
00:11:36,410 --> 00:11:38,299
when you update theta, according to
当您更新 θ 值时 根据

311
00:11:38,299 --> 00:11:40,979
theta minus alpha delta,
θ - αθ 这个式子

312
00:11:41,010 --> 00:11:42,760
we end up having exactly the
我们最终能得到完全符合最上方更新规则的

313
00:11:42,830 --> 00:11:44,948
same simultaneous updates as the
同步更新

314
00:11:44,960 --> 00:11:47,825
update rules that we have on top.
update rules that we have on top.

315
00:11:47,840 --> 00:11:48,960
So, I know that there
我知道

316
00:11:48,960 --> 00:11:50,466
was a lot that happened on
幻灯片上的内容很多

317
00:11:50,500 --> 00:11:52,608
the slides, but again, feel
但是再次重申

318
00:11:52,650 --> 00:11:54,489
free to pause the video and
请随时暂停视频

319
00:11:54,510 --> 00:11:56,592
I either encourage you to
我也鼓励你

320
00:11:56,592 --> 00:11:58,247
step through the difference. If
一步步对比这两者的差异

321
00:11:58,247 --> 00:11:59,451
you're unsure of what just happen,
如果你不清楚刚才的内容

322
00:11:59,451 --> 00:12:01,719
I encourage you to step through
我希望你能一步一步读幻灯片的内容

323
00:12:01,719 --> 00:12:02,940
the slide to make sure you
以确保你理解

324
00:12:02,940 --> 00:12:04,578
understand why is it
为什么这个式子

325
00:12:04,580 --> 00:12:07,048
that this update here with
用 δ 的这个定理

326
00:12:07,060 --> 00:12:09,612
this definition of delta, right?
定义的 好吗 ?

327
00:12:09,612 --> 00:12:10,943
Why is it that that equal
以及它为什么

328
00:12:10,943 --> 00:12:13,714
to this update on top and
和最上面的更新方式是等价的

329
00:12:13,714 --> 00:12:15,033
it's still not clear when insight is
为什么是这样子的

330
00:12:15,033 --> 00:12:18,395
that, you know, this thing over here.
你知道 就是这里的式子

331
00:12:18,400 --> 00:12:20,628
That's exactly the vector
这就是向量 x

332
00:12:20,628 --> 00:12:22,109
x and so, we're
而我们只是用了

333
00:12:22,109 --> 00:12:23,342
just taking, you know, all
你知道

334
00:12:23,342 --> 00:12:25,516
3 of these computations and compressing
这三个计算式并且压缩

335
00:12:25,516 --> 00:12:27,106
them into one step
成一个步骤

336
00:12:27,106 --> 00:12:29,778
with the this vector delta,
用这个向量 δ

337
00:12:29,778 --> 00:12:31,292
which is why we can come
这就是为什么我们能够

338
00:12:31,292 --> 00:12:33,465
up with a vectorized implementation of
矢量化地实现

339
00:12:33,490 --> 00:12:36,942
this step of linear regression this way.
线性回归

340
00:12:36,942 --> 00:12:38,639
So I hope this
所以 我希望

341
00:12:38,660 --> 00:12:40,660
step makes sense, and do
步骤是有逻辑的

342
00:12:40,660 --> 00:12:41,791
look at the video and make
请务必看视频 并且保证

343
00:12:41,791 --> 00:12:44,013
sure and see if you can understand it.
你确实能理解它

344
00:12:44,013 --> 00:12:46,058
In case you don't understand The
万一你是在不能理解

345
00:12:46,058 --> 00:12:48,029
equivalence of this math if
它们数学上等价的原因

346
00:12:48,029 --> 00:12:49,435
you implement this, this turns
你就直接实现这个算法

347
00:12:49,435 --> 00:12:50,944
out to be the right answer anyway,
算是能得到正确答案的

348
00:12:50,944 --> 00:12:52,224
so even if you didn't
所以即使你没有

349
00:12:52,224 --> 00:12:56,403
quite understand the equivalence, if you just implement it this way,
完全理解为何是等价的 如果只是实现这种算法

350
00:12:56,410 --> 00:12:58,992
you'll be able to get linear regressions to work.
你仍然实现线性回归算法

351
00:12:58,992 --> 00:13:00,663
So, if you're able to
所以如果你能

352
00:13:00,663 --> 00:13:02,216
figure out why these 2 steps
弄清楚为什么这两个步骤是等价的

353
00:13:02,216 --> 00:13:04,122
are equivalent then hopefully that
那我希望你可以对

354
00:13:04,122 --> 00:13:06,239
would give you a better understanding of vectorization
向量化有一个更好的理解

355
00:13:06,239 --> 00:13:10,121
as well, and finally,
以及 最后

356
00:13:10,121 --> 00:13:12,355
if you're implementing linear
如果你在实现线性回归的时候

357
00:13:12,370 --> 00:13:14,872
regression using more than one or two features.
使用一个或两个以上的特征量

358
00:13:14,872 --> 00:13:16,548
So, sometimes we use linear
有时我们使用

359
00:13:16,550 --> 00:13:18,078
regression with tens or hundreds
几十或几百个特征量

360
00:13:18,078 --> 00:13:19,968
thousands of features, but if
来计算线性归回

361
00:13:19,980 --> 00:13:21,853
you use the vectorized implementation
当你使用向量化地实现

362
00:13:21,853 --> 00:13:23,735
of linear regression, usually that
线性回归

363
00:13:23,735 --> 00:13:25,605
will run much faster than if
通常运行速度就会比你以前用

364
00:13:25,605 --> 00:13:26,892
you had say your old
你的 for 循环快的多

365
00:13:26,892 --> 00:13:28,163
for loop that was you
也就是自己

366
00:13:28,163 --> 00:13:31,485
know, updating theta 0 then theta 1 then theta 2 yourself.
写代码更新 θ0 θ1 θ2

367
00:13:31,500 --> 00:13:33,769
So, using a vectorized implementation, you
因此使用向量化实现方式

368
00:13:33,769 --> 00:13:34,688
should be able to get a
你应该是能够得到

369
00:13:34,688 --> 00:13:37,762
much more efficient implementation of linear regression.
一个高效得多的线性回归算法

370
00:13:37,790 --> 00:13:39,347
And when you vectorize later
而当你向量化

371
00:13:39,347 --> 00:13:40,430
algorithms that we'll see in
我们将在之后的课程里面学到的算法

372
00:13:40,430 --> 00:13:41,554
this class is a good
这会是一个很好的技巧

373
00:13:41,554 --> 00:13:43,367
trick whether an octave
无论是对于 Octave 或者

374
00:13:43,367 --> 00:13:44,767
or some of the language, the C++
一些其他的语言 如C++

375
00:13:44,767 --> 00:13:48,474
Java for getting your code to run more efficiently.
Java 来让你的代码运行得更高效 【果壳教育无边界字幕组】翻译：Jaminalia  校对：所罗门捷列夫

