1
00:00:00,210 --> 00:00:01,300
In the last video, I talked
在上一节课中
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,600 --> 00:00:03,390
about how when faced with
我讲到了应当怎样面对

3
00:00:03,520 --> 00:00:04,780
a machine learning problem, there are
机器学习问题

4
00:00:04,980 --> 00:00:07,260
often lots of different ideas on how to improve the algorithm.
有很多提高算法表现的方法

5
00:00:08,460 --> 00:00:09,510
In this video let's
在本次课程中

6
00:00:09,650 --> 00:00:11,060
talk about the concepts of error
我们将会讲到

7
00:00:11,330 --> 00:00:12,980
analysis which will help
误差分析（error analysis）的概念

8
00:00:13,070 --> 00:00:13,980
me give you a way to more
这会帮助你

9
00:00:14,300 --> 00:00:15,830
systematically make some of these decisions.
更系统地做出决定

10
00:00:18,070 --> 00:00:19,420
If you're starting work on a
如果你准备

11
00:00:19,540 --> 00:00:21,210
machine learning product or building
研究机器学习的东西

12
00:00:21,400 --> 00:00:23,340
a machine learning application, it is
或者构造机器学习应用程序

13
00:00:23,480 --> 00:00:24,880
often considered very good practice
最好的实践方法

14
00:00:25,840 --> 00:00:27,000
to start, not by building
不是建立一个

15
00:00:27,520 --> 00:00:29,070
a very complicated system with
非常复杂的系统

16
00:00:29,220 --> 00:00:30,490
lots of complex features and so
拥有多么复杂的变量

17
00:00:30,930 --> 00:00:32,450
on, but to instead start
而是

18
00:00:33,060 --> 00:00:34,120
by building a very simple
构建一个简单的算法

19
00:00:34,510 --> 00:00:35,760
algorithm, the you can implement quickly.
这样你可以很快地实现它

20
00:00:37,480 --> 00:00:38,610
And when I start on
每当我研究

21
00:00:38,740 --> 00:00:39,770
a learning problem, what I usually
机器学习的问题时

22
00:00:40,150 --> 00:00:41,350
do is spend at most one
我最多只会花一天的时间

23
00:00:41,570 --> 00:00:43,160
day, literally at most 24
就是字面意义上的24小时

24
00:00:43,460 --> 00:00:46,030
hours to try to get something really quick and dirty.
来试图很快的把结果搞出来 即便效果不好

25
00:00:47,040 --> 00:00:48,550
Frankly not at all sophisticated system.
坦白的说 就是根本没有用复杂的系统

26
00:00:49,370 --> 00:00:50,310
But get something really quick and
但是只是很快的得到的结果

27
00:00:50,400 --> 00:00:52,080
dirty running and implement
即便运行得不完美

28
00:00:52,590 --> 00:00:53,710
it and then test it on
但是也把它运行一遍

29
00:00:53,880 --> 00:00:55,870
my cross validation data. Once
最后通过交叉验证来检验数据

30
00:00:56,050 --> 00:00:57,140
you've done that, you can
一旦做完

31
00:00:57,480 --> 00:00:58,690
then plot learning curves.
你可以画出学习曲线

32
00:00:59,960 --> 00:01:02,670
This is what we talked about in the previous set of videos.
这个我们在前面的课程中已经讲过了

33
00:01:03,230 --> 00:01:05,160
But plot learning curves of the
通过画出学习曲线

34
00:01:05,370 --> 00:01:07,120
training and test errors to
以及检验误差

35
00:01:07,310 --> 00:01:08,280
try to figure out if your
来找出

36
00:01:08,400 --> 00:01:09,630
learning algorithm may be suffering
你的算法是否有

37
00:01:10,120 --> 00:01:11,240
from high bias or high
高偏差和高方差的问题

38
00:01:11,440 --> 00:01:13,180
variance or something else and
或者别的问题

39
00:01:13,440 --> 00:01:14,380
use that to try to
在这样分析之后

40
00:01:14,490 --> 00:01:15,610
decide if having more data
再来决定用更多的数据训练

41
00:01:16,080 --> 00:01:17,990
and more features and so on are likely to help.
或者加入更多的特征变量是否有用

42
00:01:18,670 --> 00:01:19,830
And the reason that this
这么做的原因是

43
00:01:20,000 --> 00:01:20,980
is a good approach is often
这在你刚接触机器学习问题时

44
00:01:21,940 --> 00:01:22,980
when you're just starting out on
是一个很好的方法

45
00:01:23,100 --> 00:01:24,460
a learning problem, there's really no
你并不能

46
00:01:24,680 --> 00:01:25,820
way to tell in advance
提前知道

47
00:01:26,480 --> 00:01:27,360
whether you need more complex
你是否需要复杂的特征变量

48
00:01:27,790 --> 00:01:29,200
features or whether you need
或者你是否需要

49
00:01:29,250 --> 00:01:30,950
more data or something else.
更多的数据 还是别的什么

50
00:01:31,280 --> 00:01:32,270
And it's just very hard to tell
提前知道你应该做什么

51
00:01:32,510 --> 00:01:33,840
in advance, that is in
是非常难的

52
00:01:33,970 --> 00:01:36,040
the absence of evidence, in
因为你缺少证据

53
00:01:36,160 --> 00:01:37,840
the absence of seeing a
缺少学习曲线

54
00:01:37,970 --> 00:01:39,130
learning curve, it's just incredibly
因此 你很难知道

55
00:01:39,750 --> 00:01:42,860
difficult to figure out where you should be spending your time.
你应该把时间花在什么地方来提高算法的表现

56
00:01:43,760 --> 00:01:45,360
And it's often by implementing even
但是当你实践一个

57
00:01:45,730 --> 00:01:46,670
a very, very quick and dirty
非常简单即便不完美的

58
00:01:46,980 --> 00:01:48,100
implementation and by plotting
方法时

59
00:01:48,540 --> 00:01:51,070
learning curves that that helps you make these decisions.
你可以通过画出学习曲线来做出进一步的选择

60
00:01:52,580 --> 00:01:53,340
So if you like, you can think
你可以

61
00:01:53,560 --> 00:01:54,490
of this as a way of
用这种方式

62
00:01:54,620 --> 00:01:56,270
avoiding what's sometimes called
来避免一种

63
00:01:56,570 --> 00:01:58,950
premature optimization in computer programming.
电脑编程里的过早优化问题

64
00:02:00,000 --> 00:02:01,070
And this is idea that just
这种理念是

65
00:02:01,200 --> 00:02:03,130
says that we should let
我们必须

66
00:02:03,460 --> 00:02:04,920
evidence guide our decisions
用证据来领导我们的决策

67
00:02:05,650 --> 00:02:06,540
on where to spend our time
怎样分配自己的时间来优化算法

68
00:02:07,160 --> 00:02:08,150
rather than use gut feeling,
而不是仅仅凭直觉

69
00:02:09,070 --> 00:02:09,680
which is often wrong.
凭直觉得出的东西一般总是错误的

70
00:02:10,930 --> 00:02:12,120
In addition to plotting learning
除了画出学习曲线之外

71
00:02:12,390 --> 00:02:13,540
curves, one other thing
一件非常有用的事是

72
00:02:13,810 --> 00:02:16,440
that's often very useful to do is what's called error analysis.
误差分析

73
00:02:18,120 --> 00:02:19,080
And what I mean by that is
我的意思是说

74
00:02:19,280 --> 00:02:20,520
that when building, say
当我们在构造

75
00:02:20,770 --> 00:02:22,190
a spam classifier, I will
比如构造垃圾邮件分类器时

76
00:02:22,470 --> 00:02:24,500
often look at my
我会看一看

77
00:02:24,730 --> 00:02:26,690
cross validation set and manually
我的交叉验证数据集

78
00:02:27,360 --> 00:02:29,110
look at the emails that my
然后亲自看一看

79
00:02:29,310 --> 00:02:30,910
algorithm is making errors on.
哪些邮件被算法错误地分类

80
00:02:31,180 --> 00:02:32,250
So, look at the spam emails
因此 通过这些

81
00:02:32,630 --> 00:02:34,440
and non-spam emails that the
被算法错误分类的垃圾邮件

82
00:02:34,640 --> 00:02:36,920
algorithm is misclassifying, and see
与非垃圾邮件

83
00:02:37,430 --> 00:02:38,590
if you can spot any systematic
你可以发现某些系统性的规律

84
00:02:39,210 --> 00:02:41,300
patterns in what type of examples it is misclassifying.
什么类型的邮件总是被错误分类

85
00:02:42,980 --> 00:02:44,560
And often by doing that, this
经常地 这样做之后

86
00:02:44,810 --> 00:02:45,960
is the process that would inspire
这个过程能启发你

87
00:02:47,170 --> 00:02:48,800
you to design new features.
构造新的特征变量

88
00:02:49,430 --> 00:02:50,420
Or they'll tell you whether the
或者告诉你

89
00:02:50,920 --> 00:02:52,150
current things or current
现在

90
00:02:52,400 --> 00:02:53,290
shortcomings of the system
这个系统的短处

91
00:02:54,270 --> 00:02:55,550
and give you the inspiration you
然后启发你

92
00:02:55,660 --> 00:02:57,680
need to come up with improvements to it.
如何去提高它

93
00:02:58,260 --> 00:03:00,070
Concretely, here's a specific example.
具体地说 这里有一个例子

94
00:03:01,350 --> 00:03:02,360
Let's say you've built a spam
假设你正在构造一个

95
00:03:02,780 --> 00:03:05,740
classifier and you
垃圾邮件分类器

96
00:03:05,840 --> 00:03:07,720
have 500 examples in
你拥有500个实例

97
00:03:07,940 --> 00:03:09,650
your cross-validation set.
在交叉验证集中

98
00:03:10,410 --> 00:03:11,760
And let's say in this example, that the
假设在这个例子中

99
00:03:12,010 --> 00:03:13,060
algorithm has a very high error
该算法有非常高的误差率

100
00:03:13,340 --> 00:03:14,640
rate, and it misclassifies a
它错误分类了

101
00:03:14,910 --> 00:03:16,500
hundred of these cross-validation examples.
一百个交叉验证实例

102
00:03:18,770 --> 00:03:19,850
So what I do is manually
所以我要做的是

103
00:03:20,450 --> 00:03:22,370
examine these 100 errors, and
人工检查这100个错误

104
00:03:22,530 --> 00:03:24,450
manually categorize them, based
然后手工为它们分类

105
00:03:24,700 --> 00:03:25,810
on things like what type
基于例如

106
00:03:25,980 --> 00:03:27,110
of email it is and
这些是什么类型的邮件

107
00:03:27,270 --> 00:03:28,630
what cues or what features you
哪些变量

108
00:03:28,710 --> 00:03:31,130
think might have helped the algorithm classify them incorrectly.
能帮助这个算法来正确分类它们

109
00:03:32,450 --> 00:03:33,880
So, specifically, by what
明确地说

110
00:03:34,080 --> 00:03:35,050
type of email it is,
通过鉴定这是哪种类型的邮件

111
00:03:35,560 --> 00:03:36,870
you know, if I look through these
通过检查

112
00:03:37,140 --> 00:03:38,180
hundred errors I may find
这一百封错误分类的邮件

113
00:03:38,520 --> 00:03:39,660
that maybe the most
我可能会发现

114
00:03:39,970 --> 00:03:41,350
common types of spam
最容易被误分类的邮件

115
00:03:41,840 --> 00:03:43,450
emails in misclassifies are maybe
可能是

116
00:03:44,010 --> 00:03:45,610
emails on pharmacy, so basically
有关药物的邮件

117
00:03:45,610 --> 00:03:48,300
these are emails trying to
基本上这些邮件都是卖药的

118
00:03:48,610 --> 00:03:50,000
sell drugs, maybe emails that are
或者

119
00:03:50,180 --> 00:03:51,740
trying to sell replicas -
卖仿品的

120
00:03:51,760 --> 00:03:54,330
those are those fake watches fake you know, random things.
比如卖假表

121
00:03:56,160 --> 00:03:59,410
Maybe have some emails trying to steal passwords.
或者一些骗子邮件

122
00:04:00,240 --> 00:04:01,400
These are also called phishing emails.
又叫做钓鱼邮件

123
00:04:02,180 --> 00:04:04,690
But that's another big category of emails and maybe other categories.
等等

124
00:04:06,160 --> 00:04:07,800
So, in terms
所以

125
00:04:08,120 --> 00:04:09,230
of classify what type of email
在检查哪些邮件被错误分类的时候

126
00:04:09,530 --> 00:04:10,420
it is, I would actually go through
我会看一看每封邮件

127
00:04:10,890 --> 00:04:11,990
and count up, you know, of
数一数 比如

128
00:04:12,200 --> 00:04:14,220
my 100 emails, maybe I
在这100封错误归类的邮件中

129
00:04:14,400 --> 00:04:15,510
find that twelve of the
我发现有12封

130
00:04:15,620 --> 00:04:17,600
mislabeled emails are pharma emails.
错误归类的邮件是和卖药有关的邮件

131
00:04:18,100 --> 00:04:19,460
And maybe four of them
4封

132
00:04:19,700 --> 00:04:20,840
are emails trying to sell
是推销仿品的

133
00:04:20,980 --> 00:04:22,680
replicas, they sell fake watches or something.
推销假表或者别的东西

134
00:04:23,720 --> 00:04:25,060
And maybe I find that 53
然后我发现

135
00:04:25,650 --> 00:04:26,970
of them are these,
有53封邮件

136
00:04:27,720 --> 00:04:29,480
what's called phishing emails, basically emails
是钓鱼邮件

137
00:04:29,730 --> 00:04:30,900
trying to persuade you to
诱骗你

138
00:04:31,020 --> 00:04:32,760
give them your password, and 31 emails are other types of emails.
告诉他们你的密码 剩下的31封别的类型的邮件

139
00:04:35,330 --> 00:04:37,210
And it's by counting up the
通过算出

140
00:04:37,280 --> 00:04:38,280
number of emails in these
每个类别中

141
00:04:38,430 --> 00:04:39,540
different categories that you might
不同的邮件数

142
00:04:39,790 --> 00:04:41,570
discover, for example, that the
你可能会发现 比如

143
00:04:41,870 --> 00:04:43,100
algorithm is doing really particularly
该算法在

144
00:04:44,170 --> 00:04:45,640
poorly on emails trying to
区分钓鱼邮件的时候

145
00:04:45,780 --> 00:04:47,240
steal passwords, and that
总是表现得很差

146
00:04:47,400 --> 00:04:49,230
may suggest that it might
这说明

147
00:04:49,380 --> 00:04:50,490
be worth your effort to look
你应该花更多的时间

148
00:04:50,690 --> 00:04:51,650
more carefully at that type
来研究这种类型的邮件

149
00:04:51,900 --> 00:04:53,350
of email, and see if
然后

150
00:04:53,450 --> 00:04:54,450
you can come up with better features
看一看你是否能通过构造更好的特征变量

151
00:04:55,070 --> 00:04:56,280
to categorize them correctly.
来正确区分这种类型的邮件

152
00:04:57,550 --> 00:04:58,930
And also, what I might
同时

153
00:04:59,000 --> 00:05:00,130
do is look at what cues,
我要做的是

154
00:05:00,550 --> 00:05:02,120
or what features, additional features
看一看哪些特征变量

155
00:05:02,620 --> 00:05:04,920
might have helped the algorithm classify the emails.
可能会帮助算法正确地分类邮件

156
00:05:06,090 --> 00:05:06,970
So let's say that some of
我们假设

157
00:05:07,060 --> 00:05:09,700
our hypotheses about things or
能帮助我们提高

158
00:05:09,840 --> 00:05:10,780
features that might help us
邮件分类表现

159
00:05:10,920 --> 00:05:13,240
classify emails better are trying
的方法是

160
00:05:13,490 --> 00:05:15,600
to detect deliberate misspellings versus
检查有意的拼写错误

161
00:05:16,220 --> 00:05:18,610
unusual email routing versus unusual, you know,
不寻常的邮件路由来源

162
00:05:19,950 --> 00:05:21,450
spamming punctuation, such as
以及垃圾邮件特有的标点符号方式

163
00:05:21,790 --> 00:05:23,230
people use a lot of exclamation marks.
比如很多感叹号

164
00:05:23,700 --> 00:05:24,470
And once again, I would manually
与之前一样

165
00:05:24,860 --> 00:05:25,670
go through and let's say
我会手动地浏览这些邮件

166
00:05:25,760 --> 00:05:27,490
I find five cases of
假设有5封这种类型的邮件

167
00:05:27,620 --> 00:05:29,400
this, and 16 of
16封这种类型的

168
00:05:29,500 --> 00:05:30,560
this, and 32 of this and
32封这种类型的

169
00:05:31,180 --> 00:05:33,620
a bunch of other types of emails as well.
以及一些别的类型的

170
00:05:34,770 --> 00:05:36,180
And if this is what
如果

171
00:05:36,350 --> 00:05:37,470
you get on your cross validation
这就是你从交叉验证中得到的结果

172
00:05:38,070 --> 00:05:39,170
set then it really tells
那么

173
00:05:39,300 --> 00:05:41,060
you that, you know, maybe deliberate spelling
这可能说明

174
00:05:41,660 --> 00:05:42,730
is a sufficiently rare phenomenon
有意地拼写错误出现频率较少

175
00:05:43,500 --> 00:05:44,480
that maybe is not really worth
这可能并不值得

176
00:05:44,840 --> 00:05:47,120
all your time trying to write
你花费时间

177
00:05:47,710 --> 00:05:48,780
algorithms to detect that.
去编写算法来检测这种类型的邮件

178
00:05:49,480 --> 00:05:50,480
But if you find a lot
但是如果你发现

179
00:05:50,780 --> 00:05:52,070
of spammers are using, you
很多的垃圾邮件

180
00:05:52,140 --> 00:05:54,150
know, unusual punctuation then
都有不一般的标点符号规律

181
00:05:54,290 --> 00:05:55,250
maybe that's a strong sign
那么这是一个很强的特征

182
00:05:55,670 --> 00:05:56,730
that it might actually be
说明你应该

183
00:05:57,000 --> 00:05:58,510
worth your while to spend
花费你的时间

184
00:05:58,780 --> 00:06:00,280
the time to develop more sophisticated
去构造基于标点符号的

185
00:06:00,910 --> 00:06:02,190
features based on the punctuation.
更加复杂的特征变量

186
00:06:03,330 --> 00:06:04,870
So, this sort of error
因此

187
00:06:05,040 --> 00:06:06,390
analysis which is really
这种类型的误差分析

188
00:06:06,690 --> 00:06:08,430
the process of manually examining
是一种手动检测的过程

189
00:06:09,190 --> 00:06:10,540
the mistakes that the algorithm
检测算法可能会犯的错误

190
00:06:10,780 --> 00:06:12,220
makes, can often help
这经常能够帮助你

191
00:06:12,560 --> 00:06:14,620
guide you to the most fruitful avenues to pursue.
找到更为有效的手段

192
00:06:16,000 --> 00:06:17,410
And this also explains why I
这也解释了为什么

193
00:06:17,590 --> 00:06:19,260
often recommend implementing a quick
我总是推荐先实践一种

194
00:06:19,550 --> 00:06:21,250
and dirty implementation of an algorithm.
快速即便不完美的算法

195
00:06:22,040 --> 00:06:22,940
What we really want to do
我们真正想要的是

196
00:06:23,260 --> 00:06:24,290
is figure out what are
找出什么类型的邮件

197
00:06:24,310 --> 00:06:26,770
the most difficult examples for an algorithm to classify.
是这种算法最难分类出来的

198
00:06:27,860 --> 00:06:29,920
And very often for different
对于不同的算法

199
00:06:30,460 --> 00:06:31,730
algorithms, for different learning algorithms,
不同的机器学习算法

200
00:06:32,010 --> 00:06:33,500
they'll often find, you
它们

201
00:06:33,560 --> 00:06:35,920
know, similar categories of examples difficult.
所遇到的问题一般总是相同的

202
00:06:37,010 --> 00:06:37,970
And by having a quick and
通过实践一些快速

203
00:06:38,060 --> 00:06:39,840
dirty implementation, that's often a
即便不完美的算法

204
00:06:39,910 --> 00:06:40,850
quick way to let you
你能够更快地

205
00:06:41,430 --> 00:06:43,070
identify some errors and quickly
找到错误的所在

206
00:06:43,620 --> 00:06:44,690
identify what are the
并且快速找出算法难以处理的例子

207
00:06:44,790 --> 00:06:47,760
hard examples so that you can focus your efforts on those.
这样你就能集中精力在这些真正的问题上

208
00:06:49,230 --> 00:06:51,220
Lastly, when developing learning algorithms,
最后 在构造机器学习算法时

209
00:06:52,260 --> 00:06:53,880
one other useful tip is
另一个有用的小窍门是

210
00:06:54,190 --> 00:06:55,230
to make sure that you have
保证你自己

211
00:06:55,590 --> 00:06:56,450
a way, that you have a
保证你能有一种

212
00:06:56,810 --> 00:06:59,710
numerical evaluation of your learning algorithm.
数值计算的方式来评估你的机器学习算法

213
00:07:02,130 --> 00:07:03,220
Now what I mean by that is that
我这么说的意思是

214
00:07:03,460 --> 00:07:04,670
if you're developing a learning algorithm,
如果你在构造一个学习算法

215
00:07:05,230 --> 00:07:07,180
it is often incredibly helpful
如果你能有一种

216
00:07:08,060 --> 00:07:09,170
if you have a way of
评估你算法的方法

217
00:07:09,460 --> 00:07:10,830
evaluating your learning algorithm
这是非常有用的

218
00:07:11,290 --> 00:07:13,100
that just gives you back a single real number.
一种用数字说话的评估方法

219
00:07:13,650 --> 00:07:14,880
Maybe accuracy, maybe error.
你的算法可能精确 可能有错

220
00:07:15,620 --> 00:07:18,390
But the single real number that tells you how well your learning algorithm is doing.
但是它能准确的告诉你你的算法到底表现有多好

221
00:07:20,280 --> 00:07:21,330
I'll talk more about this specific
在接下来的课程中

222
00:07:21,770 --> 00:07:24,650
concepts in later videos, but here's a specific example.
我会更详细的讲述这个概念 但是先看看这个例子

223
00:07:25,790 --> 00:07:26,600
Let's say we are trying to
假设我们试图

224
00:07:26,690 --> 00:07:27,990
decide whether or not we
决定是否应该

225
00:07:28,060 --> 00:07:29,140
should treat words like discount,
把像"discount""discounts""discounter""discountring"

226
00:07:29,590 --> 00:07:32,060
discounts, discounter, discounting, as the same word.
这样的单词都视为等同

227
00:07:32,370 --> 00:07:33,390
So maybe one way to
一种方法

228
00:07:33,520 --> 00:07:34,770
do that is to just
是检查这些单词的

229
00:07:35,400 --> 00:07:38,780
look at the first few characters in a word.
开头几个字母

230
00:07:38,960 --> 00:07:40,240
Like, you know, if you just look at
比如

231
00:07:40,300 --> 00:07:41,690
the first few characters of
当你在检查这些单词开头几个字母的时候

232
00:07:41,780 --> 00:07:44,640
a word, then you figure
你发现

233
00:07:44,920 --> 00:07:45,970
out that maybe all of these
这几个单词

234
00:07:46,130 --> 00:07:47,990
words are roughly -   have similar meanings.
大概可能有着相同的意思

235
00:07:50,460 --> 00:07:52,090
In natural language processing, the
在自然语言处理中

236
00:07:52,250 --> 00:07:53,270
way that this is done is
这种方法

237
00:07:53,510 --> 00:07:55,960
actually using a type of software called stemming software.
是通过一种叫做词干提取的软件实现的

238
00:07:56,940 --> 00:07:58,080
If you ever want to do
如果你想自己来试试

239
00:07:58,160 --> 00:07:59,880
this yourself, search on a
你可以

240
00:07:59,950 --> 00:08:01,240
web search engine for the
在网上搜索一下

241
00:08:01,500 --> 00:08:02,660
Porter Stemmer and that
"Porter Stemmer(波特词干提取法)"

242
00:08:02,960 --> 00:08:04,320
would be, you know, one reasonable piece of
这是在词干提取方面

243
00:08:04,620 --> 00:08:05,830
software for doing this sort
一个比较不错的软件

244
00:08:06,110 --> 00:08:07,020
of stemming, which will let
这个软件会

245
00:08:07,130 --> 00:08:08,140
you treat all of these discount,
将单词"discount""discounts"以及等等

246
00:08:08,800 --> 00:08:10,540
discounts, and so on as the same word.
都视为同一个单词

247
00:08:13,950 --> 00:08:15,930
But using a stemming software
但是这种词干提取软件

248
00:08:16,630 --> 00:08:17,710
that basically looks at the
只会检查

249
00:08:17,830 --> 00:08:19,290
first few alphabets of the
单词的头几个字母

250
00:08:19,450 --> 00:08:21,630
word more or less, it can help but it can hurt.
这有用 但是也可能会造成一些问题

251
00:08:22,240 --> 00:08:23,490
And it can hurt because, for
因为

252
00:08:23,900 --> 00:08:25,360
example, this software may
举个例子

253
00:08:25,930 --> 00:08:27,850
mistake the words universe and
因为这个软件会把单词"universe(宇宙)"

254
00:08:27,990 --> 00:08:29,980
university as being the
和"university(大学)"

255
00:08:30,070 --> 00:08:31,220
same thing because, you know,
也视为同一个单词

256
00:08:31,450 --> 00:08:33,220
these two words start off
因为

257
00:08:33,480 --> 00:08:35,480
with very similar characters, with the same alphabets.
这两个单词开头的字母是一样的

258
00:08:37,300 --> 00:08:39,050
So if you're trying
因此

259
00:08:39,280 --> 00:08:40,290
to decide whether or not
当你在决定

260
00:08:40,630 --> 00:08:42,490
to use stemming software for
是否应该使用词干提取软件用来分类

261
00:08:42,670 --> 00:08:45,960
a stem classifier, it is not always easy to tell.
这总是很难说清楚

262
00:08:46,350 --> 00:08:47,810
And in particular, error analysis
特别地

263
00:08:48,510 --> 00:08:49,590
may not actually be helpful
误差分析

264
00:08:51,030 --> 00:08:52,860
for deciding if this
也并不能帮助你决定

265
00:08:53,060 --> 00:08:54,410
sort of stemming idea is a good idea.
词干提取是不是一个好的方法

266
00:08:55,570 --> 00:08:56,740
Instead, the best way
与之相对地 最好的方法

267
00:08:57,020 --> 00:08:58,320
to figure out if using stemming
来发现词干提取软件

268
00:08:58,690 --> 00:08:59,970
software is good to help
对你的分类器

269
00:09:00,190 --> 00:09:01,570
your classifier is if you
到底有没有用

270
00:09:01,740 --> 00:09:02,980
have a way to very quickly
是迅速地着手试一试

271
00:09:03,370 --> 00:09:05,170
just try it and see if it works.
来看看它表现到底怎么样

272
00:09:08,560 --> 00:09:09,530
And in order to do this,
为了这么做

273
00:09:10,260 --> 00:09:11,350
having a way to numerically
通过数值来评估你的算法

274
00:09:12,250 --> 00:09:14,570
evaluate your algorithm, is going to be very helpful.
是非常有用的

275
00:09:15,940 --> 00:09:17,670
Concretely, maybe the most
具体地说

276
00:09:18,110 --> 00:09:19,190
natural thing to do is
自然而然地

277
00:09:19,350 --> 00:09:20,250
to look at the cross validation
你应该通过交叉验证

278
00:09:20,900 --> 00:09:23,510
error of the algorithm's performance with and without stemming.
来验证不用词干提取与用词干提取的算法的错误率

279
00:09:24,590 --> 00:09:25,560
So, if you run your
因此

280
00:09:25,800 --> 00:09:27,190
algorithm without stemming and you
如果你不在你的算法中使用词干提取

281
00:09:27,330 --> 00:09:28,430
end up with, let's say,
然后你得到 比如

282
00:09:29,080 --> 00:09:31,260
five percent classification error, and
5%的分类错误率

283
00:09:31,360 --> 00:09:32,410
you re-run it and you
然后你再使用词干提取来运行你的算法

284
00:09:32,540 --> 00:09:33,780
end up with, let's say, three
你得到 比如

285
00:09:34,110 --> 00:09:36,170
percent classification error, then this
3%的分类错误

286
00:09:36,440 --> 00:09:37,920
decrease in error very quickly
那么这很大的减少了错误发生

287
00:09:38,640 --> 00:09:39,980
allows you to decide that,
于是你决定

288
00:09:40,310 --> 00:09:42,250
you know, it looks like using stemming is a good idea.
词干提取是一个好的办法

289
00:09:43,080 --> 00:09:44,650
For this particular problem, there's
就这个特定的问题而言

290
00:09:44,940 --> 00:09:46,560
a very natural single real
这里有一个数量的评估数字

291
00:09:46,830 --> 00:09:50,210
number evaluation metric, namely, the cross validation error.
即交差验证错误率

292
00:09:50,930 --> 00:09:52,700
We'll see later, examples where coming
我们以后会发现

293
00:09:53,080 --> 00:09:54,360
up with this, sort of, single
这个例子中的评估数字

294
00:09:54,790 --> 00:09:58,220
row number evaluation metric may need a little bit more work.
还需要一些处理

295
00:09:58,790 --> 00:09:59,840
But as we'll see in
但是

296
00:09:59,930 --> 00:10:01,620
the later video, doing so would
我们可以在今后的课程中看到

297
00:10:01,750 --> 00:10:02,860
also then let you
这么做还是会让你

298
00:10:02,990 --> 00:10:04,290
make these decisions much more quickly
能更快地做出决定

299
00:10:04,760 --> 00:10:06,380
of, say, whether or not to use stemming.
比如 是否使用词干提取

300
00:10:08,700 --> 00:10:09,950
And just this one more quick example.
再说一个例子

301
00:10:10,680 --> 00:10:11,670
Let's say that you're also trying
假设

302
00:10:12,040 --> 00:10:13,450
to decide whether or not
你在想是否应该

303
00:10:13,650 --> 00:10:15,710
to distinguish between upper versus lower case.
区分单词的大小写

304
00:10:15,990 --> 00:10:16,910
So, you know, is the red
比如

305
00:10:17,060 --> 00:10:18,850
mom with uppercase M
单词"mom" 大写的"M"

306
00:10:19,060 --> 00:10:20,390
versus lower case m,
和小写的"m"

307
00:10:20,700 --> 00:10:21,720
I mean, should that be treated as
它们应该被视作

308
00:10:21,780 --> 00:10:23,810
the same word or as different words?
同一个单词还是不同的单词

309
00:10:23,970 --> 00:10:26,890
Should these be treated as the same feature or as different features?
它们应该被视作相同的特征变量还是不同的

310
00:10:27,010 --> 00:10:28,060
And so once again,
再说一次

311
00:10:28,350 --> 00:10:29,150
because we have a way
因为我们有一种

312
00:10:29,300 --> 00:10:30,790
to evaluate our algorithm, if
能够评估我们算法的方法

313
00:10:31,060 --> 00:10:32,350
you try this out here, if
如果你在这里试一试

314
00:10:32,650 --> 00:10:34,910
I stop distinguishing upper
如果我不区分

315
00:10:35,140 --> 00:10:36,490
and lower case, maybe I
大小写

316
00:10:36,600 --> 00:10:38,580
end up with 3.2%
最后得到3.2%的错误率

317
00:10:38,700 --> 00:10:39,820
error and I find that
然后我发现

318
00:10:40,020 --> 00:10:41,750
therefore this does worse
这个表现的较差些

319
00:10:42,260 --> 00:10:43,360
than, you know, if I use only
如果

320
00:10:43,640 --> 00:10:45,110
stemming, and so this lets
如果我只用了词干提取

321
00:10:45,370 --> 00:10:47,420
me very quickly decide to go
这之后我再思考

322
00:10:48,270 --> 00:10:49,720
ahead and to distinguish or to
是否要区分

323
00:10:49,820 --> 00:10:51,540
not distinguish between upper and lower case.
大小写

324
00:10:52,140 --> 00:10:53,390
So when you' re developing
因此当你在

325
00:10:53,690 --> 00:10:55,260
a learning algorithm, very often
构造学习算法的时候

326
00:10:55,650 --> 00:10:56,840
you'll be trying out lots of
你总是会去尝试

327
00:10:57,050 --> 00:10:59,930
new ideas and lots of new versions of your learning algorithm.
很多新的想法 实现出很多版本的学习算法

328
00:11:00,960 --> 00:11:02,050
If every time you try
如果每一次

329
00:11:02,350 --> 00:11:03,740
out a new idea if you
你实践新想法的时候

330
00:11:03,840 --> 00:11:05,610
end up manually examining a
你都手动地检测

331
00:11:05,750 --> 00:11:06,730
bunch of examples, you begin to
这些例子

332
00:11:06,860 --> 00:11:08,530
see better or worse, you
去看看是表现差还是表现好

333
00:11:08,640 --> 00:11:09,410
know, that's going to make it
那么这很难让你

334
00:11:09,580 --> 00:11:10,610
really hard to make decisions
做出决定

335
00:11:10,980 --> 00:11:12,410
on do you use stemming or not.
到底是否使用词干提取

336
00:11:12,580 --> 00:11:13,640
Do you distinguish upper or lowercase or not?
是否区分大小写

337
00:11:15,180 --> 00:11:16,590
But by having a single rule
但是通过一个

338
00:11:16,770 --> 00:11:18,520
number evaluation metric, you can
量化的数值评估

339
00:11:18,680 --> 00:11:21,150
then just look and see oh, did the error go up or go down?
你可以看看这个数字 误差是变大还是变小了

340
00:11:22,420 --> 00:11:23,620
And you can use that much
你可以通过它

341
00:11:23,940 --> 00:11:25,760
more rapidly, try out
更快地实践

342
00:11:25,840 --> 00:11:27,820
new ideas and almost right
你的新想法

343
00:11:27,990 --> 00:11:29,550
away tell if your new
它基本上非常直观地告诉你

344
00:11:29,690 --> 00:11:31,480
idea has improved or worsened
你的想法是提高了算法表现

345
00:11:32,440 --> 00:11:33,230
the performance of the learning algorithm
还是让它变得更坏

346
00:11:33,930 --> 00:11:35,440
and this will let
这会大大提高

347
00:11:35,560 --> 00:11:38,340
you often make much faster progress.
你实践算法时的速度

348
00:11:38,530 --> 00:11:39,720
So the recommended, strongly recommended
所以我强烈推荐

349
00:11:40,220 --> 00:11:41,790
way to do error analysis is
在交叉验证集上来实施误差分析

350
00:11:42,370 --> 00:11:44,760
on the cross validation set rather than the test set.
而不是在测试集上

351
00:11:45,490 --> 00:11:46,970
But, you know, there are
但是

352
00:11:47,240 --> 00:11:48,260
people that will do this on
还是有一些人

353
00:11:48,370 --> 00:11:49,480
the test set even though that's
会在测试集上来做误差分析

354
00:11:49,730 --> 00:11:51,530
definitely a less mathematically appropriate
即使这从数学上讲

355
00:11:52,190 --> 00:11:54,560
set of your list, recommended what
是不合适的

356
00:11:54,730 --> 00:11:55,660
you think to do than to
所以我还是推荐你

357
00:11:55,780 --> 00:11:57,240
do error analysis on your
在交叉验证向量上

358
00:11:57,450 --> 00:11:58,760
cross validation sector.
来做误差分析

359
00:11:59,140 --> 00:12:01,160
So, to wrap up this video, when starting
总结一下

360
00:12:01,830 --> 00:12:03,340
on the new machine learning problem, what
当你在研究一个新的机器学习问题时

361
00:12:03,610 --> 00:12:05,370
I almost always recommend is
我总是推荐你

362
00:12:05,610 --> 00:12:06,930
to implement a quick and
实现一个较为简单快速

363
00:12:07,030 --> 00:12:08,710
dirty implementation of your learning algorithm.
即便不是那么完美的算法

364
00:12:09,780 --> 00:12:11,760
And I've almost never seen
我几乎从未见过

365
00:12:12,120 --> 00:12:15,370
anyone spend too little time on this quick and dirty implementation.
人们这样做

366
00:12:18,640 --> 00:12:20,210
I pretty much only ever see
大家经常干的事情是

367
00:12:20,480 --> 00:12:22,050
people spend much too much
花费大量的时间

368
00:12:22,370 --> 00:12:23,720
time building their first, you know,
在构造算法上

369
00:12:24,580 --> 00:12:25,800
supposedly quick and dirty implementations.
构造他们以为的简单的方法

370
00:12:26,590 --> 00:12:28,100
So really, don't worry about
因此

371
00:12:29,070 --> 00:12:31,210
it being too quick, or don't worry about it being too dirty.
不要担心你的算法太简单 或者太不完美

372
00:12:32,120 --> 00:12:33,580
But really implement something as
而是尽可能快地

373
00:12:33,690 --> 00:12:35,220
quickly as you can, and once
实现你的算法

374
00:12:35,450 --> 00:12:37,550
you have the initial implementation this
当你有了初始的实现之后

375
00:12:37,820 --> 00:12:38,860
is then a powerful tool for
它会变成一个非常有力的工具

376
00:12:39,230 --> 00:12:40,420
deciding where to spend your
来帮助你决定

377
00:12:40,610 --> 00:12:42,170
time next, because first we
下一步的做法

378
00:12:42,390 --> 00:12:43,390
can look at the errors it makes,
因为我们可以先看看算法造成的错误

379
00:12:43,630 --> 00:12:44,720
and do this sort of error analysis
通过误差分析

380
00:12:45,280 --> 00:12:46,360
to see what mistakes it makes
来看看他犯了什么错

381
00:12:47,010 --> 00:12:48,420
and use that to inspire further development.
然后来决定优化的方式

382
00:12:49,030 --> 00:12:50,880
And second, assuming your
另一件事是

383
00:12:51,000 --> 00:12:53,360
quick and dirty implementation incorporated a
假设你有了一个快速而不完美的算法实现

384
00:12:53,620 --> 00:12:55,700
single real number evaluation metric, this
又有一个数值的评估数据

385
00:12:55,940 --> 00:12:57,660
can then be a vehicle for
这会帮助你

386
00:12:57,730 --> 00:12:58,980
you to try out different ideas
尝试新的想法

387
00:12:59,810 --> 00:13:00,810
and quickly see if the
快速地发现

388
00:13:01,030 --> 00:13:02,170
different ideas you're trying out
你尝试的这些想法

389
00:13:02,440 --> 00:13:03,830
are improving the performance of
是否能够提高算法的表现

390
00:13:03,920 --> 00:13:05,420
your algorithm and therefore let
从而

391
00:13:05,570 --> 00:13:06,470
you maybe much more quickly
你会更快地

392
00:13:06,860 --> 00:13:08,440
make decisions about what things
做出决定

393
00:13:08,760 --> 00:13:09,900
to fold, and what things to
在算法中放弃什么 吸收什么

394
00:13:10,240 --> 00:13:11,520
incorporate into your learning algorithm.

