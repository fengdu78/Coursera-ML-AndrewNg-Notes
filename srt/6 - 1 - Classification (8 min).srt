1
00:00:00,460 --> 00:00:01,410
In this and the next
在现在及未来
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,580 --> 00:00:02,730
few videos, I want to
一些视频，我想

3
00:00:02,960 --> 00:00:04,660
start to talk about classification problems,
开始谈论的分类问题，

4
00:00:05,520 --> 00:00:07,000
where the variable y that
其中变量y表示

5
00:00:07,110 --> 00:00:08,160
you want to predict is discreet
要预测是谨慎的

6
00:00:08,570 --> 00:00:10,190
valued. We'll develop an
估值。我们将开发一个

7
00:00:10,420 --> 00:00:11,860
algorithm called logistic regression,
算法称为Logistic回归，

8
00:00:12,410 --> 00:00:13,620
which is one of the
这是一个

9
00:00:13,700 --> 00:00:16,560
most popular and most widely used learning algorithms today.
最流行，最广泛使用的今天学习算法。

10
00:00:19,770 --> 00:00:22,150
Here are some examples of classification problems.
下面是分类问题的一些例子。

11
00:00:23,170 --> 00:00:24,720
Earlier, we talked about emails,
此前，我们谈到了电子邮件，

12
00:00:25,260 --> 00:00:26,700
spam classification as an
垃圾分类作为

13
00:00:27,070 --> 00:00:28,260
example of a classification problem.
例的分类问题。

14
00:00:29,380 --> 00:00:32,160
Another example would be classifying online transactions.
另一个例子是分类网上交易。

15
00:00:33,080 --> 00:00:34,110
So, if you have a website
所以，如果你有一个网站

16
00:00:34,340 --> 00:00:35,530
that sells stuff and if you
卖的东西，如果你

17
00:00:35,750 --> 00:00:36,740
want to know if a physical
要知道，如果一个物理

18
00:00:37,040 --> 00:00:39,140
transaction is fraudulent or
交易是欺诈或

19
00:00:39,260 --> 00:00:40,920
not, whether someone has, you
不，某人是否有你

20
00:00:41,060 --> 00:00:42,260
know, is using a stolen credit card
知道，是用偷来的信用卡

21
00:00:42,580 --> 00:00:43,890
or has stolen the user's password.
或窃取用户的密码。

22
00:00:44,560 --> 00:00:46,830
That's another classification problem, and
这是另一种分类问题，

23
00:00:47,030 --> 00:00:48,220
earlier we also talked about
我们前面也谈到了

24
00:00:48,410 --> 00:00:50,610
the example of classifying tumors
瘤进行分类的例子

25
00:00:51,640 --> 00:00:53,680
as a cancerous malignant or as benign tumors.
作为恶性癌或良性肿瘤。

26
00:00:55,070 --> 00:00:56,010
In all of these problems,
在所有的这些问题，

27
00:00:56,690 --> 00:00:57,610
the variable that we're trying
我们正在努力变

28
00:00:57,850 --> 00:00:58,870
to predict is a variable
预测是一个可变

29
00:00:59,290 --> 00:01:00,110
Y that we can think
y表示我们可以认为

30
00:01:00,420 --> 00:01:01,710
of as taking on two values,
作为取两个值，

31
00:01:02,600 --> 00:01:04,120
either zero or one, either
为零或1，或者

32
00:01:04,340 --> 00:01:05,780
a spam or not spam, fraudulent
垃圾邮件或者非垃圾邮件，欺诈性

33
00:01:06,620 --> 00:01:08,740
or not fraudulent, malignant or benign.
或不欺诈，恶性或良性。

34
00:01:10,490 --> 00:01:11,430
Another name for the class
该类的另一个名字

35
00:01:11,810 --> 00:01:13,160
that we denote with 0 is
我们表示具有0

36
00:01:13,810 --> 00:01:15,660
the negative class, and another
负类，和另一

37
00:01:15,950 --> 00:01:16,920
name for the class that we
命名的类，我们

38
00:01:17,020 --> 00:01:19,350
denote with 1 is the positive class.
表示与图1是阳性的类。

39
00:01:20,170 --> 00:01:21,500
So 0 may denote the
所以，0可以表示

40
00:01:22,070 --> 00:01:23,460
benign tumor and 1
良性肿瘤和1

41
00:01:23,850 --> 00:01:25,940
positive class may denote a malignant tumor.
阳性类可表示的恶性肿瘤。

42
00:01:27,090 --> 00:01:28,410
The assignment of the 2
的2的分配

43
00:01:28,860 --> 00:01:29,940
classes, you know, spam,
类，你知道，垃圾邮件，

44
00:01:30,050 --> 00:01:31,140
no spam, and so on -
没有垃圾邮件，等等 -

45
00:01:31,330 --> 00:01:32,470
the assignment of the 2
的2的分配

46
00:01:32,790 --> 00:01:34,140
classes to positive and negative,
类，以积极和消极的，

47
00:01:34,500 --> 00:01:35,950
to 0 and 1 is somewhat
为0和1是有点

48
00:01:36,250 --> 00:01:37,840
arbitrary and it doesn't really matter.
任意的，它其实并不重要。

49
00:01:38,680 --> 00:01:39,820
But often there is this
但往往有这样的

50
00:01:39,990 --> 00:01:40,970
intuition that the negative
直觉，负

51
00:01:41,460 --> 00:01:43,430
class is conveying the
类输送

52
00:01:43,590 --> 00:01:44,690
absence of something, like the absence
缺少的东西，就像没有

53
00:01:45,000 --> 00:01:47,440
of a malignant tumor, whereas one,
的恶性肿瘤，而之一，

54
00:01:47,860 --> 00:01:49,410
the positive class, is conveying
正班，被输送

55
00:01:49,950 --> 00:01:52,110
the presence of something that we may be looking for.
的东西，我们可能会寻找存在。

56
00:01:52,770 --> 00:01:54,340
But the definition of which
但它的定义

57
00:01:54,560 --> 00:01:55,400
is negative and which is positive
是负的，这是正

58
00:01:55,680 --> 00:01:58,480
is somewhat arbitrary and it doesn't matter that much.
是任意的，这一点并不重要得多。

59
00:02:00,090 --> 00:02:00,980
For now, we're going to start
现在，我们要开始

60
00:02:01,340 --> 00:02:03,030
with classification problems with just
与分类问题，只需

61
00:02:03,290 --> 00:02:04,540
two classes; zero and one.
两班;零和1。

62
00:02:05,480 --> 00:02:07,010
Later on, we'll talk about multi-class
稍后，我们将讨论多级

63
00:02:07,440 --> 00:02:09,320
problems as well, whether variable
问题还有，是否变

64
00:02:09,750 --> 00:02:10,960
Y may take on say,
Y可以采取的发言权，

65
00:02:11,550 --> 00:02:13,120
for value zero, one, two and three.
对值零，一，二，三。

66
00:02:14,220 --> 00:02:16,810
This is called a multi-class classification problem,
这就是所谓的多类分类问题，

67
00:02:17,680 --> 00:02:18,800
but for the next few
但在未来数

68
00:02:18,950 --> 00:02:20,280
videos, let's start with the
视频，让我们先从

69
00:02:20,660 --> 00:02:22,750
two class or the binary classification problem.
二级或二元分类问题。

70
00:02:23,580 --> 00:02:25,650
and we'll worry about the multi-class setting later.
我们稍后会担心多类设置。

71
00:02:26,980 --> 00:02:29,440
So, how do we develop a classification algorithm?
那么，我们如何建立一个分类算法？

72
00:02:30,530 --> 00:02:31,670
Here's an example of a
下面是一个例子

73
00:02:31,750 --> 00:02:32,730
training set for a classification
对于分类训练集

74
00:02:34,350 --> 00:02:35,800
task for classifying a tumor
任务的肿瘤分类

75
00:02:36,240 --> 00:02:37,540
as malignant or benign and
为恶性或良性，

76
00:02:37,820 --> 00:02:39,260
notice that malignancy takes on
注意到恶性肿瘤发生在

77
00:02:39,530 --> 00:02:41,200
only two values zero or
只有两个值为零或

78
00:02:41,380 --> 00:02:43,210
no or one or one or yes.
没有或一个或一个或肯定。

79
00:02:44,550 --> 00:02:45,650
So, one thing we could
所以，有一件事我们可以

80
00:02:45,850 --> 00:02:46,970
do given this training set
不要给这个训练集

81
00:02:47,440 --> 00:02:48,700
is to apply the algorithm
是应用该算法

82
00:02:49,120 --> 00:02:52,710
that we already know, linear regression to this data set
我们已经知道，线性回归，这组数据

83
00:02:53,150 --> 00:02:55,310
and just try to fit the straight line to the data.
而只是尝试，以适应直线的数据。

84
00:02:56,290 --> 00:02:57,480
So, if you take this training
所以，如果你把这个培训

85
00:02:57,780 --> 00:02:58,760
set and fill a straight
设置和填充直

86
00:02:58,900 --> 00:03:00,320
line to it, maybe you get
行吧，也许你会得到

87
00:03:00,700 --> 00:03:03,530
hypothesis that looks like that.
假设，看起来像这样。

88
00:03:03,700 --> 00:03:05,920
Alright, so that's my hypothesis, h of
好了，所以这是我的假设，H的

89
00:03:06,020 --> 00:03:07,890
x equals theta transpose
x等于西塔转

90
00:03:08,020 --> 00:03:09,330
x. If you want
按x。  如果你想

91
00:03:09,570 --> 00:03:11,270
to make predictions, one thing
作出预测，一件事

92
00:03:11,500 --> 00:03:12,980
you could try doing is then
你可以尝试做的是那么

93
00:03:13,610 --> 00:03:16,760
threshold the classifier outputs at 0.5.
阈值分类器输出为0.5。

94
00:03:17,110 --> 00:03:19,880
That is at the vertical access value 0.5.
即在垂直通道值0.5。

95
00:03:21,760 --> 00:03:23,940
And if the hypothesis outputs
并且，如果假设输出

96
00:03:24,330 --> 00:03:25,490
a value that's greater than
一个值，该值是大于

97
00:03:25,620 --> 00:03:27,510
equal to 0.5 you predict y equals one.
等于0.5您预测y等于之一。

98
00:03:27,860 --> 00:03:29,940
If it's less than 0.5, you predict y equals zero.
如果它小于0.5，你预测y等于为零。

99
00:03:31,070 --> 00:03:32,540
Let's see what happens when we do that.
让我们来看看，当我们这样做会发生什么。

100
00:03:32,740 --> 00:03:33,900
So, let's take 0.5, and
所以，让我们取0.5，和

101
00:03:34,090 --> 00:03:36,670
so, you know, that's where the threshold is.
所以，你知道，这就是门槛。

102
00:03:37,070 --> 00:03:39,260
And thus, using linear regression this way.
因此，使用线性回归这种方式。

103
00:03:39,920 --> 00:03:41,060
Everything to the right
一切的权利

104
00:03:41,330 --> 00:03:42,460
of this point, we will end
这一点，我们将结束

105
00:03:42,640 --> 00:03:43,690
up predicting as the positive
向上预测作为正

106
00:03:44,280 --> 00:03:45,390
class because of the output
因为输出级

107
00:03:45,690 --> 00:03:46,800
values are greater than 0.5
值大于0.5

108
00:03:47,270 --> 00:03:48,690
on the vertical axis and
在垂直轴上，并

109
00:03:49,340 --> 00:03:50,730
everything to the left
一切向左侧

110
00:03:51,000 --> 00:03:52,260
of that point we will end
这一点，我们将结束

111
00:03:52,490 --> 00:03:54,170
up predicting as a negative value.
向上预测为负值。

112
00:03:55,660 --> 00:03:57,570
In this particular example, it
在这个特定的例子中，它

113
00:03:57,720 --> 00:03:59,400
looks like linear regression is actually
看起来像线性回归实际上是

114
00:03:59,790 --> 00:04:01,870
doing something reasonable even though
做的事情，即使合理

115
00:04:02,190 --> 00:04:03,910
this is a classification task we're
这是一个分类的任务我们

116
00:04:04,140 --> 00:04:05,430
interested in.
感兴趣。

117
00:04:05,500 --> 00:04:07,420
But now let's try changing problem a bit.
但现在让我们尝试改变的问题了一下。

118
00:04:08,060 --> 00:04:09,360
Let me extend out the horizontal
让我伸出水平

119
00:04:10,040 --> 00:04:11,460
axis of orbit and let's
轨道的轴，让我们

120
00:04:11,650 --> 00:04:12,640
say we got one more training
说我们得到了一个更多的培训

121
00:04:12,990 --> 00:04:15,030
example way out there on the right.
例如出路在那里就对了。

122
00:04:16,520 --> 00:04:17,830
Notice that that additional training
请注意，这额外的培训

123
00:04:18,170 --> 00:04:19,200
example, this one out
例如，这一个了

124
00:04:19,390 --> 00:04:21,710
here, it doesn't actually change anything, right?
在这里，它实际上并没有改变什么，对不对？

125
00:04:22,420 --> 00:04:23,470
Looking at the training set, it
看着训练集，它

126
00:04:23,560 --> 00:04:26,340
is pretty clear what a good hypothesis is.
是相当清楚什么是好的假设是。

127
00:04:26,890 --> 00:04:27,920
Well, everything to the right of
好了，一切的权利

128
00:04:28,000 --> 00:04:29,050
somewhere around here to the
某处在这里的

129
00:04:29,190 --> 00:04:29,970
right of this we should predict
权这一点，我们应该预测

130
00:04:30,300 --> 00:04:31,280
as positive, and everything to
为阳性，什么都

131
00:04:31,480 --> 00:04:32,690
the left we should probably predict
左边，我们也许应该预测

132
00:04:33,060 --> 00:04:34,700
as negative because from this
因为从这个负

133
00:04:34,880 --> 00:04:35,940
training set it looks like
培训设置它看起来像

134
00:04:36,200 --> 00:04:37,880
all the tumors larger than, you
所有比你大的肿瘤

135
00:04:37,970 --> 00:04:39,190
know, a certain value around here
知道了，在这里一定值

136
00:04:39,490 --> 00:04:41,030
are malignant, and all the
是恶性的，并且所有的

137
00:04:41,200 --> 00:04:42,110
tumors smaller than that are
肿瘤比那些小

138
00:04:42,220 --> 00:04:44,660
not malignant, at least for this training set.
不是恶性的，至少在这个训练集。

139
00:04:46,160 --> 00:04:47,280
But once we've added
但是我们增加一次

140
00:04:47,720 --> 00:04:49,060
that extra example out here,
额外的例子在这里，

141
00:04:49,620 --> 00:04:50,660
if you now run linear regression,
如果你现在运行的线性回归，

142
00:04:51,580 --> 00:04:53,590
you instead get a straight line fit to the data.
你不是得到一条直线拟合数据。

143
00:04:54,430 --> 00:04:55,630
That might maybe look like this, and
这可能也许是这样的，和

144
00:04:57,890 --> 00:04:59,860
if you now threshold this hypothesis
如果你现在这个门槛假说

145
00:05:02,480 --> 00:05:03,460
at 0.5, you end up with
0.5，你结束了

146
00:05:04,110 --> 00:05:05,550
a threshold that's around here
一个阈值是在这里

147
00:05:06,320 --> 00:05:07,320
so that everything to the right
所以，一切的权利

148
00:05:07,570 --> 00:05:08,790
of this point you predict as
这一点，您预测为

149
00:05:08,960 --> 00:05:11,510
positive, and everything to the left of that point you predict as negative.
积极的，一切到该点的左边你预测为阴性。

150
00:05:14,580 --> 00:05:15,720
And this seems a pretty
而这似乎是一个相当

151
00:05:16,100 --> 00:05:18,500
bad thing for linear regression to have done, right?
坏事线性回归，都做了，对吧？

152
00:05:18,770 --> 00:05:19,840
Because, you know, these are
因为，你知道，这些都是

153
00:05:19,930 --> 00:05:22,010
our positive examples, these are our negative examples.
我们积极的例子，这些都是我们的负面的例子。

154
00:05:23,050 --> 00:05:24,580
It's pretty clear, we should
这是很清楚的，我们应该

155
00:05:24,800 --> 00:05:26,000
really be separating the two classes
真正分开两班

156
00:05:26,550 --> 00:05:28,180
somewhere around there, but somehow
我身边不远，但不知何故

157
00:05:28,670 --> 00:05:30,030
by adding one example way
通过添加一个实例方式

158
00:05:30,190 --> 00:05:31,280
out here to the right, this
这里的权利，这

159
00:05:31,420 --> 00:05:33,340
example really isn't giving us any new information.
例如还真是不给我们任何新的信息。

160
00:05:33,770 --> 00:05:34,950
I mean, it should be no
我的意思是，应该没

161
00:05:35,170 --> 00:05:36,300
surprise to the learning out of
惊喜地学习出

162
00:05:37,030 --> 00:05:39,100
that the example way out here turns out to be malignant.
那这里的例子的方式原来是恶性的。

163
00:05:40,230 --> 00:05:41,210
But somehow adding that example
但不知何故，并称例子

164
00:05:41,740 --> 00:05:43,420
out there caused linear regression
在那里引起的线性回归

165
00:05:44,410 --> 00:05:45,670
to change in straight line fit
在直线拟合改变

166
00:05:45,980 --> 00:05:47,650
to the data from this
从这个数据

167
00:05:48,840 --> 00:05:50,000
magenta line out here
洋红色线条在这里

168
00:05:50,840 --> 00:05:51,940
to this blue line over here,
这个蓝线在这里，

169
00:05:52,850 --> 00:05:54,770
and caused it to give us a worse hypothesis.
而造成它给我们一个更坏的假设。

170
00:05:56,950 --> 00:05:58,440
So, applying linear regression
因此，应用线性回归

171
00:05:59,080 --> 00:06:01,030
to a classification problem usually
一个分类问题通常

172
00:06:01,610 --> 00:06:03,400
isn't, often isn't a great idea.
是不是，往往不是一个好主意。

173
00:06:04,430 --> 00:06:05,750
In the first instance, in the
在一审中，

174
00:06:05,810 --> 00:06:07,090
first example before I added
之前，第一个例子我添加

175
00:06:07,540 --> 00:06:08,780
this extra training example,
这个额外的培训为例，

176
00:06:09,810 --> 00:06:11,430
previously linear regression was
此前线性回归是

177
00:06:11,650 --> 00:06:13,200
just getting lucky and it
刚开幸运，它

178
00:06:13,380 --> 00:06:14,990
got us a hypothesis that, you
我们得到了一个假设，你

179
00:06:15,090 --> 00:06:16,290
know, worked well for that particular
知道的，运作良好，对特定

180
00:06:16,670 --> 00:06:19,470
example, but usually apply
例如，但通常适用

181
00:06:19,980 --> 00:06:20,970
linear regression to a data set,
线性回归到一个数据集，

182
00:06:21,820 --> 00:06:23,040
you know, you might get lucky but
你知道，你可能会得到幸运，但

183
00:06:23,270 --> 00:06:24,130
often it isn't a good
通常它不是一个好

184
00:06:24,260 --> 00:06:25,730
idea, so I wouldn't use
的想法，所以我不会用

185
00:06:25,980 --> 00:06:27,960
linear regression for classification problems.
对于分类问题线性回归。

186
00:06:29,670 --> 00:06:30,820
Here is one other funny thing
这里是另外一个有趣的事情

187
00:06:31,250 --> 00:06:32,650
about what would happen if
会发生什么，如果

188
00:06:32,930 --> 00:06:35,510
we were to use linear regression for a classification problem.
我们用线性回归的分类问题。

189
00:06:36,690 --> 00:06:38,220
For classification, we know that
对于分类，我们知道，

190
00:06:38,450 --> 00:06:39,790
Y is either zero or one,
Y是0或1，

191
00:06:40,580 --> 00:06:41,620
but if you are using
但如果您使用的是

192
00:06:41,890 --> 00:06:43,050
linear regression, well the hypothesis
线性回归，以及假设

193
00:06:44,210 --> 00:06:45,750
can output values much larger
可输出值大得多

194
00:06:46,060 --> 00:06:47,330
than one or less than
大于1或小于

195
00:06:47,500 --> 00:06:48,820
zero, even if all
零，即使所有

196
00:06:49,050 --> 00:06:50,690
of good the training examples have labels
良好的训练样例有标签

197
00:06:51,140 --> 00:06:52,410
Y equals zero or one,
Y等于0或1，

198
00:06:53,900 --> 00:06:54,880
and it seems kind of strange
这似乎有点怪

199
00:06:55,520 --> 00:06:56,760
that even though we
即使我们

200
00:06:56,960 --> 00:06:58,160
know that the label should
知道标签应

201
00:06:58,350 --> 00:06:59,320
be zero one, it seems
为零1，似乎

202
00:06:59,420 --> 00:07:00,890
kind of strange if the
有点怪，如果

203
00:07:01,210 --> 00:07:02,580
algorithm can offer values much
算法可以提供多少价值

204
00:07:02,840 --> 00:07:04,900
larger than one or much smaller than zero.
比1大或大于零小得多。

205
00:07:09,540 --> 00:07:10,900
So what we'll do in the
所以，我们会做的

206
00:07:11,000 --> 00:07:12,400
next few videos is develop
接下来的几个视频是发展

207
00:07:12,860 --> 00:07:14,640
an algorithm called logistic regression
所谓逻辑回归算法

208
00:07:15,550 --> 00:07:17,390
which has the property that the
它具有属性的

209
00:07:17,780 --> 00:07:19,290
output, the predictions of logistic
输出，物流的预测

210
00:07:19,670 --> 00:07:21,220
regression are always between zero
回归总是零之间

211
00:07:21,630 --> 00:07:22,750
and one, and doesn't become
和1，并不会成为

212
00:07:23,060 --> 00:07:24,170
bigger than one or become less
大于1或变得不

213
00:07:24,370 --> 00:07:26,370
than zero and by
大于零并通过

214
00:07:26,530 --> 00:07:28,570
the way, logistic regression is
顺便说一下，logistic回归是

215
00:07:29,090 --> 00:07:30,150
and we will use it as
我们将使用它作为

216
00:07:30,350 --> 00:07:32,770
a classification algorithm in some,
的分类算法在某些，

217
00:07:33,330 --> 00:07:35,060
maybe sometimes confusing that
也许有时混淆了

218
00:07:35,780 --> 00:07:37,410
the term regression appears in
术语回归出现在

219
00:07:37,700 --> 00:07:39,360
his name, even though logistic regression
他的名字，即使logistic回归

220
00:07:39,970 --> 00:07:41,280
is actually a classification algorithm.
实际上是一种分类算法。

221
00:07:42,120 --> 00:07:43,040
But that's just the name it
但是，这仅仅是它的名字

222
00:07:43,160 --> 00:07:46,140
was given for historical reasons so don't be confused by that.
被赋予历史的原因，所以不要被迷惑了。

223
00:07:46,680 --> 00:07:48,340
Logistic Regression is actually a
Logistic回归实际上是一种

224
00:07:48,430 --> 00:07:50,250
classification algorithm that we
分类算法，我们

225
00:07:50,380 --> 00:07:52,030
apply to settings where the
适用于设置在哪里

226
00:07:52,160 --> 00:07:54,780
label Y is discreet valued. The 1001.
标签Y被谨慎的重视。 1001。

227
00:07:55,820 --> 00:07:57,440
So hopefully you now
所以希望你现在

228
00:07:57,680 --> 00:07:59,180
know why if you
知道是什么原因，如果你

229
00:07:59,280 --> 00:08:00,950
have a causation problem using
使用有因果关系问题

230
00:08:01,400 --> 00:08:02,660
linear regression isn't a good idea .
线性回归是不是一个好主意。

231
00:08:03,210 --> 00:08:04,480
In the next video we'll
在接下来的视频中，我们将

232
00:08:04,700 --> 00:08:05,680
start working out the details
开始制定细节

233
00:08:06,290 --> 00:08:07,640
of the logistic regression algorithm.
的logistic回归算法。

