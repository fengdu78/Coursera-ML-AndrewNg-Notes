1
00:00:00,251 --> 00:00:05,622
对于很多机器学习算法 包括线性回归、逻辑回归、神经网络等等
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:00,251 --> 00:00:05,622
For many learning algorithms, among them linear regression, logistic regression and neural networks,

3
00:00:05,622 --> 00:00:11,955
算法的实现都是通过得出某个代价函数 或者某个最优化的目标来实现的

4
00:00:05,622 --> 00:00:11,955
the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective.

5
00:00:11,955 --> 00:00:16,476
然后使用梯度下降这样的方法来求得代价函数的最小值

6
00:00:11,955 --> 00:00:16,476
And then using an algorithm like gradient descent to minimize that cost function.

7
00:00:16,476 --> 00:00:22,461
当我们的训练集较大时 梯度下降算法则显得计算量非常大

8
00:00:16,476 --> 00:00:22,461
We have a very large training set gradient descent becomes a computationally very expensive procedure.

9
00:00:22,461 --> 00:00:29,300
在这段视频中 我想介绍一种跟普通梯度下降不同的方法 随机梯度下降(stochastic gradient descent)

10
00:00:22,461 --> 00:00:29,300
In this video, we'll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent,

11
00:00:29,300 --> 00:00:37,841
用这种方法我们可以将算法运用到较大训练集的情况中

12
00:00:29,300 --> 00:00:37,841
which will allow us to scale these algorithms to much bigger training sets.

13
00:00:37,841 --> 00:00:41,928
假如你要使用梯度下降法来训练某个线性回归模型

14
00:00:37,841 --> 00:00:41,928
Suppose you are training a linear regression model using gradient descent.

15
00:00:41,928 --> 00:00:48,055
简单复习一下 我们的假设函数是这样的

16
00:00:41,928 --> 00:00:48,055
As a quick recap, the hypothesis will look like this, and the cost function will look like this,

17
00:00:48,055 --> 00:00:54,459
代价函数是你的假设在训练集样本上预测的平均平方误差的二分之一倍的求和

18
00:00:48,055 --> 00:00:54,459
which is the sum of one half of the average square error of your hypothesis on your m training examples,

19
00:00:54,459 --> 00:00:59,705
通常我们看到的代价函数都是像这样的弓形函数

20
00:00:54,459 --> 00:00:59,705
and the cost function we've already seen looks like this sort of bow-shaped function.

21
00:00:59,705 --> 00:01:06,659
因此 画出以θ0和θ1为参数的代价函数J 就是这样的弓形函数

22
00:00:59,705 --> 00:01:06,659
So, plotted as function of the parameters theta 0 and theta 1, the cost function J is a sort of a bow-shaped function.

23
00:01:06,659 --> 00:01:10,999
这就是梯度下降算法 在内层循环中

24
00:01:06,659 --> 00:01:10,999
And gradient descent looks like this, where in the inner loop of gradient descent

25
00:01:10,999 --> 00:01:15,594
你需要用这个式子反复更新参数θ的值

26
00:01:10,999 --> 00:01:15,594
you repeatedly update the parameters theta using that expression.

27
00:01:15,594 --> 00:01:22,574
在这段视频剩下的时间里 我将依然以线性回归为例

28
00:01:15,594 --> 00:01:22,574
Now in the rest of this video, I'm going to keep using linear regression as the running example.

29
00:01:22,574 --> 00:01:29,371
但随机梯度下降的思想也可以应用于其他的学习算法

30
00:01:22,574 --> 00:01:29,371
But the ideas here, the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms

31
00:01:29,371 --> 00:01:38,011
比如逻辑回归、神经网络或者其他依靠梯度下降来进行训练的算法中

32
00:01:29,371 --> 00:01:38,011
like logistic regression, neural networks and other algorithms that are based on training gradient descent on a specific training set.

33
00:01:38,011 --> 00:01:43,236
这张图表示的是梯度下降的做法 这个点表示了参数的初始位置

34
00:01:38,011 --> 00:01:43,236
So here's a picture of what gradient descent does, if the parameters are initialized to the point there

35
00:01:43,236 --> 00:01:50,072
那么在你运行梯度下降的过程中 多步迭代最终会将参数锁定到全局最小值

36
00:01:43,236 --> 00:01:50,072
then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum.

37
00:01:50,072 --> 00:01:55,193
步进的轨迹看起来非常快地收敛到全局最小

38
00:01:50,072 --> 00:01:55,193
So take a trajectory that looks like that and heads pretty directly to the global minimum.

39
00:01:55,193 --> 00:01:59,561
而梯度下降法的问题是 当m值很大时

40
00:01:55,193 --> 00:01:59,561
Now, the problem with gradient descent is that if m is large.

41
00:01:59,561 --> 00:02:08,382
计算这个微分项的计算量就变得很大 因为需要对所有m个训练样本求和

42
00:01:59,561 --> 00:02:08,382
Then computing this derivative term can be very expensive, because the surprise, summing over all m examples.

43
00:02:08,382 --> 00:02:15,644
所以假如m的值为3亿 美国就有3亿人口

44
00:02:08,382 --> 00:02:15,644
So if m is 300 million, alright. So in the United States, there are about 300 million people.

45
00:02:15,644 --> 00:02:20,783
美国的人口普查数据就有这种量级的数据记录

46
00:02:15,644 --> 00:02:20,783
And so the US or United States census data may have on the order of that many records.

47
00:02:20,783 --> 00:02:26,715
所以如果想要为这么多数据拟合一个线性回归模型的话 那就需要对所有这3亿数据进行求和

48
00:02:20,783 --> 00:02:26,715
So you want to fit the linear regression model to that then you need to sum over 300 million records.

49
00:02:26,715 --> 00:02:36,385
这样的计算量太大了 这种梯度下降算法也被称为批量梯度下降(batch gradient descent)

50
00:02:26,715 --> 00:02:36,385
And that's very expensive. To give the algorithm a name, this particular version of gradient descent is also called Batch gradient descent.

51
00:02:36,385 --> 00:02:41,352
“批量”就表示我们需要每次都考虑所有的训练样本

52
00:02:36,385 --> 00:02:41,352
And the term Batch refers to the fact that we're looking at all of the training examples at a time.

53
00:02:41,352 --> 00:02:44,303
我们可以称为所有这批训练样本

54
00:02:41,352 --> 00:02:44,303
We call it sort of a batch of all of the training examples.

55
00:02:44,303 --> 00:02:51,853
也许这不是个恰当的名字 但做机器学习的人就是这么称呼它的

56
00:02:44,303 --> 00:02:51,853
And it really isn't the, maybe the best name but this is what machine learning people call this particular version of gradient descent.

57
00:02:51,853 --> 00:02:57,157
想象一下 如果你真的有这3亿人口的数据存在硬盘里

58
00:02:51,853 --> 00:02:57,157
And if you imagine really that you have 300 million census records stored away on disc.

59
00:02:57,157 --> 00:03:05,945
那么这种算法就需要把所有这3亿人口数据读入计算机 仅仅就为了算一个微分项而已

60
00:02:57,157 --> 00:03:05,945
The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term.

61
00:03:05,945 --> 00:03:11,508
你需要将这些数据连续传入计算机 因为计算机存不下那么大的数据量

62
00:03:05,945 --> 00:03:11,508
You need to stream all of these records through computer because you can't store all your records in computer memory.

63
00:03:11,508 --> 00:03:16,425
所以你需要很慢地读取数据 然后计算一个求和 再来算出微分

64
00:03:11,508 --> 00:03:16,425
So you need to read through them and slowly, you know, accumulate the sum in order to compute the derivative.

65
00:03:16,425 --> 00:03:21,452
所有这些做完以后 你才完成了一次梯度下降的迭代

66
00:03:16,425 --> 00:03:21,452
And then having done all that work, that allows you to take one step of gradient descent.

67
00:03:21,452 --> 00:03:24,749
然后你又需要重新来一遍

68
00:03:21,452 --> 00:03:24,749
And now you need to do the whole thing again.

69
00:03:24,749 --> 00:03:28,424
也就是再读取这3亿人口数据 做个求和

70
00:03:24,749 --> 00:03:28,424
You know, scan through all 300 million records, accumulate these sums.

71
00:03:28,424 --> 00:03:32,578
然后做完这些 你又完成了梯度下降的一小步

72
00:03:28,424 --> 00:03:32,578
And having done all that work, you can take another little step using gradient descent.

73
00:03:32,578 --> 00:03:36,959
然后再做一次 你得到第三次迭代 等等

74
00:03:32,578 --> 00:03:36,959
And then do that again. And then you take yet a third step. And so on.

75
00:03:36,959 --> 00:03:40,819
所以 要让算法收敛 绝对需要花很长的时间

76
00:03:36,959 --> 00:03:40,819
And so it's gonna take a long time in order to get the algorithm to converge.

77
00:03:40,819 --> 00:03:45,375
相比于批量梯度下降 我们介绍的方法就完全不同了

78
00:03:40,819 --> 00:03:45,375
In contrast to Batch gradient descent, what we are going to do is come up with a different algorithm

79
00:03:45,375 --> 00:03:50,465
这种方法在每一步迭代中 不用考虑全部的训练样本

80
00:03:45,375 --> 00:03:50,465
that doesn't need to look at all the training examples in every single iteration,

81
00:03:50,465 --> 00:03:55,118
只需要考虑一个训练样本

82
00:03:50,465 --> 00:03:55,118
but that needs to look at only a single training example in one iteration.

83
00:03:55,118 --> 00:03:59,617
在开始介绍新的算法之前 我把批量梯度下降算法再写在这里

84
00:03:55,118 --> 00:03:59,617
Before moving on to the new algorithm, here's just a Batch gradient descent algorithm written out again

85
00:03:59,617 --> 00:04:05,794
这里是代价函数 这里是迭代的更新过程

86
00:03:59,617 --> 00:04:05,794
with that being the cost function and that being the update and of course this term here,

87
00:04:05,794 --> 00:04:10,678
梯度下降法中的这一项

88
00:04:05,794 --> 00:04:10,678
that's used in the gradient descent rule, that is the partial derivative

89
00:04:10,678 --> 00:04:17,933
是最优化目标 代价函数Jtrain(θ) 关于参数θj的偏微分

90
00:04:10,678 --> 00:04:17,933
with respect to the parameters theta J of our optimization objective, J train of theta.

91
00:04:17,933 --> 00:04:23,386
下面我们来看更高效的这种方法

92
00:04:17,933 --> 00:04:23,386
Now, let's look at the more efficient algorithm that scales better to large data sets.

93
00:04:23,386 --> 00:04:26,489
为了更好地描述随机梯度下降算法

94
00:04:23,386 --> 00:04:26,489
In order to work off the algorithms called Stochastic gradient descent,

95
00:04:26,489 --> 00:04:32,657
代价函数的定义有一点区别 我们定义参数θ

96
00:04:26,489 --> 00:04:32,657
this vectors the cost function in a slightly different way then they define the cost of the parameter theta

97
00:04:32,657 --> 00:04:40,471
关于训练样本(x(i),y(i))的代价 等于二分之一倍

98
00:04:32,657 --> 00:04:40,471
with respect to a training example x(i), y(i) to be equal to one half times the squared error

99
00:04:40,471 --> 00:04:44,791
我的假设h(x(i))跟实际输出y(i)的误差的平方

100
00:04:40,471 --> 00:04:44,791
that my hypothesis incurs on that example, x(i), y(i).

101
00:04:44,791 --> 00:04:53,386
因此这个代价函数值实际上测量的是我的假设在某个样本(x(i),y(i))上的表现

102
00:04:44,791 --> 00:04:53,386
So this cost function term really measures how well is my hypothesis doing on a single example x(i), y(i).

103
00:04:53,386 --> 00:05:01,010
你可能已经发现 总体的代价函数Jtrain可以被写成这样等效的形式

104
00:04:53,386 --> 00:05:01,010
Now you notice that the overall cost function j train can now be written in this equivalent form.

105
00:05:01,010 --> 00:05:09,606
Jtrain(θ)就是我的假设函数 在所有m个训练样本中的每一个样本(x(i),y(i))上的代价函数的平均值

106
00:05:01,010 --> 00:05:09,606
So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i), y(i).

107
00:05:09,606 --> 00:05:13,522
用这样的方法应用到线性回归中

108
00:05:09,606 --> 00:05:13,522
Armed with this view of the cost function for linear regression,

109
00:05:13,522 --> 00:05:17,636
我来写出随机梯度下降的算法

110
00:05:13,522 --> 00:05:17,636
let me now write out what Stochastic gradient descent does.

111
00:05:17,636 --> 00:05:26,940
随机梯度下降法的第一步是将所有数据打乱

112
00:05:17,636 --> 00:05:26,940
The first step of Stochastic gradient descent is to randomly shuffle the data set.

113
00:05:26,940 --> 00:05:32,539
我说的随机打乱的意思是 将所有m个训练样本重新排列

114
00:05:26,940 --> 00:05:32,539
So by that I just mean randomly shuffle, or randomly reorder your m training examples.

115
00:05:32,539 --> 00:05:37,450
这就是标准的数据预处理过程 稍后我们再回来讲

116
00:05:32,539 --> 00:05:37,450
It's sort of a standard pre-processing step, come back to this in a minute.

117
00:05:37,450 --> 00:05:42,997
随机梯度下降的主要算法如下

118
00:05:37,450 --> 00:05:42,997
But the main work of Stochastic gradient descent is then done in the following.

119
00:05:42,997 --> 00:05:48,150
在i等于1到m中进行循环

120
00:05:42,997 --> 00:05:48,150
We're going to repeat for i equals 1 through m.

121
00:05:48,150 --> 00:05:53,067
也就是对所有m个训练样本进行遍历 然后进行如下更新

122
00:05:48,150 --> 00:05:53,067
So we'll repeatedly scan through my training examples and perform the following update.

123
00:05:53,067 --> 00:06:06,523
我们按照这样的公式进行更新

124
00:05:53,067 --> 00:06:06,523
Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j.

125
00:06:06,523 --> 00:06:12,961
同样还是对所有j的值进行

126
00:06:06,523 --> 00:06:12,961
And we're going to do this update as usual for all values of j.

127
00:06:12,961 --> 00:06:24,708
不难发现 这一项实际上就是我们批量梯度下降算法中 求和式里面的那一部分

128
00:06:12,961 --> 00:06:24,708
Now, you notice that this term over here is exactly what we had inside the summation for Batch gradient descent.

129
00:06:24,708 --> 00:06:31,256
事实上 如果你数学比较好的话 你可以证明这一项 也就是这一项

130
00:06:24,708 --> 00:06:31,256
In fact, for those of you that are calculus is possible to show that that term here, that's this term here,

131
00:06:31,256 --> 00:06:43,511
是等于这个cost函数关于参数θj的偏微分

132
00:06:31,256 --> 00:06:43,511
is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i), y(i).

133
00:06:43,511 --> 00:06:47,383
这个cost函数就是我们之前先定义的代价函数

134
00:06:43,511 --> 00:06:47,383
Where cost is of course this thing that was defined previously.

135
00:06:47,383 --> 00:06:52,081
最后画上大括号结束算法的循环

136
00:06:47,383 --> 00:06:52,081
And just the wrap of the algorithm, let me close my curly braces over there.

137
00:06:52,081 --> 00:06:59,365
随机梯度下降的做法实际上就是扫描所有的训练样本

138
00:06:52,081 --> 00:06:59,365
So what Stochastic gradient descent is doing is it is actually scanning through the training examples.

139
00:06:59,365 --> 00:07:04,349
首先是我的第一组训练样本(x(1),y(1))

140
00:06:59,365 --> 00:07:04,349
And first it's gonna look at my first training example x(1), y(1).

141
00:07:04,349 --> 00:07:09,399
然后只对这第一个训练样本 我们的梯度下降

142
00:07:04,349 --> 00:07:09,399
And then looking at only this first example, it's gonna take like a basically a little gradient descent step

143
00:07:09,399 --> 00:07:13,725
只对这第一个训练样本的代价函数进行

144
00:07:09,399 --> 00:07:13,725
with respect to the cost of just this first training example.

145
00:07:13,725 --> 00:07:15,717
换句话说 我们要关注第一个样本

146
00:07:13,725 --> 00:07:15,717
So in other words, we're going to look at the first example

147
00:07:15,717 --> 00:07:21,214
然后把参数θ稍微修改一点 使其对第一个训练样本的拟合变得好一点

148
00:07:15,717 --> 00:07:21,214
and modify the parameters a little bit to fit just the first training example a little bit better.

149
00:07:21,214 --> 00:07:29,244
完成这个内层循环以后 然后再转向第二个训练样本

150
00:07:21,214 --> 00:07:29,244
Having done this inside this inner for-loop is then going to go on to the second training example.

151
00:07:29,244 --> 00:07:33,848
然后还是一样 在参数空间中进步一小步

152
00:07:29,244 --> 00:07:33,848
And what it's going to do there is take another little step in parameter space,

153
00:07:33,848 --> 00:07:39,682
也就是稍微把参数修改一点 然后让它对第二个样本的拟合更好一点

154
00:07:33,848 --> 00:07:39,682
so modify the parameters just a little bit to try to fit just a second training example a little bit better.

155
00:07:39,682 --> 00:07:44,130
做完第二个 再转向第三个训练样本

156
00:07:39,682 --> 00:07:44,130
Having done that, is then going to go onto my third training example.

157
00:07:44,130 --> 00:07:51,722
同样还是修改参数 让它更好的拟合第三个训练样本

158
00:07:44,130 --> 00:07:51,722
And modify the parameters to try to fit just the third training example a little bit better, and so on

159
00:07:51,722 --> 00:07:55,114
以此类推 直到完成所有的训练集

160
00:07:51,722 --> 00:07:55,114
until you know, you get through the entire training set.

161
00:07:55,114 --> 00:08:01,297
因此这种重复循环会遍历整个训练集

162
00:07:55,114 --> 00:08:01,297
And then this ultra repeat loop may cause it to take multiple passes over the entire training set.

163
00:08:01,297 --> 00:08:07,346
从这个角度分析随机梯度下降算法 我们能更好地理解为什么一开始要随机打乱数据

164
00:08:01,297 --> 00:08:07,346
This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set.

165
00:08:07,346 --> 00:08:10,772
这保证了我们在扫描训练集时

166
00:08:07,346 --> 00:08:10,772
This doesn't show us that when we scan through the training site here,

167
00:08:10,772 --> 00:08:15,197
我们对训练集样本的访问时随机的顺序

168
00:08:10,772 --> 00:08:15,197
that we end up visiting the training examples in some sort of randomly sorted order.

169
00:08:15,197 --> 00:08:21,229
不管你的数据是否已经随机排列过 或者一开始就是某个奇怪的顺序

170
00:08:15,197 --> 00:08:21,229
Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order,

171
00:08:21,229 --> 00:08:26,391
实际上这一步能让你的随机梯度下降稍微快一些收敛

172
00:08:21,229 --> 00:08:26,391
in practice this would just speed up the conversions to Stochastic gradient descent just a little bit.

173
00:08:26,391 --> 00:08:30,985
所以为了保险起见 最好还是先把所有数据随机打乱一下

174
00:08:26,391 --> 00:08:30,985
So in the interest of safety, it's usually better to randomly shuffle the data set if you aren't sure

175
00:08:30,985 --> 00:08:34,056
如果你不知道是否已经随机排列过的话

176
00:08:30,985 --> 00:08:34,056
if it came to you in randomly sorted order.

177
00:08:34,056 --> 00:08:37,240
但随机梯度下降的更重要的一点是

178
00:08:34,056 --> 00:08:37,240
But more importantly another view of Stochastic gradient descent is

179
00:08:37,240 --> 00:08:45,504
跟批量梯度下降不同 随机梯度下降不需要对所有m个训练样本

180
00:08:37,240 --> 00:08:45,504
that it's a lot like descent but rather than wait to sum up these gradient terms over all m training examples,

181
00:08:45,504 --> 00:08:50,624
求和来得到梯度项 而是只需要对单个训练样本求出这个梯度项

182
00:08:45,504 --> 00:08:50,624
what we're doing is we're taking this gradient term using just one single training example

183
00:08:50,624 --> 00:08:54,810
我们已经在这个过程中开始优化参数了

184
00:08:50,624 --> 00:08:54,810
and we're starting to make progress in improving the parameters already.

185
00:08:54,810 --> 00:09:02,248
就不用把所有那3亿的美国人口普查的数据拿来遍历一遍

186
00:08:54,810 --> 00:09:02,248
So rather than, you know, waiting 'till taking a path through all 300,000 United States Census records,

187
00:09:02,248 --> 00:09:05,632
不需要对所有这些数据进行扫描

188
00:09:02,248 --> 00:09:05,632
say, rather than needing to scan through all of the training examples

189
00:09:05,632 --> 00:09:09,947
然后才一点点地修改参数 直到达到全局最小值

190
00:09:05,632 --> 00:09:09,947
before we can modify the parameters a little bit and make progress towards a global minimum.

191
00:09:09,947 --> 00:09:14,975
对随机梯度下降来说 我们只需要一次关注一个训练样本

192
00:09:09,947 --> 00:09:14,975
For Stochastic gradient descent instead we just need to look at a single training example

193
00:09:14,975 --> 00:09:22,188
而我们已经开始一点点把参数朝着全局最小值的方向进行修改了

194
00:09:14,975 --> 00:09:22,188
and we're already starting to make progress in this case of parameters towards, moving the parameters towards the global minimum.

195
00:09:22,188 --> 00:09:27,558
这里把这个算法再重新写一遍 第一步是打乱数据

196
00:09:22,188 --> 00:09:27,558
So, here's the algorithm written out again where the first step is to randomly shuffle the data

197
00:09:27,558 --> 00:09:35,089
第二步是算法的关键 是关于某个单一的训练样本(x(i),y(i))来对参数进行更新

198
00:09:27,558 --> 00:09:35,089
and the second step is where the real work is done, where that's the update with respect to a single training example x(i), y(i).

199
00:09:35,089 --> 00:09:40,139
让我们来看看 这个算法是如何更新参数θ的

200
00:09:35,089 --> 00:09:40,139
So, let's see what this algorithm does to the parameters.

201
00:09:40,139 --> 00:09:43,467
之前我们已经看到 当使用批量梯度下降的时候

202
00:09:40,139 --> 00:09:43,467
Previously, we saw that when we are using Batch gradient descent,

203
00:09:43,467 --> 00:09:46,331
需要考虑所有的训练样本数据

204
00:09:43,467 --> 00:09:46,331
that is the algorithm that looks at all the training examples in time,

205
00:09:46,331 --> 00:09:53,397
批量梯度下降的收敛过程 会倾向于一条近似的直线 一直找到全局最小值

206
00:09:46,331 --> 00:09:53,397
Batch gradient descent will tend to, you know, take a reasonably straight line trajectory to get to the global minimum like that.

207
00:09:53,397 --> 00:09:59,956
与此不同的是 在随机梯度下降中 每一次迭代都会更快

208
00:09:53,397 --> 00:09:59,956
In contrast with Stochastic gradient descent every iteration is going to be much faster

209
00:09:59,956 --> 00:10:03,108
因为我们不需要对所有训练样本进行求和

210
00:09:59,956 --> 00:10:03,108
because we don't need to sum up over all the training examples.

211
00:10:03,108 --> 00:10:07,259
每一次迭代只需要保证对一个训练样本拟合好就行了

212
00:10:03,108 --> 00:10:07,259
But every iteration is just trying to fit single training example better.

213
00:10:07,259 --> 00:10:13,931
所以 如果我们从这个点开始进行随机梯度下降的话

214
00:10:07,259 --> 00:10:13,931
So, if we were to start stochastic gradient descent, oh, let's start stochastic gradient descent at a point like that.

215
00:10:13,931 --> 00:10:19,556
第一次迭代 可能会让参数朝着这个方向移动

216
00:10:13,931 --> 00:10:19,556
The first iteration, you know, may take the parameters in that direction and

217
00:10:19,556 --> 00:10:23,791
然后第二次迭代 只考虑第二个训练样本

218
00:10:19,556 --> 00:10:23,791
maybe the second iteration looking at just the second example maybe just by chance,

219
00:10:23,791 --> 00:10:28,278
假如很不幸 我们朝向了一个错误的方向

220
00:10:23,791 --> 00:10:28,278
we get more unlucky and actually head in a bad direction with the parameters like that.

221
00:10:28,278 --> 00:10:33,731
第三次迭代 我们又尽力让参数修改到拟合第三组训练样本

222
00:10:28,278 --> 00:10:33,731
In the third iteration where we tried to modify the parameters to fit just the third training examples better,

223
00:10:33,731 --> 00:10:36,418
可能最终会得到这个方向

224
00:10:33,731 --> 00:10:36,418
maybe we'll end up heading in that direction.

225
00:10:36,418 --> 00:10:42,717
然后再考虑第四个训练样本 做同样的事 然后第五第六第七 等等

226
00:10:36,418 --> 00:10:42,717
And then we'll look at the fourth training example and we will do that. The fifth example, sixth example, 7th and so on.

227
00:10:42,717 --> 00:10:46,725
在你运行随机梯度下降的过程中 你会发现

228
00:10:42,717 --> 00:10:46,725
And as you run Stochastic gradient descent, what you find is that

229
00:10:46,725 --> 00:10:52,923
一般来讲 参数是朝着全局最小值的方向被更新的 但也不一定

230
00:10:46,725 --> 00:10:52,923
it will generally move the parameters in the direction of the global minimum, but not always.

231
00:10:52,923 --> 00:11:00,117
所以看起来它是以某个比较随机、迂回的路径在朝全局最小值逼近

232
00:10:52,923 --> 00:11:00,117
And so take some more random-looking, circuitous path to watch the global minimum.

233
00:11:00,117 --> 00:11:07,630
实际上 你运行随机梯度下降 和批量梯度下降 两种方法的收敛形式是不同的

234
00:11:00,117 --> 00:11:07,630
And in fact as you run Stochastic gradient descent it doesn't actually converge in the same same sense as Batch gradient descent does

235
00:11:07,630 --> 00:11:15,196
实际上随机梯度下降是在某个靠近全局最小值的区域内徘徊

236
00:11:07,630 --> 00:11:15,196
and what it ends up doing is wandering around continuously in some region that's in some region close to the global minimum,

237
00:11:15,196 --> 00:11:18,740
而不是直接逼近全局最小值并停留在那点

238
00:11:15,196 --> 00:11:18,740
but it doesn't just get to the global minimum and stay there.

239
00:11:18,740 --> 00:11:21,676
但实际上这并没有多大问题

240
00:11:18,740 --> 00:11:21,676
But in practice this isn't a problem because, you know, so

241
00:11:21,676 --> 00:11:26,788
只要参数最终移动到某个非常靠近全局最小值的区域内

242
00:11:21,676 --> 00:11:26,788
long as the parameters end up in some region there maybe it is pretty close to the global minimum.

243
00:11:26,788 --> 00:11:32,164
只要参数逼近到足够靠近全局最小值 这也会得出一个较为不错的假设

244
00:11:26,788 --> 00:11:32,164
So, as parameters end up pretty close to the global minimum, that will be a pretty good hypothesis

245
00:11:32,164 --> 00:11:36,340
所以 通常我们用随机梯度下降法

246
00:11:32,164 --> 00:11:36,340
and so usually running Stochastic gradient descent

247
00:11:36,340 --> 00:11:43,658
也能得到一个很接近全局最小值的参数 对于实际应用的目的来说 已经足够了

248
00:11:36,340 --> 00:11:43,658
we get a parameter near the global minimum and that's good enough for, you know, essentially any, most practical purposes.

249
00:11:43,658 --> 00:11:47,121
最后一点细节 在随机梯度下降中

250
00:11:43,658 --> 00:11:47,121
Just one final detail. In Stochastic gradient descent,

251
00:11:47,121 --> 00:11:51,099
我们有一个外层循环 它决定了内层循环的执行次数

252
00:11:47,121 --> 00:11:51,099
we had this outer loop repeat which says to do this inner loop multiple times.

253
00:11:51,099 --> 00:11:53,892
所以 外层循环应该执行多少次呢

254
00:11:51,099 --> 00:11:53,892
So, how many times do we repeat this outer loop?

255
00:11:53,892 --> 00:11:59,336
这取决于训练样本的大小 通常一次就够了

256
00:11:53,892 --> 00:11:59,336
Depending on the size of the training set, doing this loop just a single time may be enough.

257
00:11:59,336 --> 00:12:02,064
最多到10次 是比较典型的

258
00:11:59,336 --> 00:12:02,064
And up to, you know, maybe 10 times may be typical

259
00:12:02,064 --> 00:12:05,852
所以我们可以循环执行内层1到10次

260
00:12:02,064 --> 00:12:05,852
so we may end up repeating this inner loop anywhere from once to ten times.

261
00:12:05,852 --> 00:12:12,309
因此 如果我们有非常大量的数据 比如美国普查的人口数据

262
00:12:05,852 --> 00:12:12,309
So if we have a you know, truly massive data set like the this US census gave us that example

263
00:12:12,309 --> 00:12:15,260
我说的3亿人口数据的例子

264
00:12:12,309 --> 00:12:15,260
that I've been talking about with 300 million examples,

265
00:12:15,260 --> 00:12:19,609
所以每次你只需要考虑一个训练样本

266
00:12:15,260 --> 00:12:19,609
it is possible that by the time you've taken just a single pass through your training set.

267
00:12:19,609 --> 00:12:23,073
这里的i就是从1到3亿了

268
00:12:19,609 --> 00:12:23,073
So, this is for i equals 1 through 300 million.

269
00:12:23,073 --> 00:12:25,720
所以可能你每次只需要考虑一个训练样本

270
00:12:23,073 --> 00:12:25,720
It's possible that by the time you've taken a single pass through your data set

271
00:12:25,720 --> 00:12:29,872
你就能训练出非常好的假设

272
00:12:25,720 --> 00:12:29,872
you might already have a perfectly good hypothesis.

273
00:12:29,872 --> 00:12:36,613
这时 由于m非常大 那么内循环只用做一次就够了

274
00:12:29,872 --> 00:12:36,613
In which case, you know, this inner loop you might need to do only once if m is very, very large.

275
00:12:36,613 --> 00:12:43,071
但通常来说 循环1到10次都是非常合理的

276
00:12:36,613 --> 00:12:43,071
But in general taking anywhere from 1 through 10 passes through your data set, you know, maybe fairly common.

277
00:12:43,071 --> 00:12:45,439
但这还是取决于你训练样本的大小

278
00:12:43,071 --> 00:12:45,439
But really it depends on the size of your training set.

279
00:12:45,439 --> 00:12:49,413
如果你跟批量梯度下降比较一下的话

280
00:12:45,439 --> 00:12:49,413
And if you contrast this to Batch gradient descent.

281
00:12:49,413 --> 00:12:53,905
批量梯度下降在一步梯度下降的过程中

282
00:12:49,413 --> 00:12:53,905
With Batch gradient descent, after taking a pass through your entire training set,

283
00:12:53,905 --> 00:12:57,034
就需要考虑全部的训练样本

284
00:12:53,905 --> 00:12:57,034
you would have taken just one single gradient descent steps.

285
00:12:57,034 --> 00:13:01,983
所以批量梯度下降就是这样微小的一次次移动

286
00:12:57,034 --> 00:13:01,983
So one of these little baby steps of gradient descent where you just take one small gradient descent step

287
00:13:01,983 --> 00:13:05,776
这也是为什么随机梯度下降法要快得多

288
00:13:01,983 --> 00:13:05,776
and this is why Stochastic gradient descent can be much faster.

289
00:13:05,776 --> 00:13:10,880
这就是随机梯度下降了

290
00:13:05,776 --> 00:13:10,880
So, that was the Stochastic gradient descent algorithm.

291
00:13:10,880 --> 00:13:15,594
如果你做好了 你应该能把在很多学习算法中应用大量数据了

292
00:13:10,880 --> 00:13:15,594
And if you implement it, hopefully that will allow you to scale up many of your learning algorithms

293
00:13:15,594 --> 99:59:59,000
并且会得到更好的算法表现

294
00:13:15,594 --> 99:59:59,000
to much bigger data sets and get much more performance that way.

