1
00:00:00,530 --> 00:00:01,650
In the last few videos, we
在最后几个视频中，(字幕翻译：中国海洋大学，周丽雅)

2
00:00:01,730 --> 00:00:03,890
talked about a collaborative filtering algorithm.
我们讨论一个协同过滤算法。

3
00:00:04,830 --> 00:00:05,890
In this video I'm going to
在这个视频中，我将

4
00:00:05,970 --> 00:00:07,120
say a little bit about the
介绍一下这个算法的

5
00:00:07,490 --> 00:00:09,090
vectorization implementation of this algorithm.
向量化实现，

6
00:00:09,980 --> 00:00:12,670
And also talk a little bit about other things you can do with this algorithm.
另外再介绍一下你使用这个算法可以实现的一些功能。

7
00:00:13,340 --> 00:00:14,520
For example, one of the
比如说，你可以用这个算法实现：

8
00:00:14,600 --> 00:00:15,830
things you can do is, given
给定一个商品，

9
00:00:16,180 --> 00:00:17,390
one product can you find
你可以找到

10
00:00:17,770 --> 00:00:19,160
other products that are related
与之相关的其他商品。

11
00:00:19,270 --> 00:00:20,210
to this so that for
比如说：

12
00:00:20,490 --> 00:00:23,140
example, a user has recently been looking at one product.
一个用户最近一直在寻找一个商品

13
00:00:23,650 --> 00:00:24,990
Are there other related products
有没有一些相关的其他商品

14
00:00:25,520 --> 00:00:27,170
that you could recommend to this user?
你能推荐给这个用户？

15
00:00:27,620 --> 00:00:28,980
So let's see what we could do about that.
所以针对这样的问题一起看一下我们如何解决

16
00:00:30,170 --> 00:00:31,190
What I'd like to do is work
我希望可以

17
00:00:31,550 --> 00:00:33,520
out an alternative way of
找到另一种方法

18
00:00:33,740 --> 00:00:35,710
writing out the predictions of the collaborative filtering algorithm.
写出协同过滤算法的预测值

19
00:00:37,370 --> 00:00:38,590
To start, here is our
首先，这是我们的数据集，

20
00:00:38,960 --> 00:00:40,440
data set with our
数据集中包含

21
00:00:40,750 --> 00:00:41,880
five movies and what I'm
五部电影，下一步

22
00:00:42,160 --> 00:00:43,150
going to do is take
我要做的是

23
00:00:43,390 --> 00:00:44,520
all the ratings by all the
得到所有用户对所有电影的评分

24
00:00:44,850 --> 00:00:46,500
users and group them into
然后把它们分组写入矩阵

25
00:00:47,080 --> 00:00:48,800
a matrix. So, here we have
这里我们有五部电影、

26
00:00:49,200 --> 00:00:51,390
five movies and four
四个用户，

27
00:00:51,670 --> 00:00:53,390
users, and so this
因此

28
00:00:53,670 --> 00:00:54,550
matrix y is going to be
这个矩阵y是5行4列的矩阵。

29
00:00:54,910 --> 00:00:57,110
a 5 by 4 matrix. It's just you know, taking all
正如你所知，构成矩阵时，要包括所有的元素，

30
00:00:57,340 --> 00:00:58,770
of the elements, all of this data.
和所有的数据，

31
00:00:59,820 --> 00:01:02,390
Including question marks, and grouping them into this matrix.
包括问号，然后把它们按组写入矩阵。

32
00:01:03,290 --> 00:01:04,470
And of course the elements of this
当然，这个矩阵的

33
00:01:04,650 --> 00:01:06,400
matrix of the (i, j) element of
第(i, j)个元素

34
00:01:06,500 --> 00:01:07,860
this matrix is really what
就是

35
00:01:08,060 --> 00:01:09,710
we were previously writing as y
我们之前说的y（i,j）

36
00:01:10,520 --> 00:01:12,090
superscript i, j. It's
其中ij是上标。它是

37
00:01:12,220 --> 00:01:13,480
the rating given to movie i
第j个用户给第i部电影的评分。

38
00:01:14,140 --> 00:01:15,640
by user j. Given this
给定矩阵y

39
00:01:16,070 --> 00:01:17,290
matrix y of all the
y包含所有我们已知的评分，

40
00:01:17,430 --> 00:01:18,520
ratings that we have, there's
有

41
00:01:18,700 --> 00:01:20,500
an alternative way of writing
另一种方法可以写出

42
00:01:20,880 --> 00:01:23,340
out all the predictive ratings of the algorithm.
这个算法的所有预测评分。

43
00:01:24,320 --> 00:01:26,210
And, in particular if you
尤其是如果你想

44
00:01:26,430 --> 00:01:27,540
look at what a certain
查看某一个用户

45
00:01:27,920 --> 00:01:29,480
user predicts on a
对某一个电影的评分预测

46
00:01:29,690 --> 00:01:31,250
certain movie, what user j
用户j

47
00:01:31,950 --> 00:01:35,540
predicts on movie i is given by this formula.
对电影i的评分预测由这个公式给出（θ(j)的转置乘以x(i)）

48
00:01:37,010 --> 00:01:38,570
And so, if you have
因此，如果你

49
00:01:39,440 --> 00:01:40,330
a matrix of the predicted
有一个预测评分的矩阵

50
00:01:40,910 --> 00:01:42,000
ratings, what you would
你所拥有的

51
00:01:42,180 --> 00:01:43,600
have is the following
就是下面这个矩阵,

52
00:01:45,030 --> 00:01:48,140
matrix where the i, j entry.
矩阵元素的标号为i,j

53
00:01:49,650 --> 00:01:51,440
So this corresponds to the rating
这对应了我们预测的

54
00:01:52,000 --> 00:01:54,020
that we predict using j
用户j给电影i的打分

55
00:01:54,460 --> 00:01:55,690
will give to movie i

56
00:01:57,130 --> 00:01:58,440
is exactly equal to that
这与

57
00:01:58,790 --> 00:02:00,680
theta j transpose
theta j 转置乘以xi的值相等。

58
00:02:00,900 --> 00:02:01,940
XI, and so, you know, this is a matrix
因此，这个矩阵中

59
00:02:02,520 --> 00:02:04,310
where this first element
第一个元素

60
00:02:04,750 --> 00:02:05,930
the one-one element is a
即第一行第一列的元素

61
00:02:06,220 --> 00:02:07,450
predictive rating of user one
是用户一对

62
00:02:07,760 --> 00:02:09,360
or movie one and this
电影一的评分预测

63
00:02:09,560 --> 00:02:11,070
element, this is the one-two
这是第一行第二个元素

64
00:02:11,430 --> 00:02:12,680
element is the predicted rating
它是第二个用户

65
00:02:13,470 --> 00:02:14,640
of user two on movie
对第一部电影的评分预测，

66
00:02:14,930 --> 00:02:16,070
one, and so on,
以此类推。

67
00:02:16,630 --> 00:02:18,670
and this is the
这是

68
00:02:19,000 --> 00:02:20,130
predicted rating of user one
第一个用户

69
00:02:20,930 --> 00:02:23,380
on the last movie and
对最后一部电影的评分预测。

70
00:02:23,640 --> 00:02:25,100
if you want, you know,
如果你要预测，

71
00:02:25,400 --> 00:02:26,870
this rating is what we
则这个评分就是

72
00:02:27,020 --> 00:02:28,050
would have predicted for this value
对这个值的预测；

73
00:02:29,050 --> 00:02:32,470
and this rating is
这个评分就是

74
00:02:32,650 --> 00:02:33,570
what we would have predicted for that
对这个值的预测，

75
00:02:33,910 --> 00:02:35,080
value, and so on.
以此类推。

76
00:02:36,180 --> 00:02:37,480
Now, given this matrix of
现在，给定这个预测评分矩阵

77
00:02:37,560 --> 00:02:39,290
Predictive ratings there is then
则有一个

78
00:02:39,610 --> 00:02:42,670
a simpler or vectorized way of writing these out.
比较简单的或者向量化的方法来写出它们。

79
00:02:43,640 --> 00:02:44,640
In particular if I define
比如说，如果我定义

80
00:02:45,120 --> 00:02:46,850
the matrix x, and this
矩阵x,它

81
00:02:46,970 --> 00:02:48,090
is going to be just
可以写成

82
00:02:48,370 --> 00:02:50,980
like the matrix we had earlier for linear regression to be
像之前讲过的线性回归的矩阵形式

83
00:02:52,070 --> 00:02:53,820
sort of x1 transpose x2
第一行是x1的转置

84
00:02:55,050 --> 00:02:57,060
transpose down to
然后第二行是x2的转置

85
00:02:58,530 --> 00:03:01,740
x of nm transpose.
一直到xnm的转置

86
00:03:02,420 --> 00:03:03,320
So I'm take all the features
我将提取所有的电影的特征

87
00:03:04,210 --> 00:03:05,670
for my movies and stack
然后逐行的

88
00:03:06,140 --> 00:03:07,260
them in rows.
写入矩阵中。

89
00:03:07,950 --> 00:03:08,860
So if you think of
所以如果将

90
00:03:08,980 --> 00:03:09,810
each movie as one example
每部电影看作一个样本

91
00:03:10,350 --> 00:03:11,200
and stack all of the features
将不同电影的所有属性都

92
00:03:11,670 --> 00:03:13,460
of the different movies and rows.
按行写入矩阵

93
00:03:14,290 --> 00:03:16,160
And if we also to
如果我们

94
00:03:16,280 --> 00:03:18,550
find a matrix capital theta,
找到一个矩阵，用大写的theta 表示

95
00:03:19,870 --> 00:03:20,840
and what I'm going to
我要做的是

96
00:03:21,180 --> 00:03:22,490
do is take each of
取出每个

97
00:03:22,750 --> 00:03:25,780
the per user parameter
用户参数向量

98
00:03:26,280 --> 00:03:28,520
vectors, and stack them in rows, like so.
像这样按行写入

99
00:03:28,790 --> 00:03:29,690
So that's theta 1, which
这是theta 1

100
00:03:30,220 --> 00:03:31,880
is the parameter vector for the first user.
是对第一个用户的参数向量

101
00:03:33,430 --> 00:03:36,100
And, you know, theta 2, and
theta 2

102
00:03:37,040 --> 00:03:38,100
so, you must stack
你必须像这样

103
00:03:38,360 --> 00:03:39,470
them in rows like this to
按行把它们写入

104
00:03:39,650 --> 00:03:41,530
define a matrix capital
来定义矩阵大写theta

105
00:03:42,070 --> 00:03:43,830
theta and so I have
因此我有

106
00:03:45,870 --> 00:03:48,410
nu parameter vectors all stacked in rows like this.
nu 个参数向量，像这样按行写入矩阵

107
00:03:50,000 --> 00:03:51,390
Now given this definition
现在给定这个

108
00:03:52,080 --> 00:03:53,400
for the matrix x and this
对矩阵x的定义，和

109
00:03:53,590 --> 00:03:54,870
definition for the matrix theta
这个对矩阵theta的定义

110
00:03:55,820 --> 00:03:56,970
in order to have a
为了获得

111
00:03:57,290 --> 00:03:59,330
vectorized way of computing the
一个向量化方法来计算

112
00:03:59,420 --> 00:04:00,330
matrix of all the predictions
预测矩阵，

113
00:04:01,060 --> 00:04:03,570
you can just compute x times
你可以只是计算x乘以

114
00:04:04,710 --> 00:04:07,050
the matrix theta transpose, and
矩阵theta 的转置，

115
00:04:07,160 --> 00:04:08,380
that gives you a vectorized way
它就是一个向量化的方法

116
00:04:08,570 --> 00:04:10,530
of computing this matrix over here.
来计算这个矩阵，

117
00:04:11,680 --> 00:04:12,460
To give the collaborative filtering
这个协同过滤算法

118
00:04:12,480 --> 00:04:15,220
algorithm that you've been using another name.
有另一个名字，

119
00:04:16,070 --> 00:04:17,190
The algorithm that we're using
我们正使用的这个算法

120
00:04:17,660 --> 00:04:19,840
is also called low rank
也叫低秩

121
00:04:21,240 --> 00:04:22,540
matrix factorization.
矩阵分解

122
00:04:24,280 --> 00:04:25,410
And so if you hear
因此如果你听到

123
00:04:25,620 --> 00:04:26,760
people talk about low rank matrix
人们谈论低秩矩阵

124
00:04:27,210 --> 00:04:29,490
factorization that's essentially exactly
分解，基本上他们所说的就是

125
00:04:30,390 --> 00:04:32,100
the algorithm that we have been talking about.
我们正在讨论的这个算法。

126
00:04:32,590 --> 00:04:33,900
And this term comes from the
这个术语来自于：

127
00:04:33,990 --> 00:04:36,100
property that this matrix
这个矩阵的数学性质；

128
00:04:36,770 --> 00:04:38,880
x times theta transpose has a
矩阵x乘以theta 的转置

129
00:04:39,110 --> 00:04:40,780
mathematical property in linear
在线性代数中有一个数学性质

130
00:04:41,030 --> 00:04:42,410
algebra called that this
称为

131
00:04:42,670 --> 00:04:43,820
is a low rank matrix and
低秩矩阵

132
00:04:44,720 --> 00:04:45,800
so that's what gives
这就是为什么算法

133
00:04:46,060 --> 00:04:47,190
rise to this name low
起名叫

134
00:04:47,340 --> 00:04:48,570
rank matrix factorization for these
低秩矩阵分解的原因。

135
00:04:48,930 --> 00:04:50,240
algorithms, because of this low
因为

136
00:04:50,410 --> 00:04:53,580
rank property of this matrix x theta transpose.
矩阵x乘以theta 的转置的低秩性质。

137
00:04:54,830 --> 00:04:55,640
In case you don't know what
如果你不知道什么

138
00:04:55,910 --> 00:04:57,310
low rank means or in case you don't
是低秩，或者如果你不知道

139
00:04:57,620 --> 00:04:59,770
know what a low rank matrix is, don't worry about it.
一个低秩矩阵是什么，不要紧，

140
00:04:59,970 --> 00:05:02,820
You really don't need to know that in order to use this algorithm.
如果只是为了使用这个算法，你不需要知道那些知识。

141
00:05:03,740 --> 00:05:04,790
But if you're an expert in
但是如果你是一个线性代数领域的专家，或了解线性代数

142
00:05:04,890 --> 00:05:06,110
linear algebra, that's what gives
那是为什么这个算法

143
00:05:06,320 --> 00:05:07,580
this algorithm, this other name
又有另一个名字

144
00:05:07,850 --> 00:05:12,370
of low rank matrix factorization.
低秩矩阵分解的原因

145
00:05:12,620 --> 00:05:14,090
Finally, having run the
最后，在已经运行了

146
00:05:14,300 --> 00:05:16,350
collaborative filtering algorithm here's
协同过滤算法之后

147
00:05:17,310 --> 00:05:18,160
something else that you can do
再讲一个问题，

148
00:05:18,530 --> 00:05:20,060
which is use the learned
利用已经学习到的属性

149
00:05:20,320 --> 00:05:23,510
features in order to find related movies.
来找到相关的电影。

150
00:05:25,060 --> 00:05:26,810
Specifically for each product i
具体的说，就是对每个商品i,

151
00:05:27,050 --> 00:05:27,810
really for each movie i, we've
比如对每个电影i，我们已经

152
00:05:28,810 --> 00:05:30,970
learned a feature vector xi.
学到一个属性向量xi,

153
00:05:31,740 --> 00:05:32,880
So, you know, when you learn a
当你学习某一组特征时，

154
00:05:32,930 --> 00:05:34,220
certain features without really know
你之前并不知道

155
00:05:34,590 --> 00:05:35,420
that can the advance what the
该选取哪些

156
00:05:35,610 --> 00:05:37,850
different features are going to be, but if you
不同的特征，但是如果你

157
00:05:37,940 --> 00:05:39,550
run the algorithm and perfectly the features
运行这个算法，一些特征

158
00:05:39,990 --> 00:05:41,690
will tend to capture what are
将捕捉到

159
00:05:41,930 --> 00:05:43,490
the important aspects of these

160
00:05:43,730 --> 00:05:45,340
different movies or different products or what have you.
不同电影或不同商品的重要的方面。

161
00:05:45,480 --> 00:05:47,120
What are the important aspects that cause
这些重要的方面将导致

162
00:05:47,610 --> 00:05:48,600
some users to like certain
一些用户喜欢某些

163
00:05:48,930 --> 00:05:49,830
movies and cause some users
电影，导致一些用户

164
00:05:50,210 --> 00:05:51,670
to like different sets of movies.
喜欢另外一些电影。

165
00:05:52,470 --> 00:05:53,380
So maybe you end up
可能你最终

166
00:05:53,540 --> 00:05:55,050
learning a feature, you know, where x1
学习一个特征，比如x1

167
00:05:55,260 --> 00:05:56,550
equals romance, x2 equals
代表浪漫爱情，x2代表

168
00:05:57,060 --> 00:05:59,180
action similar to
动作片

169
00:05:59,460 --> 00:06:00,590
an earlier video and maybe you
也许你

170
00:06:00,710 --> 00:06:02,100
learned a different feature x3 which
学到另一个不同的属性x3，

171
00:06:02,210 --> 00:06:04,520
is a degree to which this is a comedy.
描述电影的喜剧效果

172
00:06:05,330 --> 00:06:07,000
Then some feature x4 which is, you know, some other thing.
特征x4可能代表其他的特征。

173
00:06:07,270 --> 00:06:09,750
And you have N
这样你总共有N

174
00:06:09,940 --> 00:06:11,600
features all together and after
个特征，在你

175
00:06:12,610 --> 00:06:14,420
you have learned features it's actually often
学习完特征之后，实际上

176
00:06:14,750 --> 00:06:16,030
pretty difficult to go in
很难理解

177
00:06:16,420 --> 00:06:18,120
to the learned features and come
这些被学习到的特征

178
00:06:18,390 --> 00:06:19,980
up with a human understandable
并对这些特征给出人类可以理解的解释。

179
00:06:20,810 --> 00:06:22,850
interpretation of what these features really are.

180
00:06:22,950 --> 00:06:24,540
But in practice, you know, the
但是，实际上，

181
00:06:24,620 --> 00:06:27,480
features even though these features can be hard to visualize.
即使这些特征难以可视化，

182
00:06:28,100 --> 00:06:29,570
It can be hard to figure out just what these features are.
人们难以理解这些特征的含义，

183
00:06:31,070 --> 00:06:32,160
Usually, it will learn
但是，通常，算法将学到一些重要

184
00:06:32,410 --> 00:06:33,400
features that are very meaningful
特征，这些特征非常有意义，

185
00:06:33,960 --> 00:06:35,250
for capturing whatever are the
它们捕捉到一部电影的

186
00:06:35,870 --> 00:06:37,120
most important or the most salient
最重要的特征

187
00:06:37,880 --> 00:06:39,300
properties of a movie

188
00:06:39,710 --> 00:06:41,800
that causes you to like or dislike it.
这些特征导致了你喜欢或不喜欢这部电影。

189
00:06:41,860 --> 00:06:44,950
And so now let's say we want to address the following problem.
现在再看下一个问题：

190
00:06:45,970 --> 00:06:47,410
Say you have some specific movie
比如你有一部电影

191
00:06:47,790 --> 00:06:48,980
i and you want
i,你想要

192
00:06:49,120 --> 00:06:50,750
to find other movies j
找到另一部电影j

193
00:06:51,620 --> 00:06:52,680
that are related to that movie.
它与电影i相关。

194
00:06:53,150 --> 00:06:54,770
And so well, why would you want to do this?
为什么你要这样做呢？

195
00:06:54,920 --> 00:06:56,120
Right, maybe you have a
好的，假设你有一个用户

196
00:06:56,320 --> 00:06:57,840
user that's browsing movies, and they're
正在浏览电影，他们

197
00:06:58,360 --> 00:07:00,210
currently watching movie j, than
当前正在看电影j，

198
00:07:00,550 --> 00:07:01,820
what's a reasonable movie to recommend
那么在他们看完电影j后，

199
00:07:02,350 --> 00:07:04,110
to them to watch after they're done with movie j?
推荐给他们哪一部电影比较合理呢？

200
00:07:04,530 --> 00:07:06,040
Or if someone's recently purchased movie
或者如果有人最近买了电影j的碟片，

201
00:07:06,330 --> 00:07:07,470
j, well, what's a different

202
00:07:07,730 --> 00:07:11,000
movie that would be reasonable to recommend to them for them to consider purchasing.
那么向他们再推荐哪部电影更合理呢？

203
00:07:12,190 --> 00:07:13,000
So, now that you have
现在，既然你已经

204
00:07:13,080 --> 00:07:14,540
learned these feature vectors, this gives
学习到了这些特征向量，这给了

205
00:07:14,640 --> 00:07:16,080
us a very convenient way to
我们一种方便的方法去

206
00:07:16,250 --> 00:07:17,930
measure how similar two movies are.
衡量两个电影的相似度。

207
00:07:18,670 --> 00:07:20,530
In particular, movie i
举例说，电影i

208
00:07:21,460 --> 00:07:22,340
has a feature vector xi.
有一个特征向量xi,

209
00:07:23,290 --> 00:07:24,200
and so if you can find
如果你找到

210
00:07:24,640 --> 00:07:27,500
a different movie, j, so
一个另一个电影j,

211
00:07:27,710 --> 00:07:29,300
that the distance between
xi和xj的距离很小，

212
00:07:29,780 --> 00:07:30,800
xi and xj is small,

213
00:07:33,080 --> 00:07:34,010
then this is a pretty
那么，这就表明

214
00:07:34,430 --> 00:07:36,980
strong indication that, you know, movies

215
00:07:37,830 --> 00:07:41,360
j and i are somehow similar.
电影j和i相似，

216
00:07:42,320 --> 00:07:44,080
At least in the sense that some of them
至少从这个意义上说，

217
00:07:44,200 --> 00:07:46,950
likes movie i, maybe more likely to like movie j as well.
一些喜欢电影i的人也可能喜欢电影j.

218
00:07:47,760 --> 00:07:49,940
So, just to recap, if
现在，简要的回顾一下，如果

219
00:07:50,590 --> 00:07:52,130
your user is looking
你的用户正在看

220
00:07:52,510 --> 00:07:53,710
at some movie i and if
某个电影i，如果

221
00:07:54,150 --> 00:07:55,060
you want to find the 5
你想要找到5个

222
00:07:55,310 --> 00:07:56,640
most similar movies to that
与电影i最相似的电影

223
00:07:56,900 --> 00:07:57,860
movie in order to recommend
目的是推荐

224
00:07:58,230 --> 00:07:59,590
5 new movies to them, what
五部新电影给用户，你要做的是

225
00:07:59,690 --> 00:08:00,650
you do is find the five
找到5部电影j,

226
00:08:00,970 --> 00:08:02,260
movies j, with the

227
00:08:02,340 --> 00:08:03,880
smallest distance between the
这些电影的特征向量与电影i的特征向量

228
00:08:04,190 --> 00:08:05,680
features between these different movies.
有最小的距离。

229
00:08:06,550 --> 00:08:09,220
And this could give you a few different movies to recommend to your user.
这样你就能够向你的用户推荐几部不同的电影了。

230
00:08:10,010 --> 00:08:11,500
So with that, hopefully, you
希望通过以上的学习，你

231
00:08:11,680 --> 00:08:13,350
now know how to use
现在知道如何使用

232
00:08:13,700 --> 00:08:15,930
a vectorized implementation to compute
一个向量化的实现来计算

233
00:08:16,560 --> 00:08:18,130
all the predicted ratings of
所有用户对所有电影的

234
00:08:18,210 --> 00:08:20,280
all the users and all the
评分预测值。

235
00:08:20,390 --> 00:08:21,720
movies, and also how to do
也可以实现

236
00:08:21,920 --> 00:08:23,300
things like use learned features
利用已经学习到的特征

237
00:08:23,930 --> 00:08:25,360
to find what might be movies
来找到彼此

238
00:08:25,480 --> 00:08:27,490
and what might be products that aren't related to each other.
（不？）相关的电影或商品。

