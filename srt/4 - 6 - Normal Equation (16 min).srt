1
00:00:00,302 --> 00:00:01,883
In this video, we'll talk about
在这段视频中 我们要讲
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,883 --> 00:00:03,948
the normal equation, which for
标准方程法 (Normal Equation)

3
00:00:03,948 --> 00:00:05,660
some linear regression problems, will
对于某些线性回归问题

4
00:00:05,660 --> 00:00:06,981
give us a much better way
用标准方程法求解参数θ的最优值更好

5
00:00:06,981 --> 00:00:09,115
to solve for the optimal value
用标准方程法求解参数θ的最优值更好

6
00:00:09,115 --> 00:00:10,879
of the parameters theta.
用标准方程法求解参数θ的最优值更好

7
00:00:10,879 --> 00:00:13,096
Concretely, so far the
具体而言 到目前为止

8
00:00:13,096 --> 00:00:14,399
algorithm that we've been using
我们一直在使用的线性回归的算法

9
00:00:14,399 --> 00:00:16,042
for linear regression is gradient
是梯度下降法

10
00:00:16,042 --> 00:00:17,823
descent where in order
就是说 为了最小化代价函数 J (θ)

11
00:00:17,823 --> 00:00:19,410
to minimize the cost function
就是说 为了最小化代价函数 J (θ)

12
00:00:19,410 --> 00:00:21,354
J of Theta, we would take
我们使用的迭代算法

13
00:00:21,354 --> 00:00:23,792
this iterative algorithm that takes
需要经过很多步

14
00:00:23,792 --> 00:00:26,410
many steps, multiple iterations of
也就是说通过多次迭代来计算梯度下降

15
00:00:26,410 --> 00:00:28,259
gradient descent to converge
也就是说通过多次迭代来计算梯度下降

16
00:00:28,259 --> 00:00:30,396
to the global minimum.
来收敛到全局最小值

17
00:00:30,396 --> 00:00:32,563
In contrast, the normal equation
相反地

18
00:00:32,563 --> 00:00:34,413
would give us a method to
标准方程法提供了一种求θ的解析解法

19
00:00:34,413 --> 00:00:36,986
solve for theta analytically, so
标准方程法提供了一种求θ的解析解法

20
00:00:36,986 --> 00:00:38,761
that rather than needing to run
所以与其使用迭代算法

21
00:00:38,761 --> 00:00:40,594
this iterative algorithm, we can
我们可以直接一次性求解θ的最优值

22
00:00:40,594 --> 00:00:41,365
instead just solve for the
我们可以直接一次性求解θ的最优值

23
00:00:41,365 --> 00:00:42,791
optimal value for theta
我们可以直接一次性求解θ的最优值

24
00:00:42,791 --> 00:00:44,403
all at one go, so that in
所以说基本上

25
00:00:44,403 --> 00:00:46,096
basically one step you get
一步就可以得到优化值

26
00:00:46,096 --> 00:00:48,136
to the optimal value right there.
一步就可以得到优化值

27
00:00:49,136 --> 00:00:51,947
It turns out the normal equation
标准方程法有一些优点 也有一些缺点

28
00:00:52,209 --> 00:00:54,442
that has some advantages and
标准方程法有一些优点 也有一些缺点

29
00:00:54,442 --> 00:00:56,024
some disadvantages, but before
但是在我们讲解这个

30
00:00:56,024 --> 00:00:57,817
we get to that and talk about
和何时使用标准方程之前

31
00:00:57,903 --> 00:00:59,426
when you should use it, let's
让我们先对这个算法有一个直观的理解

32
00:00:59,426 --> 00:01:02,539
get some intuition about what this method does.
让我们先对这个算法有一个直观的理解

33
00:01:02,539 --> 00:01:04,633
For this week's planetary example, let's
我们举一个例子来解释这个问题

34
00:01:04,633 --> 00:01:06,120
imagine, let's take a
我们假设 有一个非常简单的代价函数J(θ)

35
00:01:06,120 --> 00:01:07,505
very simplified cost function
我们假设 有一个非常简单的代价函数J(θ)

36
00:01:07,505 --> 00:01:09,291
J of Theta, that's just the
它就是一个实数θ的函数

37
00:01:09,291 --> 00:01:11,958
function of a real number Theta.
它就是一个实数θ的函数

38
00:01:11,958 --> 00:01:13,642
So, for now, imagine that Theta
所以现在 假设θ只是一个标量

39
00:01:13,842 --> 00:01:16,615
is just a scalar value or that Theta is just a row value.
或者说θ只有一行

40
00:01:16,769 --> 00:01:18,918
It's just a number, rather than a vector.
它是一个数字 不是向量

41
00:01:19,171 --> 00:01:24,595
Imagine that we have a cost function J that's a quadratic function of this real value
假设我们的代价函数J 是这个实参数θ的二次函数

42
00:01:25,028 --> 00:01:27,420
parameter Theta, so J of Theta looks like that.
所以J (θ) 看起来是这样的

43
00:01:27,851 --> 00:01:30,336
Well, how do you minimize a quadratic function?
那么如何最小化一个二次函数呢?

44
00:01:30,720 --> 00:01:32,745
For those of you that know a little bit of calculus,
对于那些了解一点微积分的同学来说

45
00:01:32,858 --> 00:01:34,965
you may know that the way to
你可能知道

46
00:01:34,965 --> 00:01:36,628
minimize a function is to
最小化的一个函数的方法是

47
00:01:36,628 --> 00:01:38,991
take derivatives and to
对它求导 并且将导数置零

48
00:01:38,991 --> 00:01:41,707
set derivatives equal to zero.
对它求导 并且将导数置零

49
00:01:41,707 --> 00:01:44,721
So, you take the derivative of J with respect to the parameter of Theta.
所以对J求关于θ的导数

50
00:01:44,797 --> 00:01:46,847
You get some formula which I am not going to derive,
我不打算推导那些公式

51
00:01:46,847 --> 00:01:49,161
you set that derivative
你把那个导数置零

52
00:01:49,161 --> 00:01:50,782
equal to zero, and this
这样你就可以求得

53
00:01:50,782 --> 00:01:53,503
allows you to solve for
使得J(θ)最小的θ值

54
00:01:53,503 --> 00:01:57,866
the value of Theda that minimizes J of Theta.
使得J(θ)最小的θ值

55
00:01:57,866 --> 00:01:59,096
That was a simpler case
这是数据为实数的

56
00:01:59,096 --> 00:02:01,716
of when data was just real number.
一个比较简单的例子

57
00:02:01,716 --> 00:02:04,272
In the problem that we are interested in, Theta is
在这个问题中 我们感兴趣的是

58
00:02:04,929 --> 00:02:06,559
no longer just a real number,
θ不是一个实数的情况

59
00:02:06,559 --> 00:02:07,847
but, instead, is this
它是一个n+1维的参数向量

60
00:02:07,847 --> 00:02:11,986
n+1-dimensional parameter vector, and,
它是一个n+1维的参数向量

61
00:02:11,986 --> 00:02:13,809
a cost function J is
并且 代价函数J是这个向量的函数

62
00:02:13,809 --> 00:02:15,742
a function of this vector
并且 代价函数J是这个向量的函数

63
00:02:15,742 --> 00:02:17,501
value or Theta 0 through
也就是θ0到θm的函数

64
00:02:17,501 --> 00:02:18,924
Theta m. And, a cost
一个代价函数看起来是这样

65
00:02:18,924 --> 00:02:21,957
function looks like this, some square cost function on the right.
像右边的这个平方代价函数

66
00:02:22,373 --> 00:02:25,712
How do we minimize this cost function J?
我们如何最小化这个代价函数J?

67
00:02:25,712 --> 00:02:27,163
Calculus actually tells us
实际上 微积分告诉我们一种方法

68
00:02:27,163 --> 00:02:29,377
that, if you, that
实际上 微积分告诉我们一种方法

69
00:02:29,377 --> 00:02:30,709
one way to do so, is
对每个参数θ求J的偏导数

70
00:02:30,709 --> 00:02:38,604
to take the partial derivative of J, with respect to every parameter of Theta J in turn, and then, to set
对每个参数θ求J的偏导数

71
00:02:38,604 --> 00:02:40,271
all of these to 0.
然后把它们全部置零

72
00:02:40,271 --> 00:02:41,394
If you do that, and you
如果你这样做

73
00:02:41,394 --> 00:02:42,718
solve for the values of
并且求出θ0 θ1 一直到θn的值

74
00:02:42,718 --> 00:02:44,000
Theta 0, Theta 1,
并且求出θ0 θ1 一直到θn的值

75
00:02:44,000 --> 00:02:45,973
up to Theta N, then,
并且求出θ0 θ1 一直到θn的值

76
00:02:45,973 --> 00:02:47,217
this would give you that values
这样就能得到能够最小化代价函数J的θ值

77
00:02:47,217 --> 00:02:48,765
of Theta to minimize the cost
这样就能得到能够最小化代价函数J的θ值

78
00:02:48,765 --> 00:02:50,878
function J.  Where, if
这样就能得到能够最小化代价函数J的θ值

79
00:02:50,878 --> 00:02:52,176
you actually work through the
如果你真的做完微积分和求解参数θ0到θn

80
00:02:52,176 --> 00:02:53,597
calculus and work through
如果你真的做完微积分和求解参数θ0到θn

81
00:02:53,597 --> 00:02:55,194
the solution to the parameters
如果你真的做完微积分和求解参数θ0到θn

82
00:02:55,194 --> 00:02:57,316
Theta 0 through Theta N, the
如果你真的做完微积分和求解参数θ0到θn

83
00:02:57,316 --> 00:03:00,520
derivation ends up being somewhat involved.
这个偏微分最终可能很复杂

84
00:03:00,520 --> 00:03:01,625
And, what I am going
接下来我在视频中要做的

85
00:03:01,625 --> 00:03:03,113
to do in this video,
接下来我在视频中要做的

86
00:03:03,113 --> 00:03:04,852
is actually to not go
实际上不是遍历所有的偏微分

87
00:03:04,852 --> 00:03:06,297
through the derivation, which is kind
实际上不是遍历所有的偏微分

88
00:03:06,297 --> 00:03:07,657
of long and kind of involved, but
因为这样太久太费事

89
00:03:07,657 --> 00:03:08,962
what I want to do is just
我只是想告诉你们

90
00:03:08,962 --> 00:03:10,545
tell you what you need to know
你们想要实现这个过程所需要知道内容

91
00:03:10,545 --> 00:03:12,619
in order to implement this process
你们想要实现这个过程所需要知道内容

92
00:03:12,619 --> 00:03:14,138
so you can solve for the
这样你就可以解出

93
00:03:14,138 --> 00:03:15,511
values of the thetas that
偏导数为0时θ的值

94
00:03:15,511 --> 00:03:16,892
corresponds to where the
偏导数为0时θ的值

95
00:03:16,892 --> 00:03:19,273
partial derivatives is equal to zero.
偏导数为0时θ的值

96
00:03:19,273 --> 00:03:21,733
Or alternatively, or equivalently,
换个方式说 或者等价地

97
00:03:21,733 --> 00:03:23,357
the values of Theta is that
这个θ能够使得代价函数J(θ)最小化

98
00:03:23,357 --> 00:03:25,901
minimize the cost function J of Theta.
这个θ能够使得代价函数J(θ)最小化

99
00:03:25,901 --> 00:03:27,283
I realize that some of
我发现可能只有熟悉微积分的同学

100
00:03:27,283 --> 00:03:28,846
the comments I made that made
我发现可能只有熟悉微积分的同学

101
00:03:28,846 --> 00:03:29,914
more sense only to those
比较容易理解我的话

102
00:03:29,914 --> 00:03:31,896
of you that are normally familiar with calculus.
比较容易理解我的话

103
00:03:31,896 --> 00:03:33,065
So, but if you don't
所以 如果你不了解

104
00:03:33,065 --> 00:03:34,487
know, if you're less familiar
或者不那么了解微积分

105
00:03:34,487 --> 00:03:36,354
with calculus, don't worry about it.
也不必担心

106
00:03:36,354 --> 00:03:37,404
I'm just going to tell you what
我会告诉你

107
00:03:37,404 --> 00:03:38,374
you need to know in order to
要实现这个算法并且使其正常运行

108
00:03:38,374 --> 00:03:41,358
implement this algorithm and get it to work.
你所需的必要知识

109
00:03:41,358 --> 00:03:42,585
For the example that I
举个例子

110
00:03:42,585 --> 00:03:43,737
want to use as a running
我想运行这样一个例子

111
00:03:43,737 --> 00:03:46,339
example let's say that
假如说我有m=4个训练样本

112
00:03:46,339 --> 00:03:49,056
I have m = 4 training examples.
假如说我有m=4个训练样本

113
00:03:50,409 --> 00:03:52,881
In order to implement this normal
为了实现标准方程法

114
00:03:52,881 --> 00:03:56,515
equation at big, what I'm going to do is the following.
我要这样做

115
00:03:56,515 --> 00:03:57,640
I'm going to take my
看我的训练集

116
00:03:57,640 --> 00:04:00,375
data set, so here are my four training examples.
在这里就是这四个训练样本

117
00:04:00,375 --> 00:04:01,844
In this case let's assume that,
在这种情况下 我们假设

118
00:04:01,844 --> 00:04:06,073
you know, these four examples is all the data I have.
这四个训练样本就是我的所有数据

119
00:04:06,073 --> 00:04:07,890
What I am going to do is take
我所要做的是

120
00:04:07,890 --> 00:04:09,007
my data set and add
在我的训练集中加上一列对应额外特征变量的x0

121
00:04:09,007 --> 00:04:11,289
an extra column that corresponds
在我的训练集中加上一列对应额外特征变量的x0

122
00:04:11,289 --> 00:04:14,579
to my extra feature, x0,
在我的训练集中加上一列对应额外特征变量的x0

123
00:04:14,579 --> 00:04:15,967
that is always takes
就是那个取值永远是1的

124
00:04:15,967 --> 00:04:17,527
on this value of 1.
就是那个取值永远是1的

125
00:04:17,527 --> 00:04:18,681
What I'm going to do is
接下来我要做的是

126
00:04:18,681 --> 00:04:19,943
I'm then going to construct
构建一个矩阵X

127
00:04:19,943 --> 00:04:22,638
a matrix called X that's
这个矩阵基本包含了训练样本的所有特征变量

128
00:04:22,638 --> 00:04:24,632
a matrix are basically contains all
这个矩阵基本包含了训练样本的所有特征变量

129
00:04:24,632 --> 00:04:26,100
of the features from my
这个矩阵基本包含了训练样本的所有特征变量

130
00:04:26,100 --> 00:04:28,140
training data, so completely
所以具体地说

131
00:04:28,140 --> 00:04:31,528
here is my here are
这里有我所有的特征变量

132
00:04:31,528 --> 00:04:33,743
all my features and we're
这里有我所有的特征变量

133
00:04:33,743 --> 00:04:34,797
going to take all those numbers and
我们要把这些数字

134
00:04:34,797 --> 00:04:37,777
put them into this matrix "X", okay?
全部放到矩阵中X中 好吧？

135
00:04:37,777 --> 00:04:39,179
So just, you know, copy
所以只是

136
00:04:39,179 --> 00:04:41,233
the data over one column
每次复制一列的数据

137
00:04:41,233 --> 00:04:45,962
at a time and then I am going to do something similar for y's.
我要对y做类似的事情

138
00:04:45,962 --> 00:04:47,087
I am going to take the
我要对我们将要预测的值

139
00:04:47,087 --> 00:04:47,952
values that I'm trying to
我要对我们将要预测的值

140
00:04:47,952 --> 00:04:49,360
predict and construct now
构建一个向量

141
00:04:49,360 --> 00:04:52,894
a vector, like so
像这样的

142
00:04:52,894 --> 00:04:55,440
and call that a vector y.
并且称之为向量y

143
00:04:55,440 --> 00:04:58,038
So X is going to be a
所以X会是一个M*(n+1)维矩阵

144
00:04:59,653 --> 00:05:05,688
m by (n+1) - dimensional matrix, and
所以X会是一个M*(n+1)维矩阵

145
00:05:05,688 --> 00:05:07,490
Y is going to be
y会是一个m维向量

146
00:05:07,490 --> 00:05:14,421
a m-dimensional vector
y会是一个m维向量

147
00:05:14,421 --> 00:05:16,624
where m is the number of training examples
其中m是训练样本数量

148
00:05:16,984 --> 00:05:18,688
and n is, n is
n是特征变量数

149
00:05:18,688 --> 00:05:20,713
a number of features, n+1, because of
n+1是因为我加的这个额外的特征变量x0

150
00:05:20,713 --> 00:05:24,825
this extra feature X0 that I had.
n+1是因为我加的这个额外的特征变量x0

151
00:05:24,825 --> 00:05:26,350
Finally if you take
最后 如??果你用矩阵X和向量y来计算这个

152
00:05:26,350 --> 00:05:27,489
your matrix X and you take
最后 如??果你用矩阵X和向量y来计算这个

153
00:05:27,489 --> 00:05:28,595
your vector Y, and if you
最后 如??果你用矩阵X和向量y来计算这个

154
00:05:28,595 --> 00:05:31,065
just compute this, and set
θ等于 X转置乘以X的逆 乘以X转置 乘以y

155
00:05:31,065 --> 00:05:32,419
theta to be equal to
θ等于 X转置乘以X的逆 乘以X转置 乘以y

156
00:05:32,419 --> 00:05:34,440
X transpose X inverse times
θ等于 X转置乘以X的逆 乘以X转置 乘以y

157
00:05:34,440 --> 00:05:36,516
X transpose Y, this would
θ等于 X转置乘以X的逆 乘以X转置 乘以y

158
00:05:36,516 --> 00:05:38,583
give you the value of theta
这样就得到能够使得代价函数最小化的θ

159
00:05:38,583 --> 00:05:42,559
that minimizes your cost function.
这样就得到能够使得代价函数最小化的θ

160
00:05:42,559 --> 00:05:43,436
There was a lot
幻灯片上的内容比较多

161
00:05:43,436 --> 00:05:44,416
that happened on the slides and
幻灯片上的内容比较多

162
00:05:44,416 --> 00:05:47,514
I work through it using one specific example of one dataset.
我讲解了这样一个数据组的一个例子

163
00:05:47,514 --> 00:05:49,241
Let me just write this
让我把这个写成更加通用的形式

164
00:05:49,333 --> 00:05:50,770
out in a slightly more general form
让我把这个写成更加通用的形式

165
00:05:50,955 --> 00:05:53,418
and then let me just, and later on in
在之后的视频中

166
00:05:53,621 --> 00:05:56,531
this video let me explain this equation a little bit more.
我会仔细介绍这个方程

167
00:05:57,581 --> 00:06:00,687
It is not yet entirely clear how to do this.
以防你不完全清楚要如何做

168
00:06:00,687 --> 00:06:02,129
In a general case, let us
在一般情况下

169
00:06:02,129 --> 00:06:04,124
say we have M training examples
假如我们有m个训练样本

170
00:06:04,124 --> 00:06:05,697
so X1, Y1 up to
x1 y1 直到 xn yn

171
00:06:05,697 --> 00:06:09,319
Xn, Yn and n features.
n个特征变量

172
00:06:09,319 --> 00:06:10,811
So, each of the training example
所以每一个训练样本

173
00:06:10,811 --> 00:06:12,926
x(i) may looks like a vector
xi 可能看起来像一个向量

174
00:06:12,926 --> 00:06:16,297
like this, that is a n+1 dimensional feature vector.
像这样一个n+1维特征向量

175
00:06:16,943 --> 00:06:18,350
The way I'm going to construct the
我要构建矩阵X的方法

176
00:06:18,350 --> 00:06:20,674
matrix "X", this is
我要构建矩阵X的方法

177
00:06:20,674 --> 00:06:24,827
also called the design matrix
也被称为设计矩阵

178
00:06:24,827 --> 00:06:26,712
is as follows.
如下所示

179
00:06:26,712 --> 00:06:28,640
Each training example gives
每个训练样本给出一个这样的特征向量

180
00:06:28,640 --> 00:06:30,549
me a feature vector like this.
每个训练样本给出一个这样的特征向量

181
00:06:30,549 --> 00:06:34,491
say, sort of n+1 dimensional vector.
也就是说 这样的n+1维向量

182
00:06:34,491 --> 00:06:36,190
The way I am going to construct my
我构建我的设计矩阵X的方法

183
00:06:36,359 --> 00:06:39,734
design matrix x is only construct the matrix like this.
就是构建这样的矩阵

184
00:06:39,734 --> 00:06:40,834
and what I'm going to
那么 我要做的就是

185
00:06:40,834 --> 00:06:42,109
do is take the first
取第一个训练样本

186
00:06:42,109 --> 00:06:43,711
training example, so that's
取第一个训练样本

187
00:06:43,711 --> 00:06:46,350
a vector, take its transpose
也就是一个向量 取它的转置

188
00:06:46,350 --> 00:06:48,692
so it ends up being this,
它最后是这样

189
00:06:48,692 --> 00:06:50,250
you know, long flat thing and
扁长的样子

190
00:06:50,250 --> 00:06:55,153
make x1 transpose the first row of my design matrix.
让x1转置作为我设计矩阵的第一行

191
00:06:55,153 --> 00:06:56,225
Then I am going to take my
然后我要把我的

192
00:06:56,225 --> 00:06:58,682
second training example, x2, take
第二个训练样本x2

193
00:06:58,682 --> 00:07:00,437
the transpose of that and
进行转置 让它作为X的第二行

194
00:07:00,437 --> 00:07:01,838
put that as the second row
进行转置 让它作为X的第二行

195
00:07:01,838 --> 00:07:04,068
of x and so on,
以此类推

196
00:07:04,068 --> 00:07:07,206
down until my last training example.
直到最后一个训练样本

197
00:07:07,206 --> 00:07:09,279
Take the transpose of that,
取它的转置作为矩阵X的最后一行

198
00:07:09,279 --> 00:07:10,850
and that's my last row of
取它的转置作为矩阵X的最后一行

199
00:07:10,850 --> 00:07:12,665
my matrix X. And, so,
取它的转置作为矩阵X的最后一行

200
00:07:12,665 --> 00:07:14,418
that makes my matrix X, an
这样矩阵X就是一个m*(n+1)维矩阵

201
00:07:14,418 --> 00:07:17,129
M by N +1
这样矩阵X就是一个m*(n+1)维矩阵

202
00:07:17,129 --> 00:07:19,836
dimensional matrix.
这样矩阵X就是一个m*(n+1)维矩阵

203
00:07:19,836 --> 00:07:21,953
As a concrete example, let's
举个具体的例子

204
00:07:21,953 --> 00:07:23,505
say I have only one
假如我只有一个特征变量

205
00:07:23,505 --> 00:07:24,670
feature, really, only one
就是说除了x0之外只有一个特征变量

206
00:07:24,670 --> 00:07:26,631
feature other than X zero,
就是说除了x0之外只有一个特征变量

207
00:07:26,631 --> 00:07:28,165
which is always equal to 1.
而x0始终为1

208
00:07:28,165 --> 00:07:30,376
So if my feature vectors
所以如果我的特征向量

209
00:07:30,376 --> 00:07:32,186
X-i are equal to this
xi等于1 也就是x0 和某个实际的特征变量

210
00:07:32,186 --> 00:07:33,878
1, which is X-0, then
xi等于1 也就是x0 和某个实际的特征变量

211
00:07:33,878 --> 00:07:35,912
some real feature, like maybe the
xi等于1 也就是x0 和某个实际的特征变量

212
00:07:35,912 --> 00:07:37,662
size of the house, then my
比如说房屋大小

213
00:07:37,662 --> 00:07:40,947
design matrix, X, would be equal to this.
那么我的设计矩阵X会是这样

214
00:07:40,947 --> 00:07:42,589
For the first row, I'm going
第一行 就是这个的转置

215
00:07:42,589 --> 00:07:46,071
to basically take this and take its transpose.
第一行 就是这个的转置

216
00:07:46,071 --> 00:07:51,644
So, I'm going to end up with 1, and then X-1-1.
所以最后得到1 然后x(1)1

217
00:07:51,644 --> 00:07:53,309
For the second row, we're going to end
对于第二行 我们得到1 然后x(1)2

218
00:07:53,309 --> 00:07:56,077
up with 1 and then
对于第二行 我们得到1 然后x(1)2

219
00:07:56,077 --> 00:07:58,046
X-1-2 and so
对于第二行 我们得到1 然后x(1)2

220
00:07:58,046 --> 00:07:59,046
on down to 1, and
这样直到1 然后x(1)m

221
00:07:59,046 --> 00:08:01,420
then X-1-M.
这样直到1 然后x(1)m

222
00:08:01,420 --> 00:08:03,084
And thus, this will be
这样 这就会是一个m*2维矩阵

223
00:08:03,084 --> 00:08:07,776
a m by 2-dimensional matrix.
这样 这就会是一个m*2维矩阵

224
00:08:07,776 --> 00:08:08,821
So, that's how to construct
所以 这就是如何构建矩阵X 和向量y

225
00:08:08,821 --> 00:08:11,251
the matrix X. And, the
所以 这就是如何构建矩阵X 和向量y

226
00:08:11,251 --> 00:08:13,886
vector Y--sometimes I might
有时我可能会在上面画一个箭头

227
00:08:13,886 --> 00:08:15,487
write an arrow on top to
有时我可能会在上面画一个箭头

228
00:08:15,487 --> 00:08:16,541
denote that it is a vector,
来表示这是一个向量

229
00:08:16,541 --> 00:08:19,871
but very often I'll just write this as Y, either way.
但很多时候 我就只写y 是一样的

230
00:08:19,871 --> 00:08:21,182
The vector Y is obtained by
向量y这样求得的

231
00:08:21,182 --> 00:08:23,275
taking all all the labels,
把所有标签

232
00:08:23,275 --> 00:08:25,098
all the correct prices of
所有训练集中正确的房子价格

233
00:08:25,098 --> 00:08:27,076
houses in my training set, and
所有训练集中正确的房子价格

234
00:08:27,076 --> 00:08:28,963
just stacking them up into
放在一起 得到一个m维向量y

235
00:08:28,963 --> 00:08:32,011
an M-dimensional vector, and
放在一起 得到一个m维向量y

236
00:08:32,011 --> 00:08:34,511
that's Y.  Finally, having
最后 构建完矩阵X和向量y

237
00:08:34,511 --> 00:08:36,724
constructed the matrix X
最后 构建完矩阵X和向量y

238
00:08:36,724 --> 00:08:38,184
and the vector Y, we then
我们就可以通过计算X转置乘以X的逆乘以X转置乘以y来得到θ

239
00:08:38,184 --> 00:08:40,887
just compute theta as X'(1/X)
我们就可以通过计算X转置乘以X的逆乘以X转置乘以y来得到θ

240
00:08:40,887 --> 00:08:47,243
x X'Y. I just
我们就可以通过计算X转置乘以X的逆乘以X转置乘以y来得到θ

241
00:08:47,243 --> 00:08:49,356
want to make
我现在就想确保你明白这个等式

242
00:08:49,356 --> 00:08:51,348
I just want to make sure that this equation makes sense to you
我现在就想确保你明白这个等式

243
00:08:51,348 --> 00:08:52,242
and that you know how to implement it.
并且知道如何实现它

244
00:08:52,242 --> 00:08:55,221
So, you know, concretely, what is this X'(1/X)?
所以具体来说 什么是X的转置乘以X的逆？

245
00:08:55,221 --> 00:08:57,903
Well, X'(1/X) is the
X的转置乘以X的逆是X转置乘以X的逆矩阵

246
00:08:57,903 --> 00:09:02,101
inverse of the matrix X'X.
X的转置乘以X的逆是X转置乘以X的逆矩阵

247
00:09:02,101 --> 00:09:04,498
Concretely, if you were
具体来说

248
00:09:04,498 --> 00:09:08,055
to say set A to
如果你令A等于X转置乘以X

249
00:09:08,055 --> 00:09:11,120
be equal to X' x
如果你令A等于X转置乘以X

250
00:09:11,120 --> 00:09:12,542
X, so X' is a
X的转置是一个矩阵

251
00:09:12,542 --> 00:09:14,063
matrix, X' x X
X的转置乘以X是另一个矩阵

252
00:09:14,063 --> 00:09:15,305
gives you another matrix, and we
X的转置乘以X是另一个矩阵

253
00:09:15,305 --> 00:09:17,560
call that matrix A. Then, you
我们把这个矩阵称为A

254
00:09:17,560 --> 00:09:19,968
know, X'(1/X) is just
那么 X转置乘以X的逆就是矩阵A的逆

255
00:09:19,968 --> 00:09:22,352
you take this matrix A and you invert it, right!
那么 X转置乘以X的逆就是矩阵A的逆

256
00:09:23,245 --> 00:09:24,417
This gives, let's say 1/A.
也就是1/A

257
00:09:26,025 --> 00:09:28,919
And so that's how you compute this thing.
这就是计算过程

258
00:09:28,919 --> 00:09:31,451
You compute X'X and then you compute its inverse.
先计算X转置乘以X 然后计算它的逆

259
00:09:31,451 --> 00:09:34,296
We haven't yet talked about Octave.
我们还没有谈到Octave

260
00:09:34,296 --> 00:09:35,941
We'll do so in the later
我们将在之后的视频中谈到这个

261
00:09:35,941 --> 00:09:37,211
set of videos, but in the
但是在Octave编程语言

262
00:09:37,211 --> 00:09:39,073
Octave programming language or a
但是在Octave编程语言

263
00:09:39,073 --> 00:09:40,652
similar view, and also the
或者类似的MATLAB编程语言里是类似的

264
00:09:40,652 --> 00:09:42,957
matlab programming language is very similar.
或者类似的MATLAB编程语言里是类似的

265
00:09:42,957 --> 00:09:46,937
The command to compute this quantity,
计算这个量的命令

266
00:09:47,384 --> 00:09:50,326
X transpose X inverse times
X转置乘以X的逆乘以X转置乘以y

267
00:09:50,326 --> 00:09:52,537
X transpose Y, is as follows.
的代码命令如下所示

268
00:09:52,537 --> 00:09:54,903
In Octave X prime is
在Octave中 X’表示X转置

269
00:09:54,903 --> 00:09:58,354
the notation that you use to denote X transpose.
在Octave中 X’表示X转置

270
00:09:58,354 --> 00:10:00,737
And so, this expression that's
这个用红色框起来的表达式

271
00:10:00,737 --> 00:10:03,588
boxed in red, that's computing
计算的是X转置乘以X

272
00:10:03,588 --> 00:10:06,633
X transpose times X.
计算的是X转置乘以X

273
00:10:06,633 --> 00:10:08,551
pinv is a function for
pinv是用来计算逆矩阵的函数

274
00:10:08,551 --> 00:10:09,701
computing the inverse of
pinv是用来计算逆矩阵的函数

275
00:10:09,701 --> 00:10:11,818
a matrix, so this computes
所以这个计算X转置乘以X的逆

276
00:10:11,818 --> 00:10:14,656
X transpose X inverse,
所以这个计算X转置乘以X的逆

277
00:10:14,656 --> 00:10:16,453
and then you multiply that by
然后乘以X转置 再乘以y

278
00:10:16,453 --> 00:10:18,267
X transpose, and you multiply
然后乘以X转置 再乘以y

279
00:10:18,267 --> 00:10:19,712
that by Y. So you
然后乘以X转置 再乘以y

280
00:10:19,712 --> 00:10:22,325
end computing that formula
这样就算完了这个式子

281
00:10:22,325 --> 00:10:24,369
which I didn't prove,
我没有证明这个式子

282
00:10:24,369 --> 00:10:25,994
but it is possible to
尽管我并不打算这么做

283
00:10:25,994 --> 00:10:27,382
show mathematically even though I'm
但是数学上是可以证明的

284
00:10:27,382 --> 00:10:28,537
not going to do so
这个式子会给出最优的θ值

285
00:10:28,537 --> 00:10:31,071
here, that this formula gives you
这个式子会给出最优的θ值

286
00:10:31,071 --> 00:10:32,316
the optimal value of theta
这个式子会给出最优的θ值

287
00:10:32,316 --> 00:10:34,865
in the sense that if you set theta equal
就是说如果你令θ等于这个

288
00:10:34,865 --> 00:10:36,512
to this, that's the value
就是说如果你令θ等于这个

289
00:10:36,512 --> 00:10:38,000
of theta that minimizes the
这个θ值会最小化这个线性回归的代价函数J(θ)

290
00:10:38,000 --> 00:10:40,169
cost function J of theta
这个θ值会最小化这个线性回归的代价函数J(θ)

291
00:10:40,169 --> 00:10:41,993
for the new regression.
这个θ值会最小化这个线性回归的代价函数J(θ)

292
00:10:41,993 --> 00:10:44,530
One last detail in the earlier video.
最后一点

293
00:10:44,530 --> 00:10:46,131
I talked about the feature
在之前视频中我提到特征变量归一化

294
00:10:46,131 --> 00:10:47,061
skill and the idea of
在之前视频中我提到特征变量归一化

295
00:10:47,061 --> 00:10:48,878
getting features to be
和让特征变量在相似的范围内的想法

296
00:10:48,878 --> 00:10:50,726
on similar ranges of
和让特征变量在相似的范围内的想法

297
00:10:50,726 --> 00:10:54,900
Scales of similar ranges of values of each other.
将所有的值归一化在类似范围内

298
00:10:54,900 --> 00:10:56,872
If you are using this normal
如果你使用标准方程发

299
00:10:56,872 --> 00:10:59,843
equation method then feature
那么就不需要归一化特征变量

300
00:10:59,843 --> 00:11:02,315
scaling isn't actually necessary
那么就不需要归一化特征变量

301
00:11:02,315 --> 00:11:04,361
and is actually okay if,
实际上这是没问题的

302
00:11:04,361 --> 00:11:06,094
say, some feature X one
如果某个特征变量x1在0到1的区间

303
00:11:06,094 --> 00:11:07,552
is between zero and one,
如果某个特征变量x1在0到1的区间

304
00:11:07,552 --> 00:11:08,846
and some feature X two is
某个特征变量x2在0到1000的区间

305
00:11:08,846 --> 00:11:10,550
between ranges from zero to
某个特征变量x2在0到1000的区间

306
00:11:10,550 --> 00:11:12,019
one thousand and some feature
某个特征变量x2在0到1000的区间

307
00:11:12,019 --> 00:11:14,159
x three ranges from zero
某个特征变量x3在0到10^-5的区间

308
00:11:14,159 --> 00:11:15,822
to ten to the
某个特征变量x3在0到10^-5的区间

309
00:11:15,822 --> 00:11:17,263
minus five and if
某个特征变量x3在0到10^-5的区间

310
00:11:17,263 --> 00:11:18,321
you are using the normal equation method
然后如果使用标准方程法

311
00:11:18,321 --> 00:11:20,296
this is okay and there is
这样就没有问题

312
00:11:20,296 --> 00:11:21,550
no need to do features
不需要做特征变量归一化

313
00:11:21,550 --> 00:11:22,740
scaling, although of course
尽管如果你使用梯度下降法

314
00:11:22,740 --> 00:11:25,667
if you are using gradient descent,
尽管如果你使用梯度下降法

315
00:11:25,667 --> 00:11:27,814
then, features scaling is still important.
特征变量归一化仍然很重要

316
00:11:28,030 --> 00:11:31,020
Finally, where should you use the gradient descent
最后 你何时应该使用梯度下降法

317
00:11:31,020 --> 00:11:33,273
and when should you use the normal equation method.
而何时应该使用标准方程法

318
00:11:33,273 --> 00:11:35,800
Here are some of the their advantages and disadvantages.
这里列举了一些它们的优点和缺点

319
00:11:35,800 --> 00:11:38,305
Let's say you have m training
假如你有m个训练样本和n个特征变量

320
00:11:38,305 --> 00:11:40,918
examples and n features.
假如你有m个训练样本和n个特征变量

321
00:11:40,918 --> 00:11:42,854
One disadvantage of gradient descent
梯度下降法的缺点之一就是

322
00:11:42,854 --> 00:11:46,015
is that, you need to choose the learning rate Alpha.
你需要选择学习速率α

323
00:11:46,015 --> 00:11:47,374
And, often, this means running
这通常表示需要运行多次 尝试不同的学习速率α

324
00:11:47,374 --> 00:11:49,128
it few times with different learning
这通常表示需要运行多次 尝试不同的学习速率α

325
00:11:49,128 --> 00:11:51,154
rate alphas and then seeing what works best.
然后找到运行效果最好的那个

326
00:11:51,154 --> 00:11:54,274
And so that is sort of extra work and extra hassle.
所以这是一种额外的工作和麻烦

327
00:11:54,274 --> 00:11:55,976
Another disadvantage with gradient descent
梯度下降法的另一个缺点是

328
00:11:55,976 --> 00:11:57,841
is it needs many more iterations.
它需要更多次的迭代

329
00:11:57,841 --> 00:11:59,346
So, depending on the details,
因为一些细节 计算可能会更慢

330
00:11:59,346 --> 00:12:00,839
that could make it slower, although
因为一些细节 计算可能会更慢

331
00:12:00,839 --> 00:12:04,391
there's more to the story as we'll see in a second.
我们一会儿会看到更多的东西

332
00:12:04,391 --> 00:12:07,544
As for the normal equation, you don't need to choose any learning rate alpha.
至于标准方程 你不需要选择学习速率α

333
00:12:07,821 --> 00:12:11,208
So that, you know, makes it really convenient, makes it simple to implement.
所以就非常方便 也容易实现

334
00:12:11,208 --> 00:12:13,888
You just run it and it usually just works.
你只要运行一下 通常这就够了

335
00:12:13,888 --> 00:12:15,061
And you don't need to
并且你也不需要迭代

336
00:12:15,061 --> 00:12:16,129
iterate, so, you don't need
所以不需要画出J(θ)的曲线

337
00:12:16,129 --> 00:12:17,456
to plot J of Theta or
所以不需要画出J(θ)的曲线

338
00:12:17,456 --> 00:12:20,497
check the convergence or take all those extra steps.
来检查收敛性或者采取所有的额外步骤

339
00:12:20,497 --> 00:12:21,931
So far, the balance seems to
到目前为止

340
00:12:21,931 --> 00:12:23,846
favor normal the normal equation.
天平似乎倾向于标准方程法

341
00:12:24,826 --> 00:12:27,085
Here are some disadvantages of
这里列举一些标准方程法的缺点

342
00:12:27,612 --> 00:12:29,435
the normal equation, and some advantages of gradient descent.
和梯度下降法的优点

343
00:12:29,681 --> 00:12:31,447
Gradient descent works pretty well,
梯度下降法在有很多特征变量的情况下也能运行地相当好

344
00:12:31,928 --> 00:12:34,698
even when you have a very large number of features.
梯度下降法在有很多特征变量的情况下也能运行地相当好

345
00:12:34,698 --> 00:12:36,168
So, even if you
所以即使你有上百万的特征变量

346
00:12:36,168 --> 00:12:37,812
have millions of features you
所以即使你有上百万的特征变量

347
00:12:37,812 --> 00:12:40,865
can run gradient descent and it will be reasonably efficient.
你可以运行梯度下降法 并且通常很有效

348
00:12:40,865 --> 00:12:43,381
It will do something reasonable.
它会正常的运行

349
00:12:43,381 --> 00:12:46,566
In contrast to normal equation, In, in
相对地 标准方程法

350
00:12:46,566 --> 00:12:48,014
order to solve for the parameters
为了求解参数θ 需要求解这一项

351
00:12:48,014 --> 00:12:50,394
data, we need to solve for this term.
为了求解参数θ 需要求解这一项

352
00:12:50,394 --> 00:12:53,058
We need to compute this term, X transpose, X inverse.
我们需要计算这项X转置乘以X的逆

353
00:12:53,058 --> 00:12:56,328
This matrix X transpose X.
这个X转置乘以X矩阵是一个n*n的矩阵

354
00:12:56,328 --> 00:13:00,206
That's an n by n matrix, if you have n features.
如果你有n个特征变量的话

355
00:13:00,770 --> 00:13:02,947
Because, if you look
因为如果你看一下X转置乘以X的维度

356
00:13:02,947 --> 00:13:03,917
at the dimensions of
因为如果你看一下X转置乘以X的维度

357
00:13:03,917 --> 00:13:05,529
X transpose the dimension of
因为如果你看一下X转置乘以X的维度

358
00:13:05,529 --> 00:13:07,024
X, you multiply, figure out what
你可以发现他们的积的维度

359
00:13:07,024 --> 00:13:08,749
the dimension of the product
你可以发现他们的积的维度

360
00:13:08,749 --> 00:13:10,983
is, the matrix X transpose
X转置乘以X是一个n*n的矩阵

361
00:13:10,983 --> 00:13:13,727
X is an n by n matrix where
X转置乘以X是一个n*n的矩阵

362
00:13:13,727 --> 00:13:15,853
n is the number of features, and
其中 n是特征变量的数量

363
00:13:15,853 --> 00:13:18,641
for almost computed implementations
实现逆矩阵计算所需要的计算量

364
00:13:18,641 --> 00:13:20,990
the cost of inverting
实现逆矩阵计算所需要的计算量

365
00:13:20,990 --> 00:13:23,087
the matrix, rose roughly as
大致是矩阵维度的三次方

366
00:13:23,087 --> 00:13:25,707
the cube of the dimension of the matrix.
大致是矩阵维度的三次方

367
00:13:25,707 --> 00:13:28,180
So, computing this inverse costs,
因此计算这个逆矩阵需要计算大致n的三次方

368
00:13:28,180 --> 00:13:29,964
roughly order, and cube time.
因此计算这个逆矩阵需要计算大致n的三次方

369
00:13:29,964 --> 00:13:31,213
Sometimes, it's slightly faster than
有时稍微比计算n的三次方快一些

370
00:13:31,213 --> 00:13:35,050
N cube but, it's, you know, close enough for our purposes.
但是对我们来说很接近

371
00:13:35,489 --> 00:13:36,605
So if n the number of features is very large,
所以如果特征变量的数量n很大的话

372
00:13:37,643 --> 00:13:39,025
then computing this
那么计算这个量会很慢

373
00:13:39,025 --> 00:13:40,570
quantity can be slow and
那么计算这个量会很慢

374
00:13:40,570 --> 00:13:44,289
the normal equation method can actually be much slower.
实际上标准方程法会慢很多

375
00:13:44,289 --> 00:13:45,491
So if n is
因此如果n很大

376
00:13:45,491 --> 00:13:47,622
large then I might
因此如果n很大

377
00:13:47,622 --> 00:13:49,490
usually use gradient descent because
我可能还是会使用梯度下降法

378
00:13:49,490 --> 00:13:51,872
we don't want to pay this all in q time.
因为我们不想花费n的三次方的时间

379
00:13:51,872 --> 00:13:53,525
But, if n is relatively small,
但如果n比较小

380
00:13:53,525 --> 00:13:57,395
then the normal equation might give you a better way to solve the parameters.
那么标准方程法可能更好地求解参数θ

381
00:13:57,395 --> 00:13:59,080
What does small and large mean?
那么怎么叫大或者小呢？

382
00:13:59,080 --> 00:14:00,741
Well, if n is on
那么 如果n是上百的

383
00:14:00,741 --> 00:14:02,130
the order of a hundred, then
那么 如果n是上百的

384
00:14:02,130 --> 00:14:03,822
inverting a hundred-by-hundred matrix is
计算百位数乘百位数的矩阵

385
00:14:03,822 --> 00:14:06,539
no problem by modern computing standards.
对于现代计算机来说没有问题

386
00:14:06,539 --> 00:14:10,966
If n is a thousand, I would still use the normal equation method.
如果n是上千的 我还会使用标准方程法

387
00:14:10,966 --> 00:14:12,583
Inverting a thousand-by-thousand matrix is
千位数乘千位数的矩阵做逆变换

388
00:14:12,583 --> 00:14:15,408
actually really fast on a modern computer.
对于现代计算机来说实际上是非常快的

389
00:14:15,408 --> 00:14:18,406
If n is ten thousand, then I might start to wonder.
但如果n上万 那么我可能会开始犹豫

390
00:14:18,406 --> 00:14:20,618
Inverting a ten-thousand-  by-ten-thousand matrix
上万乘上万维的矩阵作逆变换

391
00:14:20,618 --> 00:14:22,208
starts to get kind of slow,
会开始有点慢

392
00:14:22,208 --> 00:14:23,471
and I might then start to
此时我可能开始倾向于

393
00:14:23,471 --> 00:14:25,007
maybe lean in the
此时我可能开始倾向于

394
00:14:25,007 --> 00:14:27,007
direction of gradient descent, but maybe not quite.
梯度下降法 但也不绝对

395
00:14:27,114 --> 00:14:28,672
n equals ten thousand, you can
n等于一万 你可以

396
00:14:28,672 --> 00:14:31,148
sort of convert a ten-thousand-by-ten-thousand matrix.
逆变换一个一万乘一万的矩阵

397
00:14:31,148 --> 00:14:34,345
But if it gets much bigger than that, then, I would probably use gradient descent.
但如果n远大于此 我可能就会使用梯度下降法了

398
00:14:34,345 --> 00:14:35,834
So, if n equals ten
所以如果n等于10^6

399
00:14:35,834 --> 00:14:36,920
to the sixth with a million
有一百万个特征变量

400
00:14:36,920 --> 00:14:38,963
features, then inverting a
那么做百万乘百万的矩阵的逆变换

401
00:14:38,963 --> 00:14:41,565
million-by-million matrix is going
那么做百万乘百万的矩阵的逆变换

402
00:14:41,565 --> 00:14:42,631
to be very expensive, and
就会变得非常费时间

403
00:14:42,631 --> 00:14:46,163
I would definitely favor gradient descent if you have that many features.
在这种情况下我一定会使用梯度下降法

404
00:14:46,163 --> 00:14:47,859
So exactly how large
所以很难给出一个确定的值

405
00:14:47,859 --> 00:14:49,282
set of features has to be
来决定何时该换成梯度下降法

406
00:14:49,282 --> 00:14:52,655
before you convert a gradient descent, it's hard to give a strict number.
来决定何时该换成梯度下降法

407
00:14:52,655 --> 00:14:53,855
But, for me, it is usually
但是 对我来说通常是

408
00:14:53,855 --> 00:14:55,501
around ten thousand that I might
在一万左右 我会开始考虑换成梯度下降法

409
00:14:55,501 --> 00:14:58,258
start to consider switching over
在一万左右 我会开始考虑换成梯度下降法

410
00:14:58,335 --> 00:15:00,663
to gradient descents or maybe,
在一万左右 我会开始考虑换成梯度下降法

411
00:15:00,663 --> 00:15:04,324
some other algorithms that we'll talk about later in this class.
或者我们将在以后讨论到的其他算法

412
00:15:04,324 --> 00:15:05,765
To summarize, so long
总结一下

413
00:15:05,765 --> 00:15:06,999
as the number of features is
只要特征变量的数目并不大

414
00:15:06,999 --> 00:15:08,475
not too large, the normal equation
标准方程是一个很好的

415
00:15:08,475 --> 00:15:12,229
gives us a great alternative method to solve for the parameter theta.
计算参数θ的替代方法

416
00:15:12,583 --> 00:15:13,983
Concretely, so long as
具体地说 只要特征变量数量小于一万

417
00:15:13,983 --> 00:15:15,749
the number of features is less
具体地说 只要特征变量数量小于一万

418
00:15:15,749 --> 00:15:17,472
than 1000, you know, I would
我通常使用标准方程法

419
00:15:17,472 --> 00:15:18,881
use, I would usually is used
我通常使用标准方程法

420
00:15:18,881 --> 00:15:21,955
in normal equation method rather than, gradient descent.
而不使用梯度下降法

421
00:15:21,955 --> 00:15:23,549
To preview some ideas that
预告一下在之后的课程中我们要讲的

422
00:15:23,549 --> 00:15:24,493
we'll talk about later in this
预告一下在之后的课程中我们要讲的

423
00:15:24,493 --> 00:15:26,235
course, as we get
随着我们要讲的学习算法越来越复杂

424
00:15:26,235 --> 00:15:27,912
to the more complex learning algorithm, for
随着我们要讲的学习算法越来越复杂

425
00:15:27,912 --> 00:15:29,617
example, when we talk about
例如 当我们讲到分类算法

426
00:15:29,617 --> 00:15:32,188
classification algorithm, like a logistic regression algorithm,
像逻辑回归算法

427
00:15:32,834 --> 00:15:34,319
We'll see that those algorithm
我们会看到

428
00:15:34,319 --> 00:15:35,467
actually...
 实际上对于那些算法

429
00:15:35,467 --> 00:15:37,592
The normal  equation method actually do not work
并不能使用标准方程法

430
00:15:37,592 --> 00:15:39,388
for those more sophisticated
对于那些更复杂的学习算法

431
00:15:39,388 --> 00:15:41,190
learning algorithms, and, we
我们将不得不仍然使用梯度下降法

432
00:15:41,190 --> 00:15:43,916
will have to resort to gradient descent for those algorithms.
我们将不得不仍然使用梯度下降法

433
00:15:43,916 --> 00:15:46,682
So, gradient descent is a very useful algorithm to know.
因此 梯度下降法是一个非常有用的算法

434
00:15:46,682 --> 00:15:48,859
The linear regression will have
可以用在有大量特征变量的线性回归问题

435
00:15:48,982 --> 00:15:50,017
a large number of features and
可以用在有大量特征变量的线性回归问题

436
00:15:50,017 --> 00:15:52,373
for some of the other algorithms
或者我们以后在课程中

437
00:15:52,373 --> 00:15:53,893
that we'll see in
会讲到的一些其他的算法

438
00:15:53,893 --> 00:15:55,438
this course, because, for them, the normal
因为 标准方程法不适合或者不能用在它们上

439
00:15:55,438 --> 00:15:58,747
equation method just doesn't apply and doesn't work.
因为 标准方程法不适合或者不能用在它们上

440
00:15:58,747 --> 00:16:00,537
But for this specific model of
但对于这个特定的线性回归模型

441
00:16:00,537 --> 00:16:02,904
linear regression, the normal equation
但对于这个特定的线性回归模型

442
00:16:02,904 --> 00:16:05,827
can give you a alternative
标准方程法是一个

443
00:16:07,219 --> 00:16:08,612
that can be much faster, than gradient descent.
比梯度下降法更快的替代算法

444
00:16:09,604 --> 00:16:11,920
So, depending on the detail of your algortithm,
所以 根据具体的问题

445
00:16:12,007 --> 00:16:14,164
depending of the detail of the problems and
所以 根据具体的问题

446
00:16:14,164 --> 00:16:15,550
how many features that you have,
以及你的特征变量的数量

447
00:16:15,550 --> 00:16:19,550
both of these algorithms are well worth knowing about.
这两算法都是值得学习的

