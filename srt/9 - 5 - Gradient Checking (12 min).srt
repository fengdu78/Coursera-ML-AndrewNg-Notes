1
00:00:00,290 --> 00:00:01,510
In the last few videos, we talked
在之前的视频中 我们讨论了

2
00:00:01,840 --> 00:00:02,770
about how to do forward-propagation
如何使用前向传播

3
00:00:03,570 --> 00:00:05,200
and back-propagation in a
和反向传播

4
00:00:05,250 --> 00:00:07,560
neural network in order to compute derivatives.
计算神经网络中的导数

5
00:00:08,800 --> 00:00:10,070
But back prop as an algorithm
但反向传播作为一个

6
00:00:10,580 --> 00:00:11,910
has a lot of details and,
有很多细节的算法

7
00:00:12,170 --> 00:00:12,920
you know, can be a little
在实现的时候

8
00:00:13,050 --> 00:00:14,930
bit tricky to implement.
会有点复杂

9
00:00:15,700 --> 00:00:17,480
And one unfortunate property is
而且有一个不好的方面是

10
00:00:17,750 --> 00:00:18,690
that there are many
在实现反向传播时

11
00:00:18,780 --> 00:00:20,080
ways to have subtle bugs in back
会遇到很多细小的错误

12
00:00:20,320 --> 00:00:22,000
prop so that if
所以如果你把它和梯度下降法

13
00:00:22,140 --> 00:00:23,130
you run it with gradient descent
或者其他优化算法一起运行时

14
00:00:23,480 --> 00:00:26,590
or some other optimization algorithm, it could actually look like it's working.
可能看起来它运行正常

15
00:00:27,240 --> 00:00:28,480
And, you know, your cost function J
并且 你的代价函数J

16
00:00:28,700 --> 00:00:29,930
of theta may end up
最后可能

17
00:00:30,090 --> 00:00:31,240
decreasing on every iteration
在每次梯度下降法迭代时

18
00:00:31,830 --> 00:00:33,660
of gradient descent, but this
都会减小

19
00:00:33,830 --> 00:00:35,180
could pull through even though
即使在实现反向传播时有一些小错误

20
00:00:35,440 --> 00:00:37,690
there might be some bug in your implementation of back prop.
可能也会检查不出来

21
00:00:38,400 --> 00:00:39,280
So it looks like J of
所以它看起来是

22
00:00:39,360 --> 00:00:40,830
theta is decreasing, but you
J(θ)在减小

23
00:00:40,920 --> 00:00:42,230
might just wind up with
但是可能你最后得到的神经网络

24
00:00:42,410 --> 00:00:43,760
a neural network that
但是可能你最后得到的

25
00:00:43,880 --> 00:00:44,970
has a higher level of error
神经网络误差

26
00:00:45,490 --> 00:00:46,540
than you would with a bug-free
比没有错误的要高

27
00:00:46,780 --> 00:00:48,130
implementation and you might
而且你很可能

28
00:00:48,330 --> 00:00:49,330
just not know that there
就是不知道

29
00:00:49,460 --> 00:00:50,470
was this subtle bug that's giving
你的结果是这些

30
00:00:50,530 --> 00:00:52,260
you this performance.
小错误导致的

31
00:00:52,950 --> 00:00:53,320
So what can we do about this?
那你应该怎么办呢

32
00:00:54,160 --> 00:00:55,940
There's an idea called gradient checking
有一个想法叫梯度检验 (Gradient Checking)

33
00:00:56,790 --> 00:00:58,720
that eliminates almost all of these problems.
可以解决基本所有的问题

34
00:00:59,250 --> 00:01:00,550
So today, every time I
我现在每次实现

35
00:01:00,770 --> 00:01:02,150
implement back propagation or a
神经网络的反向传播

36
00:01:02,370 --> 00:01:03,320
similar gradient descent algorithm on
或者类似的

37
00:01:03,450 --> 00:01:04,950
the neural network or any other
梯度下降算法

38
00:01:05,640 --> 00:01:07,310
reasonably complex model, I
或者其他比较复杂的模型

39
00:01:07,540 --> 00:01:08,840
always implement gradient checking.
我都会使用梯度检验

40
00:01:09,650 --> 00:01:10,610
And if you do this it will
如果你这么做

41
00:01:10,730 --> 00:01:12,010
help you make sure and
它会帮你确定

42
00:01:12,140 --> 00:01:13,410
sort of gain high confidence that
并且能很确信

43
00:01:13,540 --> 00:01:14,940
your implementation of forward prop
你实现的前向传播和反向传播

44
00:01:15,370 --> 00:01:17,430
and back prop or whatever, is 100% correct.
或者其他的什么 是100%正确的

45
00:01:18,240 --> 00:01:19,090
And in what I've seen
我见过很多

46
00:01:19,330 --> 00:01:20,880
this pretty much all the
这样解决那些

47
00:01:21,160 --> 00:01:23,090
problems associated with sort of
实现时容易有

48
00:01:23,420 --> 00:01:25,790
a buggy implementation of the background.
有小错误的问题

49
00:01:26,330 --> 00:01:27,470
And in the previous videos,
在之前的视频中

50
00:01:28,170 --> 00:01:29,120
I sort of ask you to take on
我一般是让你相信

51
00:01:29,390 --> 00:01:30,950
faith that the formulas I
我给出的计算

52
00:01:31,170 --> 00:01:33,000
gave for computing the deltas, and the
δ，d项

53
00:01:33,110 --> 00:01:34,220
D's, and so on, I ask
等等之类的公式

54
00:01:34,260 --> 00:01:35,480
you to take on faith that those
我要求你们相信

55
00:01:36,330 --> 00:01:37,600
actually do compute the gradients
他们计算的就是

56
00:01:38,180 --> 00:01:39,790
of the cost function, but once
代价函数的梯度

57
00:01:40,150 --> 00:01:41,740
you implement numerical gradient checking,
但一旦你们实现数值梯度检验

58
00:01:42,130 --> 00:01:43,210
which is the topic of this video,
也就是这节视频的主题

59
00:01:43,800 --> 00:01:45,250
you'll be able to verify for
你就能够自己验证

60
00:01:45,350 --> 00:01:46,490
yourself that the code you're
你写的代码

61
00:01:46,610 --> 00:01:48,530
writing is indeed computing
确实是在计算

62
00:01:49,600 --> 00:01:50,520
the derivative of the cost
代价函数J的导数

63
00:01:50,820 --> 00:01:53,060
function J.  So here's the idea.
想法是这样的

64
00:01:53,550 --> 00:01:54,520
Consider the following example.
考虑下面这个例子

65
00:01:55,450 --> 00:01:56,230
Suppose I have the function
假如我有一个

66
00:01:56,710 --> 00:01:58,140
J of theta, and I
函数J(θ)

67
00:01:58,250 --> 00:02:01,320
have some value, theta, and
并且我有个值 θ

68
00:02:01,610 --> 00:02:04,380
for this example, I'm going to assume that theta is just a real number.
在这个例子中 我假定θ只是一个实数

69
00:02:05,470 --> 00:02:08,210
And let's say I want to estimate the derivative of this function at this point.
假如说我想估计这个函数在这一点的导数

70
00:02:08,710 --> 00:02:10,220
And so the derivative is, you know, equal
这个导数等于

71
00:02:10,750 --> 00:02:13,190
to the slope of that sort of tangent line.
这条切线的斜率

72
00:02:14,270 --> 00:02:15,420
Here's how I'm going to numerically
下面我要用数值方法

73
00:02:16,180 --> 00:02:17,840
approximate the derivative, or
来计算近似的导数

74
00:02:17,970 --> 00:02:19,190
rather here's a procedure for numerically
这个是用数值方法

75
00:02:19,780 --> 00:02:21,480
approximating the derivative: I'm
计算近似导数的过程

76
00:02:21,800 --> 00:02:23,520
going to compute theta plus epsilon,
我要计算θ+ε

77
00:02:24,000 --> 00:02:25,550
so value a little bit to the right.
这个值在右边一点

78
00:02:26,340 --> 00:02:27,900
And we are going to compute theta minus epsilon.
然后计算θ-ε

79
00:02:28,410 --> 00:02:30,800
And I'm going to look
然后看这两个点

80
00:02:30,950 --> 00:02:34,360
at those two points and connect
用一条直线

81
00:02:34,840 --> 00:02:35,860
them by a straight line.
把它们连起来

82
00:02:43,160 --> 00:02:44,280
And I'm going to connect
我要把这两个点

83
00:02:44,480 --> 00:02:45,490
these two points by a straight
用一条直线连起来

84
00:02:45,680 --> 00:02:46,430
line and I'm going to
然后用这条

85
00:02:46,480 --> 00:02:47,740
use the slope of that
红色线的斜率

86
00:02:48,000 --> 00:02:49,200
little red line as my
来作为我

87
00:02:49,390 --> 00:02:50,940
approximation to the derivative,
导数的近似值

88
00:02:51,460 --> 00:02:53,110
which is the true derivative is
真正的导数是这边这条

89
00:02:53,280 --> 00:02:54,740
the slope of the blue line over there.
蓝色线的斜率

90
00:02:55,260 --> 00:02:56,660
So, you know, it seems like it would be a pretty good approximation.
这看起来是个不错的近似

91
00:02:58,220 --> 00:02:59,450
Mathematically, the slope of this
在数学上 这条红线的斜率等于

92
00:02:59,670 --> 00:03:01,340
red line is this vertical
这个垂直的高度

93
00:03:01,890 --> 00:03:03,680
height, divided by this
除以这个

94
00:03:03,890 --> 00:03:05,580
horizontal width, so this
这个水平的宽度

95
00:03:05,840 --> 00:03:07,500
point on top is J of
所以上面这点

96
00:03:08,920 --> 00:03:10,840
theta plus epsilon. This point
是J(θ+ε)

97
00:03:11,140 --> 00:03:13,020
here is J of theta minus epsilon.
这点是J(Θ-ε)

98
00:03:13,830 --> 00:03:15,450
So this vertical difference is j
垂直方向上的

99
00:03:15,670 --> 00:03:17,530
of theta plus epsilon, minus J
差是J(θ+ε)-J(θ+ε)

100
00:03:17,810 --> 00:03:18,810
of theta, minus epsilon, and
也就是说

101
00:03:19,700 --> 00:03:21,730
this horizontal distance is just 2 epsilon.
水平的距离就是2ε

102
00:03:23,620 --> 00:03:25,340
So, my approximation is going
那么 我的近似是这样的

103
00:03:25,410 --> 00:03:27,280
to be that the derivative,
J(θ)

104
00:03:29,110 --> 00:03:30,160
with respect to theta of J of
对θ的导数

105
00:03:30,490 --> 00:03:32,170
theta--add this value of
近似等于

106
00:03:32,320 --> 00:03:34,950
theta--that that's approximately J
J(θ+ε)-J(θ-ε)

107
00:03:35,150 --> 00:03:36,860
of theta plus epsilon, minus
除以2ε

108
00:03:37,460 --> 00:03:40,600
J of theta, minus epsilon, over 2 epsilon.
近似于J(θ+ε)-J(θ-ε) 除以2ε

109
00:03:42,280 --> 00:03:43,330
Usually, I use a pretty
通常

110
00:03:43,600 --> 00:03:44,790
small value for epsilon and
我给ε取很小的值

111
00:03:45,040 --> 00:03:46,270
set epsilon to be maybe
比如可能取

112
00:03:46,530 --> 00:03:48,220
on the order of 10 to the minus 4.
10的-4次方

113
00:03:48,740 --> 00:03:49,890
There's usually a large range
ε的取值在一个

114
00:03:50,190 --> 00:03:52,280
of different values for epsilon that work just fine.
很大范围内都是可行的

115
00:03:53,050 --> 00:03:54,470
And in fact, if you
实际上

116
00:03:55,280 --> 00:03:56,540
let epsilon become really small
如果你让ε非常小

117
00:03:57,010 --> 00:03:58,580
then, mathematically, this term here
那么 数学上

118
00:03:59,210 --> 00:04:00,790
actually, mathematically, you know,
这里这项实际上就是导数

119
00:04:01,000 --> 00:04:02,340
becomes the derivative, becomes exactly
就变成了函数

120
00:04:02,860 --> 00:04:04,310
the slope of the function at this point.
在这点上准确的斜率

121
00:04:05,050 --> 00:04:05,730
It's just that we don't want
只是我们不想用

122
00:04:05,910 --> 00:04:06,980
to use epsilon that's too, too
非常非常小的ε

123
00:04:07,170 --> 00:04:09,630
small because then you might run into numerical problems.
因为可能会产生数值问题

124
00:04:10,130 --> 00:04:11,070
So, you know, I usually use
所以我通常让ε

125
00:04:11,380 --> 00:04:14,200
epsilon around 10 to the minus 4, say.
差不多等于10^-4

126
00:04:14,470 --> 00:04:15,220
And by the way some of you may
顺便说一下 可能你们有些学习者

127
00:04:15,330 --> 00:04:17,590
have seen it alternative formula for
见过另外这种

128
00:04:17,750 --> 00:04:19,710
estimating the derivative which is this formula.
估计导数的公式

129
00:04:21,590 --> 00:04:23,500
This one on the right is called the one-sided difference.
右边这个叫做单侧拆分

130
00:04:24,040 --> 00:04:26,580
Whereas, the formula on the left that's called a two-sided difference.
左边这个公式叫做双侧差分

131
00:04:27,120 --> 00:04:28,670
The two-sided difference gives
双侧差分给我们了一个

132
00:04:28,890 --> 00:04:29,750
us a slightly more accurate estimate,
稍微精确些的估计

133
00:04:30,170 --> 00:04:31,410
so I usually use that rather
所以我通常用那个

134
00:04:31,670 --> 00:04:33,540
than just this one-sided difference estimate.
而不用这个单侧差分估计

135
00:04:35,900 --> 00:04:37,280
So, concretely, what you implement
具体地说 你在Octave中实现时

136
00:04:37,750 --> 00:04:39,280
in Octave is you implement the following.
要使用下面这个

137
00:04:40,270 --> 00:04:41,490
You implement call to compute, gradApprox
你的程序要调用

138
00:04:41,600 --> 00:04:43,160
which is going to
gradApprox来计算

139
00:04:43,270 --> 00:04:44,590
be approximation to zero relative
这个函数

140
00:04:45,380 --> 00:04:46,820
as just, you know, this formula: J of
会通过这个公式

141
00:04:47,200 --> 00:04:48,550
theta plus epsilon, minus J of theta,
J(θ+ε)-J(θ-ε)

142
00:04:48,730 --> 00:04:50,800
minus epsilon, divided by two times epsilon.
除以2ε

143
00:04:52,060 --> 00:04:52,980
And this will give you a
它会给出这点导数的

144
00:04:53,100 --> 00:04:56,110
numerical estimate of the gradient at that point.
数值估计

145
00:04:56,590 --> 00:04:58,910
And in this example it seems like it's a pretty good estimate.
在这个例子中 它看起来是个很好的估计

146
00:05:01,970 --> 00:05:03,460
Now, on the previous slide,
在之前的幻灯片中

147
00:05:03,710 --> 00:05:05,040
we consider the case of
我们考虑了

148
00:05:05,290 --> 00:05:07,010
when theta was a real number.
θ是一个实数的情况

149
00:05:08,000 --> 00:05:08,670
Now, let's look at the more
现在我们看更普遍的情况

150
00:05:08,900 --> 00:05:11,650
general case of where theta is a vector parameter.
θ是一个向量参数

151
00:05:12,220 --> 00:05:13,270
So let's say theta is an
假如说θ是n维向量

152
00:05:13,520 --> 00:05:14,610
Rn, and it might be unreal
它可能是我们的

153
00:05:15,000 --> 00:05:16,510
version of the parameters of
神经网络参数的

154
00:05:16,610 --> 00:05:18,010
our neural network. So
展开形式

155
00:05:18,250 --> 00:05:19,580
theta is a vector that
所以θ是一个有

156
00:05:19,800 --> 00:05:21,230
has n elements, theta 1
有n个元素的向量

157
00:05:21,350 --> 00:05:25,100
up to theta n. We
θ1到θn

158
00:05:25,240 --> 00:05:26,530
can then use a similar idea
我们可以用类似的想法

159
00:05:27,080 --> 00:05:29,300
to approximate all of the partial derivative terms.
来估计所有的偏导数项

160
00:05:30,250 --> 00:05:31,730
Concretely, the partial derivative
具体地说

161
00:05:32,420 --> 00:05:33,840
of a cost function with respect
代价函数对

162
00:05:34,110 --> 00:05:35,710
to the first parameter theta
第一个参数θ1取偏导数

163
00:05:36,110 --> 00:05:37,270
1, that can be
它可以用J

164
00:05:37,410 --> 00:05:40,270
obtained by taking J and increasing theta 1.
和增大的θ1得到

165
00:05:40,380 --> 00:05:43,030
So you have J of theta 1 plus epsilon, and so on
所以你有J(θ1+ε) 等等

166
00:05:43,520 --> 00:05:44,780
minus J of this theta
减去J(θ1-ε)

167
00:05:45,520 --> 00:05:46,820
1 minus epsilon and divide it by 2 epsilon.
然后除以2ε

168
00:05:48,130 --> 00:05:49,660
The partial derivative respect to
对第二个参数θ2

169
00:05:49,740 --> 00:05:51,090
the second parameter theta 2, is
取偏导数

170
00:05:51,620 --> 00:05:53,130
again this thing, except you're
还是这样

171
00:05:53,270 --> 00:05:54,370
taking J of, here you're
除了你要对

172
00:05:54,740 --> 00:05:56,240
increasing theta 2 by epsilon.
θ2+ε取J

173
00:05:56,570 --> 00:05:58,290
And here you're decreasing theta 2 by epsilon.
这里还有θ2-ε

174
00:05:59,100 --> 00:06:00,170
And so on down to the
这样计算后面的偏导数

175
00:06:00,260 --> 00:06:01,680
derivative with respect to
直到θn

176
00:06:01,780 --> 00:06:02,780
theta n. Would be if you
它的算法是

177
00:06:03,030 --> 00:06:04,550
increase and decrease theta n
对θn增加

178
00:06:05,060 --> 00:06:06,140
by epsilon over there.
和减少ε

179
00:06:09,790 --> 00:06:11,550
So, these equations give
这些公式

180
00:06:11,720 --> 00:06:13,580
you a way to numerically approximate
给出一个计算J

181
00:06:14,690 --> 00:06:16,500
the partial derivative of "J"
对任意参数求偏导数的

182
00:06:17,250 --> 00:06:20,100
with respect to any one of your parameters they derive.
数值近似的方法

183
00:06:23,640 --> 00:06:26,030
Concretely, what you implement is therefore, the following.
具体地说 你要实现的是下面这个

184
00:06:27,900 --> 00:06:29,260
We implement the following in Octave
我们把这个用在Octave里

185
00:06:29,820 --> 00:06:31,000
to numerically compute the derivatives.
来计算数值导数

186
00:06:32,220 --> 00:06:33,670
We say for i equals 1
假如 i

187
00:06:33,790 --> 00:06:35,110
through n where n is
等于 1 到 n

188
00:06:35,310 --> 00:06:37,140
the dimension of our parameter vector theta.
n是我们的参数向量θ的维度

189
00:06:37,730 --> 00:06:40,680
And I usually do this with the unrolled version of the parameters.
我通常用参数的展开形式来计算

190
00:06:41,250 --> 00:06:42,210
So you know theta is just
你知道θ只是我们

191
00:06:42,530 --> 00:06:44,770
a long list of all of my parameters in my neural networks.
神经网络模型的一长列参数

192
00:06:46,230 --> 00:06:47,550
I'm going to set theta plus equals
我让thetaPlus等于theta

193
00:06:47,830 --> 00:06:49,270
theta, then increase theta plus
然后给thetaPlus的第 i 项

194
00:06:49,630 --> 00:06:51,170
the ith element by epsilon.
加上EPSILON

195
00:06:51,660 --> 00:06:53,010
And so this is basically
这就是基本的

196
00:06:53,720 --> 00:06:54,830
theta plus is equal to theta
thetaPlus等于theta

197
00:06:55,340 --> 00:06:56,280
except for theta plus i,
除了thetaPlus(i)

198
00:06:56,580 --> 00:06:57,820
which is now incremented by epsilon.
它会增加EPSILON

199
00:06:58,310 --> 00:06:59,400
So if theta plus
所以如果thetaPlus

200
00:07:00,810 --> 00:07:01,880
is equal to, right, theta
等于θ1 θ2 等等

201
00:07:01,970 --> 00:07:03,370
1, theta 2, and so on and then theta
那么θi

202
00:07:04,020 --> 00:07:05,160
i has epsilon added to
增加了EPSILON

203
00:07:05,350 --> 00:07:06,590
it, and then it go down to
然后一直到θn

204
00:07:06,780 --> 00:07:08,440
theta n. So this is what theta plus is.
这就是thetaPlus的作用

205
00:07:08,690 --> 00:07:11,340
And similarly these two
类似的 这两行

206
00:07:11,530 --> 00:07:13,380
lines set theta minus to
给thetaMinus

207
00:07:13,480 --> 00:07:15,090
something similar except that
类似地赋值

208
00:07:15,560 --> 00:07:16,720
this, instead of theta i
只是θi不是加EPSILON

209
00:07:16,930 --> 00:07:19,150
plus epsilon, this now becomes theta i minus epsilon.
而是减EPSILON

210
00:07:20,670 --> 00:07:22,320
And then finally, you implement
最后 你运行这个

211
00:07:22,830 --> 00:07:24,370
this gradApprox i,
gradApprox(i)

212
00:07:25,190 --> 00:07:26,430
and this will give you
它会给你近似的

213
00:07:27,210 --> 00:07:28,420
your approximation to the partial
J(θ)对θi的

214
00:07:28,800 --> 00:07:30,250
derivative with respect to
偏导数

215
00:07:30,430 --> 00:07:32,430
theta i of J of theta.
我们实现

216
00:07:35,330 --> 00:07:36,420
And the way we use this
神经网络时

217
00:07:36,760 --> 00:07:38,530
in our neural network implementation is
是这样用的

218
00:07:38,850 --> 00:07:41,530
we would implement this, implement this
我们要实现这个

219
00:07:41,770 --> 00:07:43,310
FOR loop to compute, you know, the top partial
用for循环来计算

220
00:07:44,080 --> 00:07:45,570
derivative of the cost
代价函数对

221
00:07:45,860 --> 00:07:48,570
function with respect to every parameter in our network.
每个网络中的参数的偏导数

222
00:07:49,450 --> 00:07:51,120
And we can then take the
然后我们用从

223
00:07:51,350 --> 00:07:53,070
gradient that we got from back prop.
反向传播得到的梯度

224
00:07:53,740 --> 00:07:55,110
So DVec was the derivatives
DVec是我们从反向传播中

225
00:07:55,770 --> 00:07:57,150
we got from back prop.
得到的导数

226
00:07:58,380 --> 00:08:00,610
Right, so back prop, back-propagation was
所以后向传播是一个

227
00:08:00,890 --> 00:08:02,030
a relatively efficient way to
相对比较有效率的

228
00:08:02,090 --> 00:08:03,350
compute the derivatives or the
计算代价函数

229
00:08:03,430 --> 00:08:04,970
partial derivatives of a
对参数的导数

230
00:08:05,110 --> 00:08:06,850
cost function with respect to all of our parameters.
或偏导数的方法

231
00:08:07,820 --> 00:08:08,960
And what I usually do
接下来

232
00:08:09,350 --> 00:08:10,820
is then take my numerically
我通常做的是

233
00:08:11,440 --> 00:08:12,830
computed derivative, that is
计算数值导数

234
00:08:12,960 --> 00:08:14,080
this gradApprox that we
就是gradApprox

235
00:08:14,250 --> 00:08:15,830
just had from up here and
我们刚从上面这里得到的

236
00:08:15,920 --> 00:08:17,030
make sure that that is
来确定它等于

237
00:08:17,290 --> 00:08:19,420
equal or approximately equal
或者近似于

238
00:08:19,980 --> 00:08:21,080
up to, you know, small values
差距很小

239
00:08:21,810 --> 00:08:22,770
of numerical round off that is
非常接近我们

240
00:08:22,970 --> 00:08:25,640
pretty close to the DVec that I got from back prop.
从反向传播得到的DVec

241
00:08:26,510 --> 00:08:27,460
And if these two ways
如果这两种

242
00:08:27,930 --> 00:08:29,550
of computing the derivative give me
计算导数的方法

243
00:08:29,650 --> 00:08:31,040
the same answer or at least give me
给你相同的结果或者非常接近结果

244
00:08:31,300 --> 00:08:33,670
very similar answers, you know, up to a few decimal places.
最多几位小数的差距

245
00:08:34,720 --> 00:08:36,560
Then I'm much more confident that
那么我就非常确信

246
00:08:36,710 --> 00:08:38,720
my implementation of back prop is correct.
我实现的反向传播时正确的

247
00:08:40,000 --> 00:08:41,230
And when I plug these DVec
然后我把这些DVec向量用在

248
00:08:41,660 --> 00:08:43,320
vectors into gradient descent
梯度下降法或者

249
00:08:43,760 --> 00:08:45,610
or some advanced optimization algorithm,
其他高级优化算法里

250
00:08:45,760 --> 00:08:46,850
I can then be much
然后我就可以比较确信

251
00:08:47,100 --> 00:08:48,870
more confident that I'm computing
我计算的导数

252
00:08:49,360 --> 00:08:51,010
the derivatives correctly and therefore,
是正确的

253
00:08:51,450 --> 00:08:52,670
that hopefully my codes will
那么 我的代码

254
00:08:52,790 --> 00:08:53,890
run correctly and do a
应该也可以正确运行

255
00:08:53,980 --> 00:08:55,570
good job optimizing J of theta.
可以很好地优化J(θ)

256
00:08:57,700 --> 00:08:58,680
Finally, I want to put
最后 我想把

257
00:08:58,860 --> 00:09:00,050
everything together and tell you
所有的东西放在一起

258
00:09:00,310 --> 00:09:02,950
how to implement this numerical gradient checking.
然后告诉你怎么实现这个数值梯度检验

259
00:09:03,630 --> 00:09:04,370
Here's what I usually do.
这是我通常做的

260
00:09:04,970 --> 00:09:06,020
First thing I do, is implement
第一件事

261
00:09:06,500 --> 00:09:08,180
back-propagation to compute defects.
是实现反向传播来计算DVec

262
00:09:08,490 --> 00:09:09,560
So, this is a procedure we talked
这个步骤是我们

263
00:09:09,830 --> 00:09:11,250
about in an earlier video to
之前的视频中讲过的

264
00:09:11,490 --> 00:09:13,530
compute DVec which may be our unrolled version of these matrices.
计算DVec 它可能是这些矩阵的展开形式

265
00:09:15,410 --> 00:09:16,550
Then what I do, is implement
然后我要做的是

266
00:09:17,010 --> 00:09:20,130
a numerical gradient checking to compute gradApprox.
用gradApprox实现数值梯度检验

267
00:09:20,590 --> 00:09:23,550
So this is what I described earlier in this video, in the previous slide.
这是我在这节视频前面部分讲的 在之前的幻灯片里

268
00:09:24,900 --> 00:09:27,680
Then you should make sure that DVec and gradApprox
然后你要确定DVec和gradApprox给出接近的结果

269
00:09:28,170 --> 00:09:30,860
gives similar values, you know, let's say up to a few decimal places.
可能最多差几位小数

270
00:09:32,270 --> 00:09:33,160
And finally, and this
最后

271
00:09:33,240 --> 00:09:35,230
the important step, the more
这是最重要的一步

272
00:09:35,480 --> 00:09:36,690
you start to use your code
在使用你的代码去学习

273
00:09:37,000 --> 00:09:38,220
for learning, for seriously training
训练你的网络之前

274
00:09:38,570 --> 00:09:40,960
your network, it is important to turn off gradient checking.
重要的是要关掉梯度检验

275
00:09:41,490 --> 00:09:42,800
And to no longer compute
不再使用

276
00:09:43,630 --> 00:09:44,940
this gradApprox thing using
这节视频前面讲的

277
00:09:45,250 --> 00:09:47,660
the numerical derivative formulas that
这个数值导数公式

278
00:09:47,980 --> 00:09:48,950
we talked about earlier in this
来计算

279
00:09:50,560 --> 00:09:50,560
video.
gradApprox

280
00:09:50,960 --> 00:09:52,180
And the reason for that is the
这样做的原因是

281
00:09:52,330 --> 00:09:53,800
numeric code gradient checking code,
我们之前讲的这个

282
00:09:54,120 --> 00:09:54,930
the stuff we talked about in
数值梯度检验代码

283
00:09:55,010 --> 00:09:56,220
this video, that's a very
是一个计算量

284
00:09:56,650 --> 00:09:58,570
computationally expensive, that's a
非常大的程序

285
00:09:58,600 --> 00:10:00,960
very slow way to try to approximate the derivative.
它是一个非常慢的计算近似导数的方法

286
00:10:02,080 --> 00:10:03,490
Whereas in contrast, the back-propagation
而相对地

287
00:10:03,900 --> 00:10:04,710
algorithm that we talked about
我们之前讲的

288
00:10:04,940 --> 00:10:06,120
earlier, that is the
反向传播算法

289
00:10:06,370 --> 00:10:07,270
thing that we talked about earlier
也就是那个

290
00:10:07,460 --> 00:10:08,900
for computing, you know, D1, D2,
DVec的D(1) D(2) D(3)的算法

291
00:10:09,320 --> 00:10:11,620
D3, or for DVec. Back prop is
反向传播是一个在计算导数上

292
00:10:11,790 --> 00:10:14,930
a much more computationally efficient way of computing the derivatives.
效率更高的方法

293
00:10:17,070 --> 00:10:18,650
So once you've verified that
所以当你确认了

294
00:10:18,770 --> 00:10:20,270
your implementation of back-propagation is
你的反向传播算法是正确的

295
00:10:20,620 --> 00:10:21,840
correct, you should turn
你应该关掉梯度检验

296
00:10:22,160 --> 00:10:24,140
off gradient checking, and just stop using that.
就是不使用它

297
00:10:25,090 --> 00:10:26,380
So just to reiterate, you
再重申一下

298
00:10:26,540 --> 00:10:27,720
should be sure to disable your
在为了训练分类器

299
00:10:27,840 --> 00:10:29,380
gradient checking code before running
运行你的算法

300
00:10:29,690 --> 00:10:30,840
your algorithm for many
做很多次梯度下降

301
00:10:31,140 --> 00:10:32,560
iterations of gradient descent, or
或高级优化算法的迭代之前

302
00:10:32,670 --> 00:10:33,690
for many iterations of the
要确定你

303
00:10:33,890 --> 00:10:34,990
advanced optimization algorithms in
不再使用

304
00:10:35,820 --> 00:10:37,140
order to train your classifier.
梯度检验的程序

305
00:10:37,980 --> 00:10:39,120
Concretely, if you were
具体来说

306
00:10:39,290 --> 00:10:40,830
to run numerical gradient checking
如果你在每次的梯度下降法迭代时

307
00:10:41,340 --> 00:10:43,710
on every single integration of gradient
都运行数值梯度检验

308
00:10:44,040 --> 00:10:44,650
descent, or if you were in the
或者你用在

309
00:10:44,850 --> 00:10:45,780
inner loop of your cost function,
代价函数的内循环里

310
00:10:46,670 --> 00:10:47,910
then your code will be very slow.
你的程序会变得非常慢

311
00:10:48,240 --> 00:10:49,860
Because the numerical gradient checking
因为数值梯度检验程序

312
00:10:50,180 --> 00:10:51,690
code is much slower than
比反向传播算法

313
00:10:51,900 --> 00:10:53,960
the back-propagation algorithm, than
要慢很多

314
00:10:54,160 --> 00:10:56,160
a back-propagation method where you
反向传播算法

315
00:10:56,340 --> 00:10:57,650
remember we were computing delta
就是我们计算

316
00:10:58,000 --> 00:10:59,820
4, delta 3, delta 2, and so on.
δ(4) δ(3) δ(2) 等等的

317
00:10:59,900 --> 00:11:02,470
That was the back-propagation algorithm.
那就是反向传播算法

318
00:11:02,990 --> 00:11:05,770
That is a much faster way to compute derivatives than gradient checking.
那是一个比梯度检验更快的计算导数的方法

319
00:11:06,620 --> 00:11:08,400
So when you're ready, once
所以当你准备好了

320
00:11:08,620 --> 00:11:10,190
you verify the implementation of back-propagation
一旦你验证了

321
00:11:10,480 --> 00:11:12,140
is correct, make sure you
反向传播的实现是正确的

322
00:11:12,220 --> 00:11:13,050
turn off, or you disable
要确定你在训练算法时把它关闭了

323
00:11:13,640 --> 00:11:15,070
your gradient checking code while
或者说不再使用梯度检验程序

324
00:11:15,270 --> 00:11:17,880
you train your algorithm, or else your code could run very slowly.
否则你的程序会运行得非常慢

325
00:11:20,420 --> 00:11:22,470
So that's how you take gradients numerically.
所以如果你计算用数值方法计算导数

326
00:11:23,110 --> 00:11:24,300
And that's how you can verify that
那是你用来确定反向传播实现

327
00:11:24,420 --> 00:11:26,300
your implementation of back-propagation is correct.
是否正确的的方法

328
00:11:27,230 --> 00:11:29,290
Whenever I implement back-propagation or
当我实现反向传播

329
00:11:29,450 --> 00:11:31,130
similar gradient descent algorithm for
或者类似的复杂模型的梯度下降算法

330
00:11:31,250 --> 00:11:33,410
a complicated model, I always use gradient checking.
我经常使用梯度检验

331
00:11:33,730 --> 00:11:36,230
This really helps me make sure that my code is correct.
这的确能帮我确定我的代码是正确的

