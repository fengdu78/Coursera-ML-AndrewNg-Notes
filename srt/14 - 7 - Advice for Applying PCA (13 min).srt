1
00:00:00,090 --> 00:00:01,450
In an earlier video, I had
在早期的视频中(字幕翻译：中国海洋大学，仇志金)

2
00:00:01,610 --> 00:00:02,710
said that PCA can be
我曾经说过，PCA在某些情况下可以

3
00:00:02,840 --> 00:00:05,410
sometimes used to speed up the running time of a learning algorithm.
加快学习算法的执行效率

4
00:00:07,070 --> 00:00:08,140
In this video, I'd like
在该视频中

5
00:00:08,370 --> 00:00:09,520
to explain how to actually
我要去讲解实际情况下如何去做

6
00:00:09,820 --> 00:00:11,270
do that, and also say
也就是说

7
00:00:11,460 --> 00:00:12,900
some, just try to give
尝试着给一些建议

8
00:00:12,990 --> 00:00:14,550
some advice about how to apply PCA.
如何去应用PCA算法

9
00:00:17,110 --> 00:00:19,630
Here's how you can use PCA to speed up a learning algorithm,
下面讲解如何使用PCA算法来提高学习算法

10
00:00:20,270 --> 00:00:21,940
and this supervised learning algorithm
对监督学习算法

11
00:00:22,270 --> 00:00:23,630
speed up is actually the most
进行学习算法加速

12
00:00:23,870 --> 00:00:25,870
common use that I
我个人常常采用PCA算法

13
00:00:26,530 --> 00:00:27,720
personally make of PCA.
我个人常常采用PCA算法

14
00:00:28,710 --> 00:00:29,640
Let's say you have a supervised
比如说，你有一个

15
00:00:30,300 --> 00:00:31,660
learning problem, note this is
监督学习算法，值的注意的是

16
00:00:31,810 --> 00:00:33,380
a supervised learning problem with inputs
该监督学习算法

17
00:00:33,690 --> 00:00:35,510
X and labels Y, and
需要输入X和标签Y

18
00:00:35,810 --> 00:00:37,330
let's say that your examples
如果说，在你的例子中

19
00:00:37,830 --> 00:00:39,140
xi are very high dimensional.
xi是有非常高的维度

20
00:00:39,840 --> 00:00:41,670
So, lets say that your examples, xi are
比如说

21
00:00:41,800 --> 00:00:44,000
10,000 dimensional feature vectors.
xi是10,000维的特征向量

22
00:00:45,510 --> 00:00:46,550
One example of that, would
比如说

23
00:00:46,700 --> 00:00:48,130
be, if you were doing some computer
你要解决一些关于

24
00:00:48,540 --> 00:00:50,390
vision problem, where you have
可视化信息处理

25
00:00:50,650 --> 00:00:52,410
a 100x100 images, and so
你有一个100*100的图片

26
00:00:52,780 --> 00:00:55,550
if you have 100x100, that's 10000
如果是100*100，则就会有

27
00:00:55,850 --> 00:00:57,520
pixels, and so if xi are,
10000个像素点，所以xi

28
00:00:57,780 --> 00:00:59,240
you know, feature vectors
你知道，特征向量

29
00:00:59,760 --> 00:01:01,670
that contain your 10000 pixel
包含10,000个像素

30
00:01:02,470 --> 00:01:03,580
intensity values, then
亮度值，

31
00:01:04,410 --> 00:01:05,580
you have 10000 dimensional feature vectors.
所以你将要处理10,000维的特征矩阵

32
00:01:06,880 --> 00:01:08,530
So with very high-dimensional
对于像这种的高维

33
00:01:09,300 --> 00:01:10,890
feature vectors like this, running a
特征矩阵

34
00:01:11,320 --> 00:01:12,860
learning algorithm can be slow, right?
运行学习算法时将变的非常慢，不是吗?

35
00:01:13,030 --> 00:01:14,300
Just, if you feed 10,000 dimensional
如果你要使用10,000维的

36
00:01:14,790 --> 00:01:16,980
feature vectors into logistic regression,
特征矩阵进行逻辑回归

37
00:01:17,570 --> 00:01:19,780
or a new network, or support vector machine or what have you,
或一个新的网络，或支持向量机，或你想要的操作

38
00:01:20,450 --> 00:01:22,000
just because that's a lot of data,
因为数据量太大

39
00:01:22,200 --> 00:01:23,060
that's 10,000 numbers,
包含10,000成员

40
00:01:24,130 --> 00:01:25,970
it can make your learning algorithm run more slowly.
将会造成你的学习算法运行非常的慢

41
00:01:27,170 --> 00:01:28,530
Fortunately with PCA we'll be
幸运的时，PCA算法

42
00:01:28,680 --> 00:01:29,810
able to reduce the dimension of
可以减少要处理

43
00:01:30,060 --> 00:01:31,050
this data and so make
数据的维度

44
00:01:31,180 --> 00:01:32,410
our algorithms run more
从而使得算法更加高效

45
00:01:32,920 --> 00:01:34,440
efficiently. Here's how you
至于如何去做

46
00:01:34,590 --> 00:01:35,780
do that. We are going
我们首先

47
00:01:35,980 --> 00:01:37,030
first check our labeled
观察已经被标记的训练集

48
00:01:37,400 --> 00:01:39,520
training set and extract just
和只抽取输入的参数

49
00:01:39,800 --> 00:01:41,830
the inputs, we're just going to extract the X's
我们只抽取出X

50
00:01:42,730 --> 00:01:45,130
and temporarily put aside the Y's.
把Y先临时放一边

51
00:01:45,860 --> 00:01:46,750
So this will now give us
所以我们目前得到

52
00:01:47,090 --> 00:01:49,150
an unlabelled training set x1
一个无标签的训练集x1

53
00:01:49,400 --> 00:01:51,000
through xm which are maybe
到xm，这些数据集可能是

54
00:01:51,240 --> 00:01:53,600
there's a ten thousand dimensional data,
一万维的数据

55
00:01:53,940 --> 00:01:55,800
ten thousand dimensional examples we have.
在我们一万维的样本中

56
00:01:55,870 --> 00:01:57,230
So just extract the input vectors
仅仅抽取出了输入的向量

57
00:01:58,370 --> 00:01:58,930
x1 through xm.
x1 到 xm

58
00:02:00,650 --> 00:02:01,810
Then we're going to apply PCA
此时，我们继续使用PCA

59
00:02:02,700 --> 00:02:03,740
and this will give me a
将要使用一个

60
00:02:03,980 --> 00:02:06,100
reduced dimension representation of the
低维的数据来代表

61
00:02:06,410 --> 00:02:08,010
data, so instead of
取代

62
00:02:08,260 --> 00:02:09,540
10,000 dimensional feature vectors I now
10,000维的特征向量

63
00:02:09,780 --> 00:02:11,880
have maybe one thousand dimensional feature vectors.
可能我只需要一千维的特征向量

64
00:02:12,330 --> 00:02:13,500
So that's like a 10x savings.
这样就是一个10倍的储存量

65
00:02:15,110 --> 00:02:17,260
So this gives me, if you will, a new training set.
所以这样就给我们，如果你想要的话，一个新的训练集

66
00:02:17,910 --> 00:02:19,430
So whereas previously I might
之前我可能

67
00:02:19,620 --> 00:02:21,180
have had an example x1, y1,
有一个样本x1, y1

68
00:02:21,490 --> 00:02:24,340
my first training input, is now represented by z1.
我之前的训练集的输入，现在用z1来取代

69
00:02:24,580 --> 00:02:25,800
And so we'll have a
因此，我们将要

70
00:02:26,050 --> 00:02:27,010
new sort of training example,
使用新的方式对训练集进行表达

71
00:02:28,210 --> 00:02:29,240
which is Z1 paired with y1.
Z1 与 y1 进行对应

72
00:02:30,700 --> 00:02:33,170
And similarly Z2, Y2, and so on, up to ZM, YM.
相似的Z2, Y2等等，直到 ZM, YM

73
00:02:33,770 --> 00:02:35,300
Because my training examples are
因为我的训练样本是

74
00:02:35,460 --> 00:02:36,980
now represented with this much
新的方式表示

75
00:02:37,480 --> 00:02:41,040
lower dimensional representation Z1, Z2, up to ZM.
该方式是低维的，Z1, Z2, 到 ZM

76
00:02:41,310 --> 00:02:42,340
Finally, I can take this
最终，我可以使用

77
00:02:43,650 --> 00:02:45,060
reduced dimension training set and
这个低维的训练集

78
00:02:45,240 --> 00:02:46,540
feed it to a learning algorithm maybe
去使用神经网络算法

79
00:02:46,640 --> 00:02:47,900
a neural network, maybe logistic
或者是逻辑回归算法

80
00:02:48,280 --> 00:02:49,450
regression, and I can
我可以

81
00:02:49,750 --> 00:02:51,990
learn the hypothesis H, that
学习假设函数H

82
00:02:52,230 --> 00:02:53,830
takes this input, these low-dimensional
根据输入，使用低维

83
00:02:54,330 --> 00:02:56,230
representations Z and tries to make predictions.
的Z，尝试去进行预测

84
00:02:57,890 --> 00:02:59,030
So if I were using logistic
所以，如果我使用逻辑回归

85
00:02:59,460 --> 00:03:00,880
regression for example, I would
我将要

86
00:03:01,060 --> 00:03:02,760
train a hypothesis that outputs, you know,
训练一个获得输出的假设函数，你知道

87
00:03:03,080 --> 00:03:04,020
one over one plus E to
1加上一个E的

88
00:03:04,180 --> 00:03:06,020
the negative-theta transpose
负的theta的转置

89
00:03:07,620 --> 00:03:10,150
Z, that
Z，这样

90
00:03:10,610 --> 00:03:11,530
takes this input to one these
输入一个Z矩阵

91
00:03:11,960 --> 00:03:13,660
z vectors, and tries to make a prediction.
就可以尝试去进行预测

92
00:03:15,260 --> 00:03:16,310
And finally, if you have
最终，如果你

93
00:03:16,630 --> 00:03:17,800
a new example, maybe a new
有一个新的样本，可能是一个

94
00:03:17,920 --> 00:03:20,060
test example X. What
新的测试样本X，

95
00:03:20,220 --> 00:03:21,340
you do is you would
你要做的是

96
00:03:22,130 --> 00:03:23,730
take your test example x,
你将要使用测试样本的x

97
00:03:24,960 --> 00:03:26,590
map it through the same mapping
通过PCA的映射关系

98
00:03:26,990 --> 00:03:27,860
that was found by PCA
进行映射

99
00:03:28,220 --> 00:03:29,610
to get you your corresponding z.
获得对应的z

100
00:03:30,390 --> 00:03:31,280
And that z then gets
获得z后

101
00:03:31,950 --> 00:03:33,740
fed to this hypothesis, and this
把z带入到假设函数中

102
00:03:33,910 --> 00:03:35,450
hypothesis then makes a
这个假设函数

103
00:03:35,750 --> 00:03:36,740
prediction on your input x.
可以预测你输入的x

104
00:03:38,110 --> 00:03:40,090
One final note, what PCA does
最后，PCA如何

105
00:03:40,510 --> 00:03:42,350
is it defines a mapping from
定义一个映射

106
00:03:42,710 --> 00:03:45,090
x to z and
从x到z

107
00:03:45,960 --> 00:03:46,970
this mapping from x to
这个映射从x到

108
00:03:47,050 --> 00:03:48,280
z should be defined by running
z应该被定义

109
00:03:48,580 --> 00:03:50,840
PCA only on the training sets.
只在训练集上执行PCA

110
00:03:51,650 --> 00:03:53,310
And in particular, this mapping that
尤其特别的是，这个映射是

111
00:03:53,530 --> 00:03:54,770
PCA is learning, right, this
PCA的学习过程，没错

112
00:03:54,950 --> 00:03:57,650
mapping, what that does is it computes the set of parameters.
这个映射，是计算一系列的参数

113
00:03:58,210 --> 00:04:00,500
That's the feature scaling and mean normalization.
是特征的缩放和归一化过程

114
00:04:01,240 --> 00:04:04,040
And there's also computing this matrix U reduced.
这里经常计算矩阵Ureduced

115
00:04:04,680 --> 00:04:05,510
But all of these things that
但所有这些事情

116
00:04:05,670 --> 00:04:06,980
U reduce, that's like a
计算矩阵Ureduced，就像

117
00:04:07,120 --> 00:04:08,420
parameter that is learned
通过学习PCA得到的参数

118
00:04:08,670 --> 00:04:09,950
by PCA and we should
我们应该

119
00:04:10,150 --> 00:04:12,270
be fitting our parameters only to
拟合这些参数，仅仅在

120
00:04:12,480 --> 00:04:13,990
our training sets and not
训练集上

121
00:04:14,040 --> 00:04:16,250
to our cross validation or test sets and
而不是交叉验证或者在测试集上

122
00:04:16,370 --> 00:04:17,560
so these things the U reduced
求解Ureduced这些事情

123
00:04:18,180 --> 00:04:19,460
so on, that should be
就是说

124
00:04:19,820 --> 00:04:22,430
obtained by running PCA only on your training set.
只在训练集上运行，得到的PCA

125
00:04:23,300 --> 00:04:26,930
And then having found U reduced, or having found the parameters for feature
然后，找到Ureduced，或者找到特征参数

126
00:04:27,350 --> 00:04:28,620
scaling where the mean normalization
缩放，即归一化

127
00:04:29,320 --> 00:04:31,790
and scaling the scale
和缩放规模

128
00:04:32,180 --> 00:04:34,500
that you divide the features by to get them on to comparable scales.
使得你划分的特征，让他们具有可比性

129
00:04:34,760 --> 00:04:36,010
Having found all those parameters
找到所有的参数

130
00:04:36,570 --> 00:04:38,010
on the training set, you can
在训练集上

131
00:04:38,220 --> 00:04:41,560
then apply the same mapping to other examples that may be
于是，你可以应用这相同的映射到其他样本中

132
00:04:41,820 --> 00:04:45,020
In your cross-validation sets or
如在你交叉验证集

133
00:04:45,180 --> 00:04:46,680
in your test sets, OK?
或者你的测试集，OK？

134
00:04:47,150 --> 00:04:48,340
Just to summarize, when you're
总结一下，当你

135
00:04:48,450 --> 00:04:49,790
running PCA, run your
在运行PCA时，运行

136
00:04:49,900 --> 00:04:51,070
PCA only on the
PCA仅仅在

137
00:04:51,220 --> 00:04:52,450
training set portion of the
训练集部分的数据

138
00:04:52,490 --> 00:04:55,880
data not the cross-validation set or the test set portion of your data.
不可以是在交叉验证集和测试集数据部分

139
00:04:56,410 --> 00:04:57,620
And that defines the mapping from
定义了x到z的映射

140
00:04:57,870 --> 00:04:58,770
x to z and you can
你可以

141
00:04:58,950 --> 00:05:00,320
then apply that mapping to
应用这个映射到

142
00:05:00,560 --> 00:05:02,240
your cross-validation set and your
你的交叉验证集

143
00:05:02,290 --> 00:05:03,370
test set and by the
和你的测试集

144
00:05:03,450 --> 00:05:04,660
way in this example I talked
顺便说一下，在这个例子中，

145
00:05:05,000 --> 00:05:06,660
about reducing the data from
我讲了减少一万维的数据

146
00:05:06,950 --> 00:05:08,510
ten thousand dimensional to one
到

147
00:05:08,740 --> 00:05:10,350
thousand dimensional, this is actually
一千维的数据，实际上

148
00:05:10,660 --> 00:05:11,950
not that unrealistic. For many
这不是说不现实的

149
00:05:12,280 --> 00:05:14,720
problems we actually reduce the dimensional data. You
在许多问题上，我们需要去减少数据的维度

150
00:05:17,600 --> 00:05:18,700
know by 5x maybe by 10x
你可以减少5倍或者10倍

151
00:05:18,780 --> 00:05:20,910
and still retain most of the variance and we can do this
仍然保留较大的方差，我们这样做可以

152
00:05:21,270 --> 00:05:22,680
barely effecting the performance,
几乎不影响性能

153
00:05:23,900 --> 00:05:25,840
in terms of classification accuracy, let's say,
分类精度，比方说

154
00:05:26,240 --> 00:05:27,970
barely affecting the classification
几乎不影响分类

155
00:05:28,770 --> 00:05:30,320
accuracy of the learning algorithm.
算法的准确度

156
00:05:31,090 --> 00:05:32,140
And by working with lower dimensional
而且通过较低的维度

157
00:05:32,590 --> 00:05:33,730
data our learning algorithm
数据，我们的学习算法

158
00:05:34,060 --> 00:05:36,500
can often run much much faster.
通常运行的非常快

159
00:05:36,910 --> 00:05:38,120
To summarize, we've so far talked
总而言之,我们到目前为止

160
00:05:38,410 --> 00:05:40,920
about the following applications of PCA.
我们讨论下PCA应用

161
00:05:41,970 --> 00:05:43,780
First is the compression application where
首先，压缩应用

162
00:05:44,020 --> 00:05:45,140
we might do so to reduce
我们可能需要这样做，

163
00:05:45,500 --> 00:05:46,440
the memory or the disk space
来减少存储器或硬盘空间

164
00:05:46,590 --> 00:05:47,960
needed to store data and we
用来存储数据

165
00:05:48,240 --> 00:05:49,390
just talked about how to
我们仅仅讨论如何

166
00:05:49,460 --> 00:05:51,630
use this to speed up a learning algorithm.
使用该方法去加速学习算法

167
00:05:52,100 --> 00:05:53,870
In these applications, in order
在该应用中，

168
00:05:54,130 --> 00:05:56,240
to choose K, often we'll
需要去选择一个K，我们经常

169
00:05:56,420 --> 00:05:58,770
do so according to, figuring
这样做的依据是，计算出

170
00:05:59,160 --> 00:06:00,590
out what is the percentage of
方差保留的百分比

171
00:06:00,810 --> 00:06:03,880
variance retained, and so
通常

172
00:06:04,780 --> 00:06:06,320
for this learning algorithm, speed
这种学习算法

173
00:06:06,570 --> 00:06:10,050
up application often will retain 99%  of the variance.
加速应用，需要保留99%的方差

174
00:06:10,530 --> 00:06:11,690
That would be a very typical choice
这是一种非常典型的

175
00:06:12,100 --> 00:06:14,270
for how to choose k. So
如何去选择k的方式

176
00:06:14,730 --> 00:06:16,640
that's how you choose k for these compression applications.
所以对于压缩应用，就是你如何选择k

177
00:06:17,850 --> 00:06:19,590
Whereas for visualization applications
而对于可视化应用

178
00:06:20,760 --> 00:06:22,100
while usually we know
虽然我们通常知道

179
00:06:22,230 --> 00:06:23,550
how to plot only two dimensional
如何去标记二维数据

180
00:06:24,020 --> 00:06:25,520
data or three dimensional data,
或三维数据

181
00:06:26,540 --> 00:06:28,650
and so for visualization applications, we'll
对于可视化应用程序

182
00:06:28,830 --> 00:06:29,660
usually choose k equals 2
我们通常选择k等于2

183
00:06:29,710 --> 00:06:31,930
or k equals 3, because we can plot
或者k等于3，因为我们可以

184
00:06:32,740 --> 00:06:33,500
only 2D and 3D data sets.
标记2D和3D数据集

185
00:06:34,510 --> 00:06:35,720
So that summarizes the main
总结一下，

186
00:06:36,020 --> 00:06:37,230
applications of PCA, as well
应用PCA的主要是

187
00:06:37,870 --> 00:06:39,580
as how to choose the
如何去选择

188
00:06:39,670 --> 00:06:41,540
value of k for these different applications.
k值，对于不同的应用

189
00:06:42,890 --> 00:06:45,710
I should mention that
我必须提出

190
00:06:46,400 --> 00:06:48,100
there is often one frequent misuse of PCA and
PCA算法经常被滥用

191
00:06:48,800 --> 00:06:50,300
you sometimes hear about others
有时候你会听到

192
00:06:50,580 --> 00:06:51,820
doing this hopefully not too often.
别人希望这样去做，不是太频繁

193
00:06:52,230 --> 00:06:54,780
I just want to mention this so that you know not to do it.
我只是仅仅提一下，以至于你知道不要这样去做

194
00:06:55,480 --> 00:06:56,460
And there is one bad use of
这里有一个非常坏的PCA应用

195
00:06:56,540 --> 00:06:59,170
PCA, which iss to try to use it to prevent over-fitting.
iss曾经尝试使用PCA去防止过拟合

196
00:07:00,380 --> 00:07:00,660
Here's the reasoning.
这里的推理

197
00:07:01,910 --> 00:07:03,080
This is not a great
不是非常棒的

198
00:07:03,730 --> 00:07:04,610
way to use PCA,
使用PCA方式

199
00:07:04,670 --> 00:07:05,630
but here's the reasoning behind
但该方法背后的原因是

200
00:07:05,690 --> 00:07:07,080
this method, which is,you know
你知道

201
00:07:07,350 --> 00:07:09,090
if we have Xi, then
如果我们有Xi

202
00:07:09,300 --> 00:07:10,660
maybe we'll have n features, but
可能我们将要有n个特征

203
00:07:10,830 --> 00:07:12,640
if we compress the data, and
但是如果我们压缩这些数据

204
00:07:12,750 --> 00:07:13,700
use Zi instead
使用Zi替代

205
00:07:14,270 --> 00:07:15,410
and that reduces the number
减少

206
00:07:15,560 --> 00:07:17,050
of features to k, which
特征数量k

207
00:07:17,290 --> 00:07:19,300
could be much lower dimensional. And
可以得到很低的维度

208
00:07:19,410 --> 00:07:21,130
so if we have a much smaller
因此，如果你有一个非常少的

209
00:07:21,490 --> 00:07:22,520
number of features, if k
数量的特征

210
00:07:22,770 --> 00:07:25,800
is 1,000 and n is
比如k是1,000

211
00:07:26,090 --> 00:07:27,010
10,000, then if we have
n是10,000

212
00:07:27,780 --> 00:07:29,390
only 1,000 dimensional data, maybe
但是我们只有1,000维的数据

213
00:07:29,670 --> 00:07:30,580
we're less likely to over-fit
可能我们不太可能过拟合

214
00:07:31,260 --> 00:07:32,230
than if we were using 10,000-dimensional
相反，如果我们使用10,000维的数据

215
00:07:33,280 --> 00:07:34,980
data with like a thousand features.
1,000个特征

216
00:07:35,950 --> 00:07:37,160
So some people think
一些人就会认为

217
00:07:37,360 --> 00:07:39,360
of PCA as a way to prevent over-fitting.
PCA是一种防止过拟合的方法

218
00:07:39,950 --> 00:07:41,940
But just to emphasize this
但是，仅仅强调这一点

219
00:07:42,110 --> 00:07:44,000
is a bad application of PCA
是PCA的非常糟糕的应用

220
00:07:44,260 --> 00:07:46,080
and I do not recommend doing this.
我不建议去这样做

221
00:07:46,520 --> 00:07:48,430
And it's not that this method works badly.
这并不是说该方法工作不好

222
00:07:49,000 --> 00:07:49,920
If you want to use
如果你想去

223
00:07:50,330 --> 00:07:51,560
this method to reduce the dimensional
使用该方法去减少数据维度

224
00:07:51,890 --> 00:07:52,830
data, to try to prevent over-fitting,
防止过拟合

225
00:07:53,690 --> 00:07:54,830
it might actually work OK.
它可能效果也会很好

226
00:07:55,560 --> 00:07:56,720
But this just is not
但是，这仅仅

227
00:07:57,040 --> 00:07:58,340
a good way to address
不是一种非常好的方式

228
00:07:58,680 --> 00:08:00,390
over-fitting and instead, if you're
处理过拟合问题，相反，如果

229
00:08:00,510 --> 00:08:01,810
worried about over-fitting, there is
你担心过拟合

230
00:08:02,030 --> 00:08:03,420
a much better way to address
这有一种非常好的方式去防止过拟合发生

231
00:08:03,800 --> 00:08:05,680
it, to use regularization instead of
使用规则化来代替使用PCA

232
00:08:05,900 --> 00:08:07,910
using PCA to reduce the dimension of the data.
来减少数据维度

233
00:08:08,670 --> 00:08:10,000
And the reason is, if
理由是，

234
00:08:11,010 --> 00:08:12,150
you think about how PCA works,
你想一下，PCA是如何工作的

235
00:08:12,900 --> 00:08:13,950
it does not use the labels y.
它不需要使用标签y

236
00:08:14,530 --> 00:08:15,680
You are just looking
你仅仅是使用

237
00:08:16,050 --> 00:08:17,220
at your inputs xi, and you're
输入的xi

238
00:08:17,340 --> 00:08:19,070
using that to find a
你是使用它去寻找

239
00:08:19,130 --> 00:08:21,150
lower-dimensional approximation to your data.
低维对你的数据进行近似

240
00:08:21,390 --> 00:08:22,840
So what PCA does,
所以PCA

241
00:08:23,190 --> 00:08:25,410
is it throws away some information.
会舍掉一些信息

242
00:08:26,460 --> 00:08:28,040
It throws away or reduces the
它扔掉或减少

243
00:08:28,180 --> 00:08:29,680
dimension of your data without
数据的维度

244
00:08:30,110 --> 00:08:31,390
knowing what the values of y
不关心y值是什么

245
00:08:32,380 --> 00:08:33,700
is, so this is probably
所以这可能是较好的使用PCA

246
00:08:34,250 --> 00:08:35,770
okay using PCA this way
这种方式

247
00:08:35,920 --> 00:08:37,750
is probably okay if, say
可能是好的

248
00:08:37,990 --> 00:08:39,190
99 percent of the
99%的

249
00:08:39,410 --> 00:08:40,400
variance is retained, if you're keeping most
方差信息被保留，如果你保持

250
00:08:40,830 --> 00:08:41,970
of the variance, but
较多的方差

251
00:08:42,100 --> 00:08:44,230
it might also throw away some valuable information.
但是，它也可能会丢掉更多的有价值的信息

252
00:08:45,010 --> 00:08:45,980
And it turns out that
事实证明

253
00:08:46,310 --> 00:08:47,580
if you're retaining 99% of
如果你保留了99%的方差

254
00:08:47,820 --> 00:08:49,260
the variance or 95%
或者95%的方差

255
00:08:49,360 --> 00:08:50,940
of the variance or whatever, it
诸如此类

256
00:08:51,020 --> 00:08:52,310
turns out that just using
事实证明，只使用

257
00:08:52,720 --> 00:08:54,650
regularization will often give
规则化常常会给你带来

258
00:08:54,790 --> 00:08:56,010
you at least as good
至少一样好的效果

259
00:08:56,220 --> 00:08:57,880
a method for preventing over-fitting
来防止过拟合

260
00:08:58,900 --> 00:09:00,340
and regularization will often just
规则化

261
00:09:00,590 --> 00:09:02,220
work better, because when you
常常会工作的更好，因为

262
00:09:02,350 --> 00:09:03,890
are applying linear regression or logistic
当你是应用线性回归或者逻辑回归

263
00:09:04,250 --> 00:09:05,240
regression or some other method
或其它的一些方法

264
00:09:05,600 --> 00:09:07,390
with regularization, well, this minimization
进行规则化时，这个最小化问题

265
00:09:08,010 --> 00:09:09,420
problem actually knows what the
实际上是知道

266
00:09:09,480 --> 00:09:10,740
values of y are, and
y的值

267
00:09:10,960 --> 00:09:12,680
so is less likely to throw
所以不太可能

268
00:09:12,880 --> 00:09:14,330
away some valuable information, whereas
损失掉一些值的信息

269
00:09:14,730 --> 00:09:15,790
PCA doesn't make use
然而，PCA不使用

270
00:09:16,060 --> 00:09:17,810
of the labels and is more
标签

271
00:09:17,850 --> 00:09:19,940
likely to throw away valuable information.
更有可能丢失一些值的信息

272
00:09:20,230 --> 00:09:21,370
So, to summarize, it is
因此，总结一下

273
00:09:21,620 --> 00:09:22,900
a good use of PCA, if your
使用PCA比较好的方式

274
00:09:23,010 --> 00:09:24,380
main motivation to speed up
你关心

275
00:09:24,530 --> 00:09:26,490
your learning algorithm, but using
学习算法的速度，

276
00:09:26,790 --> 00:09:28,360
PCA to prevent over-fitting, that
但是使用PCA去防止过拟合

277
00:09:28,650 --> 00:09:29,630
is not a good use of
不是一种非常好的方式

278
00:09:30,030 --> 00:09:32,270
PCA, and using regularization instead
使用规则化来代替

279
00:09:32,900 --> 00:09:36,190
is really what many people
正确的，许多人

280
00:09:36,440 --> 00:09:40,490
would recommend doing instead. Finally,
建议这样取代，最后

281
00:09:41,310 --> 00:09:43,350
one last misuse of PCA.
减少对PCA的滥用

282
00:09:43,750 --> 00:09:45,760
And so I should say PCA is a very useful algorithm,
所以我应该说主成分分析是一种非常有用的算法,

283
00:09:46,270 --> 00:09:49,170
I often use it for the compression on the visualization purposes.
我经常使用它来达到压缩可视化文件的目的

284
00:09:50,230 --> 00:09:51,400
But, what I sometimes
但是，我有时候发现

285
00:09:51,570 --> 00:09:53,310
see, is also people sometimes
一些人在有时候

286
00:09:53,710 --> 00:09:56,080
use PCA where it shouldn't be.
在不需要使用PCA的时候使用PCA

287
00:09:56,220 --> 00:09:57,940
So, here's a pretty common thing that
因此，这有一个很常见的事情

288
00:09:58,030 --> 00:09:59,140
I see, which is if someone
我发现，

289
00:09:59,330 --> 00:10:00,330
is designing a machine-learning system,
如果一些人在设计机器学习系统时，

290
00:10:01,010 --> 00:10:02,130
they may write down the
他们可能写下

291
00:10:02,200 --> 00:10:04,150
plan like this: let's design a learning system.
这样一个计划：让我们设计一个学习系统

292
00:10:05,060 --> 00:10:06,080
Get a training set and then,
获得训练集

293
00:10:06,570 --> 00:10:07,350
you know, what I'm going to
你知道，我们将要

294
00:10:07,400 --> 00:10:08,700
do is run PCA, then train
执行PCA

295
00:10:08,860 --> 00:10:11,200
logistic regression and then test on my test data.
接下来，训练逻辑回归，然后测试我们的数据

296
00:10:11,680 --> 00:10:12,770
So often at the very
所以，经常

297
00:10:13,090 --> 00:10:14,360
start of a project,
在一个项目开始时

298
00:10:14,600 --> 00:10:15,600
someone will just write out a
一些人会写下

299
00:10:15,720 --> 00:10:16,980
project plan than says lets
一个工程计划

300
00:10:17,310 --> 00:10:18,610
do these four steps with PCA inside.
包括PCA在内的四步计划

301
00:10:20,210 --> 00:10:21,220
Before writing down a project
在写下项目计划之前

302
00:10:21,530 --> 00:10:23,350
plan the incorporates PCA like
结合PCA

303
00:10:23,560 --> 00:10:24,860
this, one very good
一个非常好的问题被提出

304
00:10:25,030 --> 00:10:27,110
question to ask is, well, what if we
如果我们

305
00:10:27,630 --> 00:10:28,560
were to just do the whole
如何直接去做

306
00:10:29,540 --> 00:10:31,470
without using PCA.
而不使用PCA

307
00:10:32,170 --> 00:10:33,450
And often people do not
通常在人们

308
00:10:33,800 --> 00:10:34,940
consider this step before
不考虑这一步

309
00:10:35,440 --> 00:10:37,080
coming up with a complicated project plan and
在提出一个复杂的项目计划

310
00:10:37,920 --> 00:10:40,620
implementing PCA and so on.
和实现PCA之前

311
00:10:40,810 --> 00:10:42,360
And sometime, and so specifically,
有些情况下，具体地说

312
00:10:43,050 --> 00:10:44,300
what I often advise people
我常常建议人们

313
00:10:44,670 --> 00:10:45,980
is, before you implement
在实现PCA之前

314
00:10:46,450 --> 00:10:47,970
PCA, I would first
我首先建议

315
00:10:48,220 --> 00:10:49,410
suggest that, you know, do
你知道

316
00:10:49,600 --> 00:10:50,770
whatever it is, take whatever it
无论做什么

317
00:10:50,850 --> 00:10:52,030
is you want to do and first
执行一切你想做的

318
00:10:52,450 --> 00:10:53,650
consider doing it with your
首先考虑

319
00:10:53,980 --> 00:10:56,420
original raw data xi, and
你最原始的数据xi

320
00:10:56,600 --> 00:10:57,860
only if that doesn't do
只有不去做你想要的

321
00:10:57,960 --> 00:10:59,650
what you want, then implement PCA before using Zi.
然后，在使用Zi之前，应用PCA

322
00:11:01,010 --> 00:11:02,420
So, before using PCA you know,
因此，使用PCA之前

323
00:11:03,030 --> 00:11:03,930
instead of reducing the dimension
代替减少数据维度

324
00:11:04,360 --> 00:11:05,710
of the data, I would consider
而应该好好考虑

325
00:11:06,640 --> 00:11:08,020
well, let's ditch this PCA step,
让我们抛弃PCA这一步

326
00:11:08,420 --> 00:11:09,690
and I would consider, let's
我需要考虑

327
00:11:10,040 --> 00:11:11,460
just train my learning algorithm
仅仅在我的原始数据上

328
00:11:12,440 --> 00:11:13,560
on my original data.
训练学习算法

329
00:11:14,410 --> 00:11:15,990
Let's just use my original raw
让我们仅仅使用我原始的

330
00:11:16,300 --> 00:11:17,770
inputs xi, and I would
输入数据xi

331
00:11:18,180 --> 00:11:19,550
recommend, instead of putting
我建议，取代

332
00:11:19,720 --> 00:11:20,910
PCA into the algorithm, just
把PCA放入到算法中

333
00:11:21,030 --> 00:11:23,250
try doing whatever it is you're doing with the xi first.
首先去尝试仅仅使用xi

334
00:11:24,090 --> 00:11:25,000
And only if you have
除非

335
00:11:25,150 --> 00:11:26,180
a reason to believe that doesn't
你认为存在一个原因导致不能工作

336
00:11:26,480 --> 00:11:27,590
work, so that only if your
比如

337
00:11:27,790 --> 00:11:29,470
learning algorithm ends up
你的学习算法结束

338
00:11:29,510 --> 00:11:31,100
running too slowly, or only if
运行太慢

339
00:11:31,280 --> 00:11:32,680
the memory requirement or the
或者内存

340
00:11:32,910 --> 00:11:34,140
disk space requirement is too large,
或者硬盘空间需要太大

341
00:11:34,430 --> 00:11:35,850
so you want to compress your
因此你需要去压缩

342
00:11:36,190 --> 00:11:37,810
representation, but if only
你的表示方法

343
00:11:38,000 --> 00:11:39,020
using the xi doesn't work,
但是如果仅仅使用xi不能正常工作

344
00:11:39,360 --> 00:11:40,640
only if you have evidence or strong
只有如果你有证据

345
00:11:40,950 --> 00:11:41,890
reason to believe that using
或强有力的原因认为

346
00:11:42,380 --> 00:11:43,890
the xi won't work, then implement
使用xi不能工作

347
00:11:44,380 --> 00:11:46,730
PCA and consider using the compressed representation.
此时使用PCA，来考虑进行压缩

348
00:11:47,990 --> 00:11:48,830
Because what I do see, is
因为我所看到的

349
00:11:49,100 --> 00:11:50,380
sometimes people start off with
有时候，一些人

350
00:11:50,530 --> 00:11:51,520
a project plan that incorporates PCA
开始做项目计划

351
00:11:52,100 --> 00:11:54,580
inside, and sometimes they,
总是把PCA包含在里面

352
00:11:54,650 --> 00:11:55,620
whatever they're
有时候，他们

353
00:11:55,820 --> 00:11:57,380
doing will work just
做的非常好

354
00:11:57,660 --> 00:11:59,520
fine, even with out using PCA instead.
即使没有使用PCA在里面

355
00:11:59,840 --> 00:12:01,650
So, just consider that
因此，仅仅考虑

356
00:12:01,860 --> 00:12:03,130
as an alternative as well, before you
作为一种替代选择

357
00:12:03,320 --> 00:12:04,170
go to spend a lot of
在你花大量时间

358
00:12:04,300 --> 00:12:05,570
time to get PCA in, figure
去获得PCA，

359
00:12:05,770 --> 00:12:08,100
out what k is and so on.
计算出k等之前

360
00:12:08,250 --> 00:12:09,330
So, that's it for PCA.
这就是主成份分析

361
00:12:09,800 --> 00:12:11,000
Despite these last sets of
尽管最近的结论说

362
00:12:11,080 --> 00:12:12,380
comments, PCA is an
PCA是

363
00:12:12,690 --> 00:12:14,060
incredibly useful algorithm, when you
非常有用的算法

364
00:12:14,150 --> 00:12:15,330
use it for the appropriate applications
当你在适当的应用程序使用它时

365
00:12:16,070 --> 00:12:17,480
and I've actually used PCA pretty
实际上，我经常使用PCA

366
00:12:17,770 --> 00:12:19,330
often and for me,
对我来说，非常频繁

367
00:12:19,580 --> 00:12:20,650
I use it mostly to speed
我主要使用PCA

368
00:12:20,850 --> 00:12:22,150
up the running time of my learning algorithms.
来提高我的学习算法的运行速度

369
00:12:22,880 --> 00:12:24,310
But I think, just as
但是，我认为

370
00:12:24,400 --> 00:12:25,690
common an application of PCA,
仅仅常见的应用可以使用PCA

371
00:12:26,020 --> 00:12:27,300
is to use it to
常常用在

372
00:12:27,410 --> 00:12:29,030
compress data, to reduce
压缩数据

373
00:12:29,620 --> 00:12:30,650
the memory or disk space
减少对内存和硬盘空间需求

374
00:12:30,990 --> 00:12:33,130
requirements, or to use it to visualize data.
或者用来处理图像数据

375
00:12:34,270 --> 00:12:35,710
And PCA is one of
PCA是非常

376
00:12:35,750 --> 00:12:36,960
the most commonly used and one
常用的方式之一

377
00:12:36,990 --> 00:12:39,420
of the most powerful unsupervised learning algorithms.
最强大的无监督算法之一

378
00:12:40,060 --> 00:12:41,210
And with what you've learned
你之前学过的知识

379
00:12:41,420 --> 00:12:43,120
in these videos, I think hopefully
和在该视频学到的内容

380
00:12:43,500 --> 00:12:44,710
you'll be able to implement
我希望你可以去

381
00:12:45,150 --> 00:12:46,280
PCA and use them
通过所有的上述目的

382
00:12:46,500 --> 00:12:47,930
through all of these purposes as well
来更好的使用PCA

