1
00:00:00,120 --> 00:00:01,220
If you run the learning algorithm
当你运行一个学习算法时（字幕整理，中国海洋大学，黄海广，haiguang2000@qq.com）

2
00:00:01,710 --> 00:00:02,640
and it doesn't do as well
如果这个算法的表现不理想

3
00:00:02,840 --> 00:00:04,520
as you are hoping, almost all
那么多半是出现

4
00:00:04,740 --> 00:00:05,670
the time it will be because
两种情况

5
00:00:06,100 --> 00:00:07,650
you have either a high bias
要么是偏差比较大

6
00:00:08,010 --> 00:00:09,530
problem or a high variance problem.
要么是方差比较大

7
00:00:09,860 --> 00:00:10,940
In other words they're either an
换句话说 出现的情况要么是欠拟合

8
00:00:11,130 --> 00:00:13,140
underfitting problem or an overfitting problem.
要么是过拟合问题

9
00:00:14,260 --> 00:00:15,090
And in this case it's very
那么这两种情况

10
00:00:15,350 --> 00:00:16,580
important to figure out
哪个和偏差有关

11
00:00:16,790 --> 00:00:17,970
which of these two problems is
哪个和方差有关

12
00:00:18,280 --> 00:00:19,500
bias or variance or a bit of both that you
或者是不是和两个都有关

13
00:00:20,210 --> 00:00:20,430
actually have.
搞清楚这一点非常重要

14
00:00:21,050 --> 00:00:21,980
Because knowing which of these
因为能判断出现的情况

15
00:00:22,440 --> 00:00:23,890
two things is happening would give
是这两种情况中的哪一种

16
00:00:24,060 --> 00:00:25,940
a very strong indicator for whether
其实是一个很有效的指示器

17
00:00:26,180 --> 00:00:27,490
the useful and promising ways
指引着可以改进算法的

18
00:00:27,770 --> 00:00:29,030
to try to improve your algorithm.
最有效的方法和途径

19
00:00:30,230 --> 00:00:31,270
In this video, I would like
在这段视频中

20
00:00:31,380 --> 00:00:33,030
to delve more deeply into
我想更深入地探讨一下

21
00:00:33,220 --> 00:00:34,850
this bias and various issue and
有关偏差和方差的问题

22
00:00:35,180 --> 00:00:36,530
understand them better as well
希望你能对它们有一个更深入的理解

23
00:00:36,790 --> 00:00:38,470
as figure out how to look
并且也能弄清楚怎样评价一个学习算法

24
00:00:38,610 --> 00:00:42,910
at and evaluate knows whether or not we might have a bias problem or a variance problem.
能够判断一个算法是偏差还是方差有问题

25
00:00:43,030 --> 00:00:45,750
Since this would be critical to
因为这个问题对于弄清

26
00:00:45,900 --> 00:00:48,180
figuring out how to improve the performance of learning algorithm that you implement.
如何改进学习算法的效果非常重要

27
00:00:48,640 --> 00:00:52,270
So you've already
好的 这几幅图

28
00:00:52,680 --> 00:00:53,690
seen this figure a few times,
你已经见过很多次了

29
00:00:54,190 --> 00:00:55,230
where if you fit two simple
如果你用两个很简单的假设来拟合数据

30
00:00:55,710 --> 00:00:57,900
hypothesis, like a straight line that that underfits the data.
比如说用一条直线 那么不足以拟合这组数据(欠拟合)

31
00:00:59,660 --> 00:01:00,720
If you fit a two complex
而如果你用两个很复杂的假设来拟合时

32
00:01:01,250 --> 00:01:02,870
hypothesis, then that might
那么对训练集来说

33
00:01:03,400 --> 00:01:05,050
fit the training set perfectly but
则会拟合得很好

34
00:01:05,270 --> 00:01:06,810
overfit the data and this
但又过于完美(过拟合)

35
00:01:06,930 --> 00:01:09,000
may be hypothesis of some
而像这样的

36
00:01:09,340 --> 00:01:11,000
intermediate level of complexity,
中等复杂度的假设

37
00:01:11,810 --> 00:01:13,120
of some, maybe degree two
比如某种二次多项式的假设

38
00:01:13,390 --> 00:01:15,770
polynomials are not too low and not too high degree.
次数既不高也不低

39
00:01:16,560 --> 00:01:17,340
That's just right.
这种假设对数据拟合得刚刚好

40
00:01:17,560 --> 00:01:18,480
And gives you the best
此时对应的的泛化误差

41
00:01:19,100 --> 00:01:20,740
generalization error out of these options.
也是三种情况中最小的

42
00:01:21,770 --> 00:01:22,960
Now that we're armed with the
现在我们已经掌握了

43
00:01:23,030 --> 00:01:25,130
notion of training and validation
训练集 验证集和测试集的概念

44
00:01:26,100 --> 00:01:27,550
in test sets, we can understand
我们就能更好地理解

45
00:01:28,290 --> 00:01:30,530
the concepts of bias and variance a little bit better.
偏差和方差的问题

46
00:01:31,310 --> 00:01:33,140
Concretely, let our
具体来说

47
00:01:33,370 --> 00:01:34,920
training error and cross
我们沿用之前所使用的

48
00:01:35,050 --> 00:01:36,620
validation error be defined as
训练集误差和验证集

49
00:01:36,850 --> 00:01:38,440
in the previous videos, just say,
误差的定义

50
00:01:38,680 --> 00:01:40,110
the squared error, the average
也就是平方误差

51
00:01:40,450 --> 00:01:41,420
squared error as measured
即对训练集数据进行预测

52
00:01:41,830 --> 00:01:42,810
on the 20 sets or as
或对验证集数据进行预测

53
00:01:42,930 --> 00:01:44,710
measured on the cross validation set.
所产生的平均平方误差

54
00:01:46,560 --> 00:01:47,690
Now let's plot the following figure.
下面我们来画出如下这个示意图

55
00:01:48,470 --> 00:01:49,930
On the horizontal axis I am
横坐标上表示的是

56
00:01:50,010 --> 00:01:52,000
going to plot the degree of polynomial,
多项式的次数

57
00:01:52,400 --> 00:01:53,380
so as I go the right
因此横坐标越往右的位置

58
00:01:54,810 --> 00:01:57,050
I'm going to be fitting higher and higher order polynomials.
表示多项式的次数越大

59
00:01:58,000 --> 00:02:02,687
So will the left of this figure, where maybe D equals one, we're
那么我们来画这幅图对应的情况,d可能等于1的情况,

60
00:02:02,687 --> 00:02:07,500
going to be fitting very simple figures.Whereas way here on the right of the
是用很简单的函数来进行拟合,而在右边的这个图中

61
00:02:07,500 --> 00:02:12,187
horizontal axis, have much larger values of D. So a much higher degree of
水平横坐标表示有更多更大的d值.表示更高次数的

62
00:02:12,187 --> 00:02:17,487
polynomial. And so here that's going to correspond to fitting. Much more complex
多项式.因此这些位置对应着使用更复杂的

63
00:02:17,487 --> 00:02:22,836
functions to your training set. Let's look at the training error and the cross
函数来拟合你的训练集时所需要的d值。让我们来把训练集误差和交叉

64
00:02:22,836 --> 00:02:27,342
validation error and plot them on this figure. Let's start with the training
验证集误差画在这个坐标中。我们先来画训练集误差

65
00:02:27,342 --> 00:02:32,086
error. As we increase the degree of the polynomial, we're going to be able to fit
随着我们增大多项式的次数，我们将对训练集拟合得

66
00:02:32,086 --> 00:02:36,533
our training set better and better. And so, if D=1 then it's a relatively high
越来越好。所以如果d等于1时，对应着一个比较大的

67
00:02:36,533 --> 00:02:41,157
training error. If we have a very high degree polynomial our training error is
训练误差。而如果我们的多项式次数很高时，我们的训练误差

68
00:02:41,157 --> 00:02:45,960
going to be really low maybe even zero because we'll fit the training set really well.
就会很小，甚至可能等于0，因为可能非常拟合训练集

69
00:02:45,960 --> 00:02:50,644
And so as we increase the degree of polynomial, we find typically that the
所以，当我们增大多项式次数时，我们不难发现

70
00:02:50,644 --> 00:02:55,776
training error decreases. So, I'm going to write J. Subscript three of data there.
训练误差明显下降。这里我写上J下标3来表示训练集误差

71
00:02:55,776 --> 00:03:01,565
Because our training error tends to decrease with the degree of polynomial
因为随着我们对数据拟合所需多项式次数的增大

72
00:03:01,565 --> 00:03:07,652
that we fit to the data. Next let's look at the cross validation error. Or for that
训练误差是趋于下降的。接下来我们再看交叉验证误差

73
00:03:07,652 --> 00:03:13,812
matter, if we look at the test set error we'll get a pretty similar result as if we
事实上如果我们观察测试集误差的话，我们会得到一个和交叉验证误差

74
00:03:13,812 --> 00:03:19,292
were to plot the cross validation error. So, we know that if D=1. We're fitting a
非常接近的结果。所以，我们知道如果d等于1的话

75
00:03:19,292 --> 00:03:23,758
very simple function. And so we may be under fitting the training set. And so
意味着用一个很简单的函数来拟合数据。也就是说

76
00:03:23,758 --> 00:03:28,343
we're going to have a very high cross validation error. If we fit, you know, an
 我们会得到一个较大的交叉验证误差。而如果我们用

77
00:03:28,343 --> 00:03:33,107
intermediate degree polynomial, there's, we have a D equals two in our example on
一个中等大小的多项式次数来拟合时，在前一张幻灯片中我们用的d等于2

78
00:03:33,107 --> 00:03:38,049
the previous slide, we're going to have a much lower cross validation error, because
那么我们会得到一个更小的交叉验证误差

79
00:03:38,049 --> 00:03:43,110
we're just fitting, finding a much better fit to the data. And conversely, if D were
因为我们找了一个能够更好拟合数据的次数，同样地，反过来，如果次数d

80
00:03:43,110 --> 00:03:47,874
too high, so if D took on, say, a value of four, then we're getting over fitting, and
太大，比如说d的值取为4，那么我们又过拟合了

81
00:03:47,874 --> 00:03:52,669
so we ended with a high value for cross validation error. So, if you were to. vary
我们又会得到一个较大的交叉验证误差

82
00:03:52,669 --> 00:03:57,854
this smoothly and plot the curve. You might end up with a curve like that. Where,
因此 如果你平稳地过渡这几个点，你可以绘制出一条平滑的曲线，就像这样

83
00:03:57,854 --> 00:04:03,737
that's J.C.V. Of staza. Indicating that you plot J. Tesla's data, you get
我用Jcv(θ)来表示。同样地，如果你画出Jtest(θ)，你也将得到

84
00:04:03,737 --> 00:04:09,465
something very similar. And so, this sort of plot also helps us to better understand
一条类似的曲线，这样一幅图也同时能帮助我们更好地理解

85
00:04:09,465 --> 00:04:13,829
the notions of bias and variance. Concretely, suppose you've applied a
偏差和方差的概念。具体来说，假设你得出了一个

86
00:04:13,829 --> 00:04:18,447
learning algorithm, and it's not performing as well as you were hoping.
学习算法，而这个算法并没有表现地如你期望那么好。

87
00:04:18,447 --> 00:04:23,507
So, if your cross-validation set error, or your test set error is high. How can we
所以你的交叉验证误差或者测试集误差都很大。我们应该

88
00:04:23,507 --> 00:04:28,758
figure out if the learning algorithm is suffering from high bias or if it's suffer
如何判断此时的学习算法正处于高偏差的问题还是

89
00:04:28,758 --> 00:04:34,130
from high variance? So the setting of the cause validation error being high,
高方差的问题？交叉验证误差比较大的情况，

90
00:04:34,130 --> 00:04:39,789
corresponds to either this regime or this regime. So this regime on the left
对应着曲线中的这一端，或者这一端，那么左边的这一端，

91
00:04:39,789 --> 00:04:45,747
corresponds to a high bias problem. That is, if you're fitting a overly low order
对应的就是高偏差的问题。也就是你使用了一个过于小的

92
00:04:45,747 --> 00:04:51,556
polynomial, such as a D=1. When we really needed a higher order polynomial to fit
多项式次数，比如d等于1。但实际上我们需要一个较高的多项式次数来拟合数据

93
00:04:51,556 --> 00:04:57,513
the data. Whereas in contrast, this regime corresponds to a high variance problem.
相反地，右边这一端对应的是高方差问题。

94
00:04:57,513 --> 00:05:03,247
That is, if D, the degree of polynomial was too large for the data set that we
也就是说，多项式次数d对于我们的数据来讲太大了，

95
00:05:03,247 --> 00:05:08,535
have. And this figure just has a clue for how to distinguish between these two
这幅图也提示了我们怎样区分这两种情况。

96
00:05:08,535 --> 00:05:15,790
cases. Concretely for the high bias case.That is the case of under fitting. What we
具体地说，对于高偏差的情况，也就是对应欠拟合的情况，我们

97
00:05:15,790 --> 00:05:21,790
find is that both the cross validation error and the trading error are going to
发现交叉验证误差和训练误差都会

98
00:05:21,790 --> 00:05:29,770
be high. So if your algorithm is suffering from a bias problem. The training set
很大。因此，如果你的算法有偏差问题的话，那么训练集

99
00:05:29,770 --> 00:05:38,162
error, will be high. And you might find that the cross validation error will also
误差将会比较大。同时你可能会发现交叉验证集误差

100
00:05:38,162 --> 00:05:45,271
be high. It might be a close. Maybe just slightly higher than a training error. And
也很大。两个误差可能很接近，或者可能验证误差稍大一点

101
00:05:45,271 --> 00:05:52,211
so, if you see this combination that's a sign your algorithm may be suffering from
所以如果你看到这样的组合情况，那就表示你的算法正处于

102
00:05:52,211 --> 00:05:58,981
high bias. In contrast if your algorithm is suffering from high variance then if
高偏差的问题。反过来，如果你的算法处于高方差的问题，

103
00:05:58,981 --> 00:06:05,921
you look here. We'll notice that J-train that is the training error is going to be
那么如果你观察这里，我们会发现 Jtrain就是训练误差，

104
00:06:05,921 --> 00:06:13,626
low. That is your fitting the training set very well. Where as your, cross validation
会很小。也就意味着，你对训练集数据拟合得非常好。而你的交叉验证误差也一样。

105
00:06:13,626 --> 00:06:18,575
error. Assuming that this is say the squared era. Which we're trying to
假设此时我们最小化的是平方误差，

106
00:06:18,575 --> 00:06:22,757
minimize . Where as in contrast, your error on the cross
而反过来，你的交叉验证集误差

107
00:06:22,757 --> 00:06:28,080
validation set or your cross function in the cross validation set will be much bigger
或者说你的交叉验证集对应的代价函数的值，将会远远大于

108
00:06:28,080 --> 00:06:32,884
Than your training set error. So, there's a double greater than sign. That's the
训练集误差。这里的双大于符号是一个

109
00:06:32,884 --> 00:06:37,935
math symbol for much greater than, denoted by two greater than signs. And so, if you
数学符号，表示远远大于，用两个大于符号表示。因此如果

110
00:06:37,935 --> 00:06:43,047
see this combination of values then that might give you, that's a clue that your
你看见这种组合的情况，这就预示着你的

111
00:06:43,047 --> 00:06:47,482
learning algorithm maybe suffering from high variance. And might be over
学习算法可能正处于高方差和过拟合的情况

112
00:06:47,482 --> 00:06:51,732
emphasizing. And the key that distinguishes these two cases is if you
同时，区分这两种不同情形的关键依据是

113
00:06:51,732 --> 00:06:56,844
have a high bias problem your training set error will also be high. Your hypothesis
如果你的算法处于高偏差的情况，那么你的训练集误差会很大。因为你的假设

114
00:06:56,844 --> 00:07:02,080
is just not fitting the training set well. And if you have a high variance problem.
不能很好地拟合训练集数据。而当你处于高方差的问题时

115
00:07:02,080 --> 00:07:06,852
Your training set error will usually be low. That is much lower than your cross
你的训练误差通常都会很小，并且远远小于

116
00:07:06,852 --> 00:07:11,684
validation error. So hopefully that gives you a somewhat better understanding of the
交叉验证误差。好的，但愿这节课能让你更清楚地理解

117
00:07:11,684 --> 00:07:15,845
two problems of bias and variants. I still have a lot more to say about bias and
偏差和方差这两种问题。在之后几段视频中，我还将对偏差和误差做更多的解释

118
00:07:15,845 --> 00:07:19,954
variants in the next few videos. But what we'll see later is that by diagnosing
但我们之后要关注的是诊断一个学习算法

119
00:07:19,954 --> 00:07:24,011
whether a learning algorithm may be suffering from high bias or high variance,
是否处于高偏差或高方差的情况。

120
00:07:24,011 --> 00:07:28,432
we'll show you even more details of how to do that in later videos. We'll see that by
在后面几段视频中我还将向你展示更多细节，我们将会看到

121
00:07:28,432 --> 00:07:32,697
figuring out whether a learning algorithm may be suffering from high bias or high
通过分清一个学习算法是处于高偏差还是高误差，

122
00:07:32,697 --> 00:07:36,806
variance, or a combination of both, that, that would give us much better guidance
还是两种情况的结合，这能够更好地指引我们

123
00:07:36,806 --> 00:07:42,367
promising things to try in order to improve the performance of a learning algorithm.
应该采取什么样的措施，来提高学习算法的性能表现.

